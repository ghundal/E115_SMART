chunk_id,document,class,page,chunk_text,chunk_length,chunk_method
9612f272-2d6b-4384-9e16-5184e67ebdfe,Regularization and Sparse Models.pdf,CSCI_83,0,"Regularization and Sparse Models
Steve Elston
11/03/2022",56,semantic
032b394c-2a8c-4872-a30f-3fd391c4a740,Regularization and Sparse Models.pdf,CSCI_83,1,"Dealing with Overﬁt Models
Example: We suspect that the terms, ses and prog are signiﬁcant in predicting social science test scores:
##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.348## Model:                            OLS   Adj. R-squared:                  0.309## Method:                 Least Squares   F-statistic:                     8.753## Date:                Fri, 11 Nov 2022   Prob (F-statistic):           1.55e-09## Time:                        07:55:39   Log-Likelihood:                -495.01## No. Observations:                 140   AIC:                             1008.## Df Residuals:                     131   BIC:                             1035.## Df Model:                           8                                         ## Covariance Type:            nonrobust                                         ## ============================================================================================##                                coef    std err          t      P>|t|      [0.025      0.975]## --------------------------------------------------------------------------------------------## Intercept                   46.1667      2.478     18.627      0.000      41.264      51.070## C(ses)[T.2]                  3.7564      3.437      1.093      0.276      -3.043      10.556## C(ses)[T.3]                 10.3333      4.293      2.407      0.017       1.841      18.826## C(prog)[T.2]                 6.0476      3.378      1.791      0.076      -0.634      12.729## C(prog)[T.3]               -10.2917      3.919     -2.626      0.010     -18.044      -2.539## C(ses)[T.2]:C(prog)[T.2]    -0.8457      4.402     -0.192      0.848      -9.555       7.863## C(ses)[T.3]:C(prog)[T.2]    -2.5476      5.114     -0.498      0.619     -12.664       7.569## C(ses)[T.2]:C(prog)[T.3]     9.2257      4.954      1.862      0.065      -0.573      19.025## C(ses)[T.3]:C(prog)[T.3]     1.2917      6.788      0.190      0.849     -12.136      14.719## ==============================================================================## Omnibus:                       10.190   Durbin-Watson:                   2.059## Prob(Omnibus):                  0.006   Jarque-Bera (JB):               10.371## Skew:                          -0.651   Prob(JB):                      0.00560## Kurtosis:                       3.290   Cond. No. 18.7## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",2732,semantic
74a47c4c-b673-4d67-a688-53f37b2fe55f,Regularization and Sparse Models.pdf,CSCI_83,1,"formula = 'socst ~ C(ses)*C(prog)'
#linear_model = smf.ols(""socst ~ C(ses)*C(prog)"", data=test_scores_train).fit()linear_model = smf.ols(formula, data=test_scores_train).fit()print(linear_model.summary())",204,semantic
60cfb9b3-b4c1-4624-9b79-bb4002e85d6d,Regularization and Sparse Models.pdf,CSCI_83,2,"Dealing with Overﬁt Models
Example: After 3 or 4 rounds of guess and cut feature pruning, we arrive at a model with only signiﬁcant coefﬁcients:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.348## Model:                            OLS   Adj. R-squared:                  0.309## Method:                 Least Squares   F-statistic:                     8.753## Date:                Fri, 11 Nov 2022   Prob (F-statistic):           1.55e-09## Time:                        07:55:39   Log-Likelihood:                -495.01## No.",770,semantic
b5905208-6586-4a06-8252-ce2110d7ad25,Regularization and Sparse Models.pdf,CSCI_83,2,"Observations:                 140   AIC:                             1008.## Df Residuals:                     131   BIC:                             1035.## Df Model:                           8                                         ## Covariance Type:            nonrobust                                         ## ========================================================================================##                            coef    std err          t      P>|t|      [0.025      0.975]## ----------------------------------------------------------------------------------------## C(ses)[1]:C(prog)[1]    46.1667      2.478     18.627      0.000      41.264      51.070## C(ses)[2]:C(prog)[1]    49.9231      2.381     20.965      0.000      45.212      54.634## C(ses)[3]:C(prog)[1]    56.5000      3.505     16.120      0.000      49.566      63.434## C(ses)[1]:C(prog)[2]    52.2143      2.295     22.755      0.000      47.675      56.754## C(ses)[2]:C(prog)[2]    55.1250      1.518     36.320      0.000      52.123      58.127## C(ses)[3]:C(prog)[2]    60.0000      1.568     38.277      0.000      56.899      63.101## C(ses)[1]:C(prog)[3]    35.8750      3.035     11.819      0.000      29.870      41.880## C(ses)[2]:C(prog)[3]    48.8571      1.874     26.077      0.000      45.151      52.563## C(ses)[3]:C(prog)[3]    47.5000      4.293     11.065      0.000      39.008      55.992## ==============================================================================## Omnibus:                       10.190   Durbin-Watson:                   2.059## Prob(Omnibus):                  0.006   Jarque-Bera (JB):               10.371## Skew:                          -0.651   Prob(JB):                      0.00560## Kurtosis:                       3.290   Cond. No. 2.83## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
linear_model = smf.ols(""socst ~ - 1 + C(ses):C(prog)"", data=test_scores_train).fit()
linear_model.summary()",2091,semantic
4ba0bbdb-887c-4015-8e3c-f8d1183c6fb5,Regularization and Sparse Models.pdf,CSCI_83,3,"Dealing with Overﬁt Models
Let’s compare the results of the unpruned and pruned models
The metrics indicate the ﬁt is exactly the same
Why prefer the sparse (pruned) model? What are the consequences of the over-ﬁt model? Several predictors (features) are included that are not needed
Including non-signiﬁcant predictors can only increase noise and reduce generalization of a model
Colinear features confound model ﬁting - change of coefﬁcient values correlated
For model with linear response, consider the effect of an unexpected value of a non-signiﬁcant predictor
But manually pruning a model with a great many features is a doomed task!",639,semantic
61128a67-f2f7-46e6-811d-012a06248335,Regularization and Sparse Models.pdf,CSCI_83,3,"What if we included all the interactions with type of school, race and sex? We now have up to 5th order interaction - 45 model coefﬁcients with none signiﬁcant!!",161,semantic
ed16462e-7cb7-4d08-9c35-5ae30ce76d12,Regularization and Sparse Models.pdf,CSCI_83,4,"Dealing with Overﬁt Models
We want our models to be sparse
A sparse model has the minimum complexity required to explain the data
The sparse model is a manifestation of Occam’s Razor
A scientiﬁc principle that the simplest of competing theories is the preferred one
Sparse models use the minimum number of independent variables (features)
Are considered parsimonious
Generalize well
Use regularization methods to identify minimum coefﬁcient set",444,semantic
6a640f1b-96a9-4f7c-8001-48fa93f77e98,Regularization and Sparse Models.pdf,CSCI_83,5,"Dealing with Overﬁt Models
Idea!: Try systematically pruning the model using some metric
Leads to the the step-wise regression algorithm
Forward step-wise regression adds most explanatory variable one at a time
Backward step-wise regression removes lest explanatory variable one at a time
Can go both directions - see the R documentation
Hard to ﬁnd a good metric
But, making multiple hypothesis tests is a fraught undertaking
 is null or insigniﬁcant predictor
High probability of Type 1 or Type 2 error
Type 1 error, fail to accept ,  include insigniﬁcant predictor
Type 2 error, fail to reject ,  drop signiﬁcant predictor
𝐻0
𝐻0→
𝐻0→",636,semantic
a25ac2f9-482a-4fff-b733-008feadaa797,Regularization and Sparse Models.pdf,CSCI_83,6,"Regularization - The Bias-Variance Trade-Off
Regularization is a systematic approach to preventing over-ﬁtting
To understand regularization need to understand the bias-variance trade-off
To better understand this trade-off decompose mean square error:
 the label vector the feature matrix the trained model
Expanding this relation gives us:
Δ=𝐸[(𝑌−(𝑋) ]𝑦2 𝑓̂  )2
𝑌=𝑋=(𝑥)=𝑓̂ 
Δ=(𝐸[(𝑋)]−(𝑋) +𝐸[((𝑋)−𝐸[(𝑋)] ]+𝑦2 𝑓̂  𝑓̂  )
2 𝑓̂  𝑓̂  )2 𝜎2
Δ=𝐵𝑖𝑎 +𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒+𝐼𝑟𝑟𝑒𝑑𝑢𝑐𝑖𝑏𝑙𝑒 𝐸𝑟𝑟𝑜𝑟𝑦2 𝑠2",473,semantic
d1c1bb1c-a290-49cd-a84d-39b8d5a12e27,Regularization and Sparse Models.pdf,CSCI_83,7,"Regularization - The Bias-Variance Trade-Off
How do we interpret the bias-variance trade-off relationship:
- , the expected squared difference between the model output and the expected model output is the variance of the model
- For low variance model, 
- Model generalizes since variance is low for each prediction, 
, the expected value of the difference between the model output and the expected model output is the bias of the model
For unbiased model, 
Example: OLS model with  is unbiased
 is inherent or irreducable error in data
Δ=(𝐸[(𝑋)]−(𝑋) +𝐸[((𝑋)−𝐸[(𝑋)] ]+𝑦2 𝑓̂  𝑓̂  )
2 𝑓̂  𝑓̂  )2 𝜎2
𝐸[((𝑋)−𝐸[(𝑋)] ]𝑓̂  𝑓̂  )2
𝐸[((𝑋)−𝐸[(𝑋)] ]→0𝑓̂  𝑓̂  )2
(𝑋)𝑓̂ 
(𝐸[(𝑋)]−(𝑋)𝑓̂  𝑓̂  )
2
𝐸[ (𝑋)]−(𝑋)]=0𝑓̂  𝑓̂ 
𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙𝑠∼ 𝑁(0, )𝜎2
𝜎2",725,semantic
342d0fcc-0f86-4bb6-8b9c-a015e6325892,Regularization and Sparse Models.pdf,CSCI_83,8,"Regularization - The Bias-Variance Trade-Off
There is a trade-off between bias and variance
Need to ﬁnd the trade-off point
The trade-off between bias and variance",163,semantic
23ce0456-478c-4e51-845a-a26dae1e1e6c,Regularization and Sparse Models.pdf,CSCI_83,9,"Eigendecomposition - Review
For OLS model need to ﬁnd  for  design matrix 
Decompose the  covariance matrix  into eigenvalues and eigenvectors:
where,
 is the  matrix of  orthonormal eigenvectors
And, the  eigenvalues are represented as diagonal matrix:
For real-valued  the eigenvectors  are real valued so:
( 𝐴 𝐴∝ 𝐶𝑜𝑣(𝐴 𝐴𝐴𝑇 )−1 )−1 𝑛×𝑝 𝐴
𝑝×𝑝 𝑐𝑜𝑣(𝐴)= 𝐴1𝑝𝐴𝑇
𝑐𝑜𝑣(𝐴)=𝑄Λ𝑄−1
𝑄 𝑝×𝑝 𝑝
𝑝
Λ=
⎡
⎣
⎢⎢⎢⎢
,0,…,0𝜆1
0, ,…𝜆2
⋮ , ⋮ , ⋮ , ⋮ 0,0,…,𝜆𝑝
⎤
⎦
⎥⎥⎥⎥
𝑐𝑜𝑣(𝐴) 𝑄
=𝑄−1 𝑄𝑇",458,semantic
8d45928c-ae3d-4547-93ea-65d32fcba7f5,Regularization and Sparse Models.pdf,CSCI_83,10,"Eigendecomposition - Review
The inverse of the covariance can be computed from its eigendecomposition
Where  is written:
𝑐𝑜𝑣(𝐴 = 𝑄)−1 𝑄𝑇Λ−1
𝑄−1
=Λ−1
⎡
⎣
⎢⎢⎢⎢⎢
,0,…,01𝜆1
0, ,…1𝜆2
⋮ , ⋮ , ⋮ , ⋮ 0,0,…,1𝜆𝑝
⎤
⎦
⎥⎥⎥⎥⎥",211,semantic
802740c1-fa6f-4e07-812d-be59b71ab10e,Regularization and Sparse Models.pdf,CSCI_83,11,"Eigendecomposition - Review
At ﬁrst look eigen-decomposition seems a bit mysterious
The eigenvalues are the roots of the covariance matrix
Similar to the familiar roots of a polynomial
For square matrix, , and some norm 1 vector, , we can ﬁnd a root, :
For  dimensional matrix there are  eigenvalues and orthogonal eigenvalue
But, there is no guarantee that the  values of  are unique
If columns of  are colinear, there are  unique eigenvectors
𝐴 𝑥 𝜆
𝐴𝑥𝐴𝑥−𝜆𝑥(𝐴−𝐼𝜆)𝑥
=𝜆𝑥=0=0
𝑝×𝑝 𝑝
𝑝 𝜆
𝐴 <𝑝",488,semantic
795116b6-ce49-4d3f-828c-38a5c269e465,Regularization and Sparse Models.pdf,CSCI_83,12,"Regularization - Ill-Posed Problems
If columns of  are not linearly independent, the covariance matrix is ill-posed and the eigenvectors are under-determined
The eigenvalues are ordered, 
For ill-posed covariance matrix
The smallest 
So, 
Uh oh! The inverse covariance matrix does not exist! With colinear features  confounded ﬁtting
With unifromative features  projecting random noise
𝐴
𝑐𝑜𝑣(𝐴)=𝑄Λ𝑄𝑇
𝑐𝑜𝑣(𝐴 = 𝑄)−1 𝑄𝑇Λ−1
≥ ≥…≥𝜆1 𝜆2 𝜆𝑝
→0𝜆𝑖
→∞1𝜆𝑖
→0 →𝜆𝑖
→0 →𝜆𝑖",457,semantic
6719d42a-600a-4183-be91-3f243c0c6b3a,Regularization and Sparse Models.pdf,CSCI_83,13,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
Recall:
The normal equations provide a solution:
This solution requires ﬁnding the inverse of the covariance matrix, 
But this inverse may be unstable
In mathematical terminology we say the problem is ill-posed
𝑏⃗ 
𝑥=𝐴𝑏+𝜖
𝑏=( 𝐴 𝑥𝐴𝑇 )−1𝐴𝑇
( 𝐴𝐴𝑇 )−1",339,semantic
c6bde2f4-9f54-49c6-9723-535955ae78ec,Regularization and Sparse Models.pdf,CSCI_83,14,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
We can add a bias term to the diagonal of the covariance matrix
the L2 or Euclidean norm minimization problem):
Where the L2 norm of the coefﬁcient vector is:
𝑏⃗ 
𝑚𝑖𝑛[∥ 𝐴⋅ 𝑥−𝑏∥ +  ∥ 𝑏∥ ]𝛼2
𝑜𝑟𝑏=( 𝐴+ 𝐼 𝑥𝐴𝑇 𝛼2)−1𝐴𝑇
||𝑏||=( + +…+ =(𝛽21 𝛽22 𝛽2𝑚)
12
∑𝑖=1
𝑚
𝛽2𝑖)
12",350,semantic
e28488c3-7675-427d-87a4-2acb49eab18e,Regularization and Sparse Models.pdf,CSCI_83,15,"L2 Regularization
How can we understand this relationship? Adds values along the diagonal of the covariance matrix
This creates a so called ridge in the covariance, 
Leads to the term ridge regression
Constrain the L2 norm values of the model coefﬁcients using the penalty term 
Larger  is more bias but lover variance
Larger  makes the inverse of the covariance more stable
L2 regularization is a soft constraint on the model coefﬁcients
Even smallest coefﬁcients are not driven to 0
Coefﬁcients can grow in value, but under the constraint
𝑏=( 𝐴+ 𝐼 𝑥𝐴𝑇 𝛼2)−1𝐴𝑇
𝑐𝑜 = 𝐴+𝑣𝑟𝑒𝑔𝑢𝑙𝑎𝑟𝑖𝑧𝑒𝑑 𝐴𝑇 𝛼2
𝛼
𝛼
𝛼",593,semantic
4c2bf7ed-dae4-4a16-b161-1d9fccde1a8d,Regularization and Sparse Models.pdf,CSCI_83,16,"L2 Regularization
Eigen-decomposition of the regularized covariance matrix:
The inverse covariance matrix is then:
With any , the inverse eigenvalues of the inverse covariance matrix are bounded
Increasing  increases bias, but increases the stability of the inverse
𝐴+ 𝐼=𝑄𝐴𝑇 𝛼2
⎡
⎣
⎢⎢⎢⎢
+ ,0,…,0𝜆1 𝛼2
0, + ,…𝜆2 𝛼2
⋮ , ⋮ , ⋮ , ⋮ 0,0,…, +𝜆𝑝 𝛼2
⎤
⎦
⎥⎥⎥⎥𝑄𝑇
( 𝐴+ 𝐼 = 𝑄𝐴𝑇 𝛼2)
−1 𝑄𝑇
⎡
⎣
⎢⎢⎢⎢⎢⎢
,0,…,01+𝜆1 𝛼2
0, ,…1+𝜆2 𝛼2
⋮ , ⋮ , ⋮ , ⋮ 0,0,…, 1+𝜆𝑝𝛼2
⎤
⎦
⎥⎥⎥⎥⎥⎥
𝛼>0
𝛼",458,semantic
156bcb0a-6bae-4525-87c4-c99806a948d6,Regularization and Sparse Models.pdf,CSCI_83,17,"L2 Regularization
Example: compute the eigenvalues of a covariance matrix
## array([41.33783544, 13.34471885,  8.85600018,  2.37491505,  1.7218998 ,##         1.6220889 ,  0.11774255,  0.38848204,  0.45853941])
The condition number of the covariance is 
Add regularization with  and compute the eigenvalues
## array([41.43783544, 13.44471885,  8.95600018,  2.47491505,  1.8218998 ,##         1.7220889 ,  0.21774255,  0.48848204,  0.55853941])
The condition number of the covariance is 
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
Y, X = dmatrices(""socst_zero_mean ~ C(ses, levels=[1,2,3])*C(prog, levels=[1,2,3])"", data=test_scores)cov_X = np.matmul(np.transpose(X),X)cov_X = np.divide(cov_X, float(cov_X.shape[0 ]))
np.real(np.linalg.eigvals(cov_X))
∼ 90
=0.1𝛼2
alpha_sqr = 0 . 1  alpha_sqr = np.diag([alpha_sqr] * cov_X.shape[0 ])#alpha_sqr = np.diag([alpha] * cov_X.shape[0])#cov_X = np.divide(np.matmul(np.transpose(X),X), float(cov_X.shape[0]))
cov_X = np.add(cov_X, alpha_sqr)np.real(np.linalg.eigvals(cov_X))
∼ 72",1063,semantic
900d2f55-4103-4fc0-a550-7f3040ac6da1,Regularization and Sparse Models.pdf,CSCI_83,18,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
The norm of the coefﬁcient vector, , is constrained
L2 norm constraint of model coefﬁcients
𝑏⃗ 
𝑏⃗",190,semantic
402492f7-7e97-41ca-95a0-1f5851fc1492,Regularization and Sparse Models.pdf,CSCI_83,19,"L2 Regularization
Example: Increasing constraint on model coefﬁcients with larger L2 regularization hyperparameter
def regularized_coefs(df_train, df_test, alphas, L1_wt=0 . 0 , n_coefs=8 ,
                      formula = 'socst_zero_mean ~ C(ses)*C(prog)',                       label='socst_zero_mean'):    '''Function that computes a linear model for each value of the regularization 
    parameter alpha and returns an array of the coefficient values. The L1_wt     determines the trade-off between L1 and L2 regularization'''    coefs = np.zeros((len(alphas),n_coefs + 1 ))    MSE_train = []
    MSE_test = []    for i,alpha in enumerate(alphas):        ## First compute the training MSE
        temp_mod = smf.ols(formula, data=df_train).fit_regularized(alpha=alpha,L1_wt=L1_wt)        coefs[i,:] = temp_mod.params        MSE_train.append(np.mean(np.square(df_train[label] - temp_mod.predict(df_train))))        ## Then compute the test MSE
        MSE_test.append(np.mean(np.square(df_test[label] - temp_mod.predict(df_test))))            return coefs, MSE_train, MSE_test
alphas = np.arange(0 . 0 , 0 . 3 , step = 0 .",1125,semantic
c09f45bd-da06-499d-ad73-c29ab2931daa,Regularization and Sparse Models.pdf,CSCI_83,19,"0 0 3 )   #alphas = np.arange(0.0, 100.0, step = 1.0)  Betas, MSE_test, MSE_train = regularized_coefs(test_scores_train, test_scores_test, alphas) #, formula=formula)
#Betas[:5]",177,semantic
ff1abcc2-f620-4912-8161-cee7a7e21d3f,Regularization and Sparse Models.pdf,CSCI_83,20,"L2 Regularization
Example: Increasing constraint on model coefﬁcients with larger L2 regularization hyperparameter",114,semantic
af37a787-0634-4171-8d89-5f976ed17bdd,Regularization and Sparse Models.pdf,CSCI_83,21,"L1 Regularization
The L1 norm provides regularization with different properties
Constrains the model parameters using the L1 norm:
This form looks a lot like the L2 regularization formulation
 is the L1 norm
Compute the l1 norm of the  model parameters:
 is the absolute value of . 𝑚𝑖𝑛[∥ 𝐴⋅ 𝑥−𝑏∥ +  ∥ 𝑏 ]𝛼2 ∥ 1
∥ 𝑏∥ 1
𝑚
||𝑏| =(| |+| |+…+| |)=( | ||1 𝑏1 𝑏2 𝑏𝑚 ∑𝑖=1
𝑚
𝑏𝑖)
1
| |𝑏𝑖 𝑏𝑖",380,semantic
68acfe24-1d63-4e14-85e5-8a91fcbba178,Regularization and Sparse Models.pdf,CSCI_83,22,"L1 Regularization
What are the properties of the L1 regularization
L1 norm is a hard constraint
L1 regularization drives coefﬁcients to zero
The hard constraint property leads to the term lasso regularization
Lasso regression is a method of feature selection",258,semantic
238eff15-c85f-4837-8ab8-f11bf6c430ba,Regularization and Sparse Models.pdf,CSCI_83,23,"L1 Regularization
The Lasso regularization is a strong constraint on coefﬁcient values
Some coefﬁcients are forced to zero
The constraint curve is like a lasso
L1 norm constraint of model coefﬁcients",199,semantic
dc637e35-0fb5-4ace-87f8-83d43b233950,Regularization and Sparse Models.pdf,CSCI_83,24,"L1 Regularization
Example: Increasing constraint on model coefﬁcients with larger L1 regularization hyperparameter
## array([[ -6.13412699,   3.62867936,  10.20615579,   5.88403028,##         -10.44293454,  -0.68215407,  -2.3846305 ,   9.37695361,##           1.44233432],##        [ -5.72441839,   3.43435166,   8.75855958,   4.84586199,##          -9.40956564,   0.05313331,   0. ,   7.99487046,##           0. ],##        [ -5.55039369,   3.33123726,   8.51053097,   4.79924961,##          -9.03159592,   0.",510,semantic
d2c27d64-9805-486b-8741-c64fb182325b,Regularization and Sparse Models.pdf,CSCI_83,24,",   0. ,   7.41265712,##           0. ],##        [ -5.35379746,   3.19004869,   8.26727483,   4.72128125,##          -8.68047004,   0. ,   0. ,   6.87279024,##           0. ],##        [ -5.15720816,   3.04893453,   8.02408465,   4.64326246,##          -8.32934311,   0. ,   0. ,   6.33285484,##           0. ]])
alphas = np.arange(0 .",336,semantic
cc387bf0-8e57-4377-a2b0-92ca1e6e0f8b,Regularization and Sparse Models.pdf,CSCI_83,24,"0 , 0 . 6 , step = 0 . 0 2 )
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=1 . 0 ) #, formula=formula)Betas[:5 ]",164,semantic
00fe9fdb-8a0c-4629-91bc-a8f483c4cea3,Regularization and Sparse Models.pdf,CSCI_83,25,"L1 Regularization
Example: Increasing constraint on model coefﬁcients with larger L1 regularization hyperparameter",114,semantic
6ad2d68f-00a1-4b20-bf9c-b59832cf2078,Regularization and Sparse Models.pdf,CSCI_83,26,"Elastic Net Regularization
Do we always have to choose between the soft constraint of L2 and the hard constraint of L1? L2 regularization works well for colinear features as soft constraint
Down-weights colinear features
But soft constraint so poor model selection
L1 regularization provides good model selection as hard constraint
Drives coefﬁcients of non-informative variables to 0
But poor selection for colinear features
The Elastic Net weights L1 and L2 regularization
Hyperparameter  weights L1 vs. L2 regularization
Hyperparameter  sets strength of regularization
𝜆
𝛼
𝑚𝑖𝑛[∥ 𝐴⋅ 𝑥−𝑏∥ + 𝜆 𝛼∥ 𝑏 + (1−𝜆) 𝛼∥ 𝑏 ]∥ 1 ∥ 2",620,semantic
5ac29d0d-5d00-41ef-a4ed-591fa95e9ae4,Regularization and Sparse Models.pdf,CSCI_83,27,"Elastic Net Regularization
Example: Increasing constraint on model coefﬁcients with larger L1 regularization hyperparameter
alphas = np.arange(0 . 0 , 0 . 5 , step = 0 .",169,semantic
09a2b79f-ee79-4892-956a-4172625ed769,Regularization and Sparse Models.pdf,CSCI_83,27,"0 2 )
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=0 . 5 )",111,semantic
54c54d37-775f-40a7-b9fb-9b5d9af258ba,Regularization and Sparse Models.pdf,CSCI_83,28,"Elastic Net Regularization
Example: Increasing constraint on model coefﬁcients with larger L1 regularization hyperparameter",123,semantic
251b92c6-edcb-4547-8388-9378db386ca7,Regularization and Sparse Models.pdf,CSCI_83,29,"Elastic Net Regularization
Check the model summary for , 
## Intercept                  -3.715924## C(ses)[T.2]                 2.048593## C(ses)[T.3]                 4.615190## C(prog)[T.2]                3.589684## C(prog)[T.3]               -5.103465## C(ses)[T.2]:C(prog)[T.2]    0.455652## C(ses)[T.3]:C(prog)[T.2]    2.310118## C(ses)[T.2]:C(prog)[T.3]    2.149526## C(ses)[T.3]:C(prog)[T.3]    0.000000## dtype: float64
𝜆=0.5𝛼=0.10
lm_elastic = smf.ols(""socst_zero_mean ~ C(ses)*C(prog)"", data=test_scores_train).fit_regularized(alpha=0 . 1 , L1_wt=0 . 5 )
lm_elastic.params",581,semantic
ad580293-7dd8-4a64-9603-769077f3d60c,Regularization and Sparse Models.pdf,CSCI_83,30,"Summary
Over-ﬁt models and regularization
Bias variance trade-off between ﬁt to training data (bias) and generalization error (vaiance)
Prefer minimal or sparse models
L2 regularization is a soft constraint
L1 regularization is a hard constraint
ElasticNet trade-off between L1 and L2",284,semantic
82226b61-73e1-4527-baa9-706cd97a6bd6,Forecasting And Time Series Analysis.pdf,CSCI_83,0,"Forecasting And Time Series Analysis
Steve Elston
11/10/2022",60,semantic
4c29c79d-9de3-48ff-aefb-736d3e9d0240,Forecasting And Time Series Analysis.pdf,CSCI_83,1,"Why Are Time Series Useful? Data are often time-ordered
Estimates 30% of data science problems include time series data
Must use speciﬁc time series models",155,semantic
04cc6bf8-0b7a-47b9-a157-4897886d8b6f,Forecasting And Time Series Analysis.pdf,CSCI_83,2,"Why Are Time Series Useful? “It’s tough to make predictions, especially about the future!”Karl Kristian Steincke, Danish politician, ca 1937
Demand forecasting: Electricity production, Internet bandwidth, Trafﬁc management, Inventory management, sales forecasting
Medicine: Time dependent treatment effects, EKG, EEG
Engineering and Science: Signal analysis, Analysis of physical processes
Capital markets and economics: Seasonal unemployment, Price/return series, Risk analysis",478,semantic
4d44e2f2-ee93-4090-b638-4057f97df43e,Forecasting And Time Series Analysis.pdf,CSCI_83,3,"Why Are Time Series Data Different? Models must account for time series behavior
Most statistical and machine learning assume data samples are independent identically distributed (iid)
But, this is not the case for time series data
Time series values are correlated in time
Time series data exhibit Serial correlation
Serial correlation of values
Serial correlation of errors
Violate iid assumptions of many statistical and ML Models`",434,semantic
9f625620-f5f6-45a7-b1d7-0dd923df92c2,Forecasting And Time Series Analysis.pdf,CSCI_83,4,"Why Are Time Series Data Different
Examples of series correlation:
Temperature forecasts, where the future values are correlated with the current values
The opening price of a stock is correlated with the price at the previous close
The daily sales volume of a product is correlated with the previous sales volume
A medical patient’s blood pressure reading is correlated with the previous observations",401,semantic
190a07c4-7eb3-41a0-92d2-d3c9687114ab,Forecasting And Time Series Analysis.pdf,CSCI_83,5,"History of Time Series Analysis
Time series analysis have a long history- Recognized the serial dependency in time series data early on- Joseph Fourier and Siemon Poisson worked on time series problems in the early 19th Century
Joseph Fourier",242,semantic
553ce4b5-8721-4173-92d4-2d9c6b2d7efb,Forecasting And Time Series Analysis.pdf,CSCI_83,6,"History of Time Series Analysis
Modern history of time series analysis started with George Udny Yule (1927) and Gilbert Walker (1931)
Yule worked on sunspot time series
Walker was attempting to forecast the tropical monsoon cycle
Developed the autoregressive (AR) model to account for serial correlation
The AR model is foundation of modern time series models
George Yule, time series pioneer",392,semantic
4137c46e-a733-4f5d-a42f-80d0fcbc6c38,Forecasting And Time Series Analysis.pdf,CSCI_83,7,"History of Time Series Analysis
Mathematical prodigy, Norbert Weiner, invented ﬁlters for stochastic time series processes, starting in the 1920s
Weiner’s ﬁlter theory is the basis of many time series ﬁlter methods
Predictive ﬁlters for noisy signals; not discussed here
Weiner process model for random walks is widely used
Norbert Weiner: Invented time series ﬁlters",367,semantic
9178ec2a-b4f4-42d4-8cfe-46325b457710,Forecasting And Time Series Analysis.pdf,CSCI_83,8,"History of Time Series Analysis
George Box and Gwilym Jenkins fully developed a statistical theory of time series by extending the work of Yule and Walker in the 1950s and 1960s
Extended the AR model to include moving average (MA) terms
Included the integrative term to create the ARIMA model
The ARIMA model is our focus
George EP Box created general time series model
Seminal time series analysis book",403,semantic
3bba8a53-bc84-4478-9763-14682d10b699,Forecasting And Time Series Analysis.pdf,CSCI_83,9,"History of Time Series Analysis
21st Century time series analysis
Considerable research continues to expand the frontiers
Bayesian time series models
R bsts package and Python PyMC3
Long short term memory (LSTM) model
Hidden Markov Models (HMMs) widely used
Python Scikit Learn HMM or R HMM package",298,semantic
69faa7fa-596e-4de4-ba0f-a8b1be0c163a,Forecasting And Time Series Analysis.pdf,CSCI_83,10,"Software for Time Series Analysis
Most statistical packages have considerable time series modeling capability
R time series analysis packages are wide and deep
Much leading edge research appears ﬁrst in R packages
CRAN Time Series Task View, maintained by Rob Hyndman, contains curated index to R time series packages
Primary Python time series analysis package in Statsmodels.tsa
Bayesian time series models supported in PyMC. Many newer Python time series packages packages, including:
Darts package includes cutting edge methods like hierarcical models
Facebook Kats package - strong in forecasting including the PROFIT model
GrayKite Linkedin’s forecasting package",668,semantic
6ab5fc33-1102-4cdb-85eb-70f47d6789b2,Forecasting And Time Series Analysis.pdf,CSCI_83,11,"Fundamentals of Time Series
What are the fundamental properties of time series
Representation and sampling
White noise series
Stationary time series
Autocorrelation and partial autocorrelation
Random walk series
Trend
Seasonal effects",234,semantic
2b491e92-6467-4269-aa4d-ccbe6ecacd95,Forecasting And Time Series Analysis.pdf,CSCI_83,12,"Time Series Representation
Time series are expressed as a time ordered sequence of values 
We work with discrete samples in time order
In regular time series the sample interval  is ﬁxed
Time measured from start of series 
Or, time measured within an interval, multiples of 
Even continuous time processes are sampled in practice
Temperature
Pressure
Home price
( , , ,…, )𝑥1𝑥2𝑥3 𝑥𝑛
Δ𝑡
(0,Δ𝑡,2Δ𝑡,…,𝑛Δ𝑡)
Δ𝑡",405,semantic
e3ac92aa-5cb9-4c21-853e-1a0696e79d61,Forecasting And Time Series Analysis.pdf,CSCI_83,13,"White Noise Series
White noise series are fundamental
Values are independent identically distributed (iid) Normally distributed
Can express values, , of a white noise series as:
No serial correlation between values
There is no predictive information in a white noise series
We want the residuals of time series models to be white noise series
( , , ,…, )𝑤1𝑤2𝑤3 𝑤𝑛
𝑋(𝑡)=( , , ,…, )𝑤1𝑤2𝑤3 𝑤𝑛
𝑤ℎ 𝑒𝑟𝑒∼ 𝑁(0, )𝑤𝑡 𝜎2",409,semantic
98d0b1c3-eafa-4679-980e-bd490882d838,Forecasting And Time Series Analysis.pdf,CSCI_83,14,"White Noise Series
What does a white noise series look like?",60,semantic
b0999923-1b85-4ad0-919f-ea8a70c2d07f,Forecasting And Time Series Analysis.pdf,CSCI_83,15,"White Noise Series
What does a white noise series look like? Each value is a sample is iid Normally distributed
No trend",120,semantic
064f8a5d-f365-45e0-be4e-e761c5867c00,Forecasting And Time Series Analysis.pdf,CSCI_83,16,"Stationary Time Series
A white noise series is stationary
A stationary time series has statistical properties constant in time
For example, a stationary time series has constant mean and variance over any sample interval
Many time series models require stationarity
Often transform time series to make them stationary
More on this shortly",338,semantic
cc909546-bf53-4936-8e5f-af01eab80d52,Forecasting And Time Series Analysis.pdf,CSCI_83,17,"Stationary Time Series
What tests can be used for stationarity? Plots
Qualitative
Nonstationarity from seasonality and trend are usually visible
The Dicky Fuller test
Hypothesis test for stationarity
Null hypothesis is stationarity
Based on roots of AR(1) model; to be discussed shortly",286,semantic
dd45d81b-e044-4773-8b0e-1a87c2c3eb24,Forecasting And Time Series Analysis.pdf,CSCI_83,18,"Autocorrelation Properties of White Noise Series
Can measure the correlation of a time series with itself
The time series is correlated at different time offsets
Each time step of offset is called a lag
The autocorrelation function (ACF) is measured between the series and the series lagged in time
Always 1 at lag 0; 𝐴𝐶𝐹(0)=1.0",328,semantic
e7392343-40c4-48e2-a3aa-8ae48b9830a9,Forecasting And Time Series Analysis.pdf,CSCI_83,19,"Autocorrelation Properties of White Noise Series
We compute the autocorrelation at lag :
Where:
Notice that for any series, 
Autocorrelation at each lag has values in the range 
𝑘
= = ( −𝜇)⋅ ( −𝜇)𝜌𝑘 𝛾𝑘
𝑛𝜎2 1𝑛𝜎2∑𝑡=1
𝑁
𝑦𝑡 𝑦𝑡−𝑘
𝑘𝑦𝑡
𝛾𝑘
𝜇
𝜎2
=𝑙𝑎𝑔=𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑜𝑛 𝑎𝑡 𝑡𝑖𝑚𝑒 𝑡=𝑐𝑜𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑙𝑎𝑔 𝑘=𝑚𝑒𝑎𝑛 𝑜𝑓 𝑡ℎ 𝑒 𝑠𝑒𝑟𝑖𝑒𝑠
=𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑜𝑓 𝑡ℎ 𝑒 𝑠𝑒𝑟𝑖𝑒𝑠= ( −𝜇)⋅ ( −𝜇)1𝑛−1Σ𝑁𝑡=1𝑦𝑡 𝑦𝑡
=1𝜌0
−1.0≥𝜌≥1.0",368,semantic
c77ef7ad-9691-44d6-b1a5-fecc4e1063e1,Forecasting And Time Series Analysis.pdf,CSCI_83,20,"Autocorrelation Properties of White Noise Series
The partial autocorrelation is another important property of time series
The partial autocorrelation function (PACF) is the residual autocorrelation once autocorrelation is accounted for
To compute the partial autocorrelation to lag :
Compute the autocorrelation to lag 
Remove the linearly predictable autocorrelation component of the time series
Compute the (partial) autocorrelation of the residual to lag 
The 0 lag value of the partial autocorrelation is always 1.0
𝑘
𝑘
𝑘",525,semantic
bebeedc0-6c22-4e52-9425-11548879a84f,Forecasting And Time Series Analysis.pdf,CSCI_83,21,"Autocorrelation Properties of White Noise Series
What are the autocorrelation and partial autocorrelation properties of a white noise series? The autocorrelation and partial autocorrelation are 0 for all 
Autocorrelation plot shows value at each lag selected
𝑘>0",262,semantic
81a50522-7cfb-4344-bbd0-82ecb62c132c,Forecasting And Time Series Analysis.pdf,CSCI_83,22,"Random Walk Time Series
Random walks are a commonly encountered properties of time series
Change in value of random walk series at one time step:
The next value in the random walk is then:
Or, with a little bit of algebra:
-  is the ith innovation-  observarition at time - A random walk is an integrative process; sum or integral of innovations
Note: innovations are referred to by other names:- Shocks in the stochastic process literature- Returns in ﬁnancial analytics
= −𝑤𝑡 𝑦𝑡 𝑦𝑡−1
= +𝑦𝑡 𝑦𝑡−1 𝑦𝑡
= +𝑦𝑡 𝑤𝑡 ∑𝑖=0
𝑡−1
𝑤𝑖
𝑤𝑖=𝑦𝑡 𝑡",528,semantic
aa93411c-e985-416a-ad02-c095f78a221b,Forecasting And Time Series Analysis.pdf,CSCI_83,23,"Random Walk Time Series
What does a random walk time series look like? Integrating innovations leads to a ‘drift-like’ behavior
No actual trend; random walk will eventually change apparent slope
Example with iid Normal innovations:",231,semantic
bccb455a-4593-480d-bd9e-3658bd9f6935,Forecasting And Time Series Analysis.pdf,CSCI_83,24,"Random Walk Time Series
Autocorrelation of white noise series dies slowly
Partial autocorrealtion nonzero at one lag",116,semantic
664e7256-aa06-48fa-82dc-76af1e3f69ec,Forecasting And Time Series Analysis.pdf,CSCI_83,25,"Random Walk Time Series
Random walk series is not Normally distributed",70,semantic
0321a978-b985-4c86-8fbf-9b6fb7089708,Forecasting And Time Series Analysis.pdf,CSCI_83,26,"Random Walk Time Series
Random walk time series are non-stationary
Consider the covariance of a time series at lag :
For a random walk, the increase in covariance is unbounded in time:
Unbounded and time dependent variance make a random walk non-stationary
𝑘
=𝐶𝑜𝑣(, )𝛾𝑘 𝑦𝑡 𝑦𝑡+𝑘
=𝐶𝑜𝑣(, )=𝑡 →∞ 𝑎𝑠 𝑡→∞𝛾𝑘 𝑦𝑡 𝑦𝑡+𝑘 𝜎2",311,semantic
e3ccbaf4-6bc3-423d-857f-67e49afda706,Forecasting And Time Series Analysis.pdf,CSCI_83,27,"Time Series With Trend
Many real-world time series have a long-term trend
A trend is a long term change in the mean value of the time series
Typically model trend as linear, polynomial, non-parametric splines, etc. PROFIT algorithm uses generalized additive model (GAM)
Consider an example of a white noise series with a linear trend",333,semantic
04d8b074-ed31-4e7c-b3f1-e97acc1ac513,Forecasting And Time Series Analysis.pdf,CSCI_83,28,"Time Series With Trend
Trend models are not just strait lines
Polynomial regression
Piece-wise polynomial regression - e.g. splines
Used in PROFIT algorithm
A generalized additive model
Local polynomial regression - e.g. LOESS
Used in Statsmodels",246,semantic
71674531-70f8-4f73-8f25-399df9cf0ecb,Forecasting And Time Series Analysis.pdf,CSCI_83,29,"Time Series With Trend
Time series with trend are non-stationary
Any time series with trend is non-stationary
Mean and variance are dependent of window used to compute them
The distribution of even a white noise series with trend is non-Normal",243,semantic
3ddfe829-24b2-43e5-863b-68e114721e48,Forecasting And Time Series Analysis.pdf,CSCI_83,30,"Time Series With Trend
ACF and PACF are only properly deﬁned for stationary series
For non-stationary series, the ACF dies off slowly
Integrative innovations lead to long-term dependency
The PACF dies off quickly with lag
Example: ACF and PACF of the white noise series with trend",280,semantic
b9d403c1-ad6d-4d41-be5a-550378604622,Forecasting And Time Series Analysis.pdf,CSCI_83,31,"Time Series With Seasonal Effects
Many (most?) real-world time series have seasonal effect
A seasonal effect has a measurable effect that occurs periodically
Examples of seasonal events include:
Day of the week
Last day of the month
Month of the year
Annual holiday
Option expiration date
Game day, e.g. Supper Bowl
Electrical impulses in a heart - EKG
Orbits of planets
Time series with seasonal effects are non-stationary
Mean and variance depends of sample window
…",468,semantic
e5f22abf-a99e-4f5f-bebb-41e451be51c6,Forecasting And Time Series Analysis.pdf,CSCI_83,32,"Time Series With Seasonal Effects
Use regression models for seasonal effects
Simple regression model:
Coefﬁcient for each interval in period; e.g. 12 coefﬁcients for monthly effects
Coefﬁcient for speciﬁc effect - e.g. date of holiday
Basis function regression
PROFIT algorithm uses Fourier basis functions
A generalized additive model",335,semantic
746c9ce3-1e11-4d8e-8111-ddabe2186161,Forecasting And Time Series Analysis.pdf,CSCI_83,33,"Time Series With Seasonal Effects
Example of a time series with a seasonal effect
A white noise series with trend and seasonal behavior
The seasonal behavior is periodic",169,semantic
38143d92-726f-45b0-afb7-77e6d7cfbb21,Forecasting And Time Series Analysis.pdf,CSCI_83,34,"Time Series Models
Many types of time series models; we will only consider the most widely used
Time series decomposition
Exponential models
ARIMA model
Serially correlated components
Integrative component",205,semantic
565ee5a2-4b80-4b6e-9f49-e2cb2d26d4ba,Forecasting And Time Series Analysis.pdf,CSCI_83,35,"Time Series Decomposition
Two possible models for seasonal effects
Goal, decompose the time series into its components
The Seasonal Trend decomonsition model using Loess (STL) model
Uses a nonparametric regression model to decompose time series into components
Components are seasonal (S), trend (T), and the residual (R)
Additive decomposition model
Multiplicative decomposition model
Differencing model",404,semantic
788ad550-08cf-4c32-b333-b959d82b8f80,Forecasting And Time Series Analysis.pdf,CSCI_83,36,"Time Series Decomposition
The additive decomposition model is expressed as as the sum of the components:
Used when seasonal effect is constant in time
Examples: Physical process
𝑇𝑆(𝑡)=𝑆(𝑡)+𝑇(𝑡)+𝑅(𝑡)",198,semantic
e4514a58-f68e-4dad-946d-eb3ec6bfd857,Forecasting And Time Series Analysis.pdf,CSCI_83,37,"Time Series Decomposition
The Multiplicative decomposition model is expressed as as the product of the components:
The multiplicative form is can be hard to work with, so log transform to additive model
Use when seasonal effect changes in time
Example, economic time series
𝑇𝑆(𝑡)=𝑆(𝑡)∗ 𝑇(𝑡)∗ 𝑅(𝑡)
𝑙𝑜𝑔(𝑇𝑆(𝑡))=𝑙𝑜𝑔(𝑆(𝑡))+𝑙𝑜𝑔(𝑇(𝑡))+𝑙𝑜𝑔(𝑅(𝑡))
= (𝑡)+ (𝑡)+ (𝑡)𝑆𝑙 𝑇𝑙 𝑅𝑙",361,semantic
ab091e80-4a2c-4af7-84da-d69370b6ab2f,Forecasting And Time Series Analysis.pdf,CSCI_83,38,"Time Series Decomposition
Example of addative STL decomposition of time series with linear trend and seasonal effect
The original series plot is on top
Notice the estimated trend is not a straight line; a result of noise
Residuals are relatively small and homoscedastic",269,semantic
9537618d-73ba-4b47-ba53-60a371425dfc,Forecasting And Time Series Analysis.pdf,CSCI_83,39,"Time Series Difference Operators
Is there an alternative for dealing with trend? How do we deal with random walks? Difference operators are useful for both cases
Difference operators return the innovations
Difference operators can be of any order in principle
Typically use ﬁrst order differences
Difference operator of order n computes a series n shorter than original
∇ = −𝑦𝑡 𝑦𝑡 𝑦𝑡−1",385,semantic
91053a1f-07a2-42bf-be64-6d7d44393eae,Forecasting And Time Series Analysis.pdf,CSCI_83,40,"Time Series Difference Operators
Example of a ﬁrst order difference operator applied to random walk
The innovations look random
Need to verify statistical properties",165,semantic
0ef0f61f-a178-4283-8fcc-1821801fcb91,Forecasting And Time Series Analysis.pdf,CSCI_83,41,"Time Series Difference Operators
Statistical properties of the difference series
Compute the ACF and PACF
The plots indicate the difference series is white noise",161,semantic
19d30777-079c-494e-bcc2-4d077f7b7055,Forecasting And Time Series Analysis.pdf,CSCI_83,42,"Time Series Forecasting Models
Forecasting is the goal of much of time series analysis
Exponential models; extrapolation from simple smoothers
ARIMA and SARIMAX models; time series linear models
For comprehensive introduction see Forecasting: Principles and Practice, Hyndman and Athanaosopoulos, 3rd edition, 2018, available as book or free online
Rob Hyndman’s blog is a source of many interesting ideas and example in time series analysis",441,semantic
b8aed954-6b6d-41bf-933c-07888a036d10,Forecasting And Time Series Analysis.pdf,CSCI_83,43,"Exponential Smoothing Models
Exponential smoothing models are simple and widely used
Consider the simple ﬁrst order model
Set initial conditions:
The smoothed update is:
And, the smoothing coefﬁcient is, 
But, model only works if no trend
=𝑠0 𝑦0
=𝛼 +(1−𝛼)𝑠𝑡 𝑦𝑡 𝑠𝑡−1
= 𝛼( − ),𝑠𝑡−1 𝑦𝑡 𝑠𝑡−1
𝑡>0
0≤𝛼≤1",297,semantic
96b4162e-c1e6-4f7c-a9bf-ba06e9fee8d9,Forecasting And Time Series Analysis.pdf,CSCI_83,44,"Exponential Smoothing Models
Decay and exponential smoothing
We can understand the smoothing parameter  in terms of a decay constant, 
An innovation or shock has an effect for all future time
Effect decays exponentially with time, 
𝛼 𝜏
𝛼=1−𝑒( )Δ𝑇𝜏
Δ𝑇",250,semantic
3f1e8291-5ca6-41f2-8669-0107b46f2640,Forecasting And Time Series Analysis.pdf,CSCI_83,45,"Exponential Smoothing Models
Can extend exponential smoothing model to accommodate trend
Algorithm known as double exponential smoothing or Holt-Winters double exponential smoothing
Update smoothed values and slope at each time step
Start with initial values
Update relationships for both smoothed value and slope
Additional slope smoothing hyperparameter, 
Use third order update includes seasonality in Holt-Winters smoother
=𝑠1 𝑦1
= −𝑏1 𝑦2 𝑦1
=𝛼 +(1−𝛼)( + )𝑠𝑡 𝑦𝑡 𝑠𝑡−1 𝑏𝑡−1
=𝛽( − )+(1−𝛽)𝑏𝑡 𝑠𝑡 𝑠𝑡−1 𝑏𝑡−1
0≤𝛽≤1",510,semantic
c430da55-866f-47f2-88a2-ad879ea83455,Forecasting And Time Series Analysis.pdf,CSCI_83,46,"Exponential Smoothing Models
Exponential smoothing models are useful for forecasting
Forecast dependent on the choice of smoothing parameters
Can forecast with ﬁrst, second, third order models
For second order model (with trend) the forecast  steps ahead is:
Third order update include seasonal terms
Holt-Winters smoother is a linear model! 𝑚
= +𝑚𝐹𝑡+𝑚 𝑠𝑡 𝑏𝑡",358,semantic
475a4067-9c8b-4b92-a0c3-943c9cbd9c94,Forecasting And Time Series Analysis.pdf,CSCI_83,47,"Exponential Smoothing Models
Example of smoothing trend plus white noise series
Decreasing the smoothing parameter, , increases smoothing
Additionally, smooth trend
Additional examples in Statsmodels user documentation
𝛼",220,semantic
665a5cbd-927e-4167-873a-29a6c4d97a3b,Forecasting And Time Series Analysis.pdf,CSCI_83,48,"The ARIMA and SARIMAX Model
The ARIMA model is composed three components:
Autoregressive component (AR) accounts for partial autocorrelation
Serial correlation of observatons
Integrative component (I) accounts random walks and trend
Moving Average (MA) accounts for autocorrelation
Serial correlation of model error
SARIMAX model adds:
Seasonal components (S)
Exogenous variables (X)",383,semantic
0b6f39b3-dcc8-4000-8682-cbdda7f022c4,Forecasting And Time Series Analysis.pdf,CSCI_83,49,"The Autoregressive Model
Autoregressive model relates past observed values to the current value
An autoregressive model of order , , uses the last p observations:
An AR process has the following properties:
 always
Number of nonzero PACF values 
A shock at any time will affect the result as 
AR model assume stationary time series
𝑝𝐴𝑅(𝑝)
= + ,…, +𝑥𝑡 𝜙1𝑦𝑡−1 𝜙2𝑦𝑡−2 𝜙𝑝𝑦𝑡−𝑝 𝑤𝑡
𝑤ℎ 𝑒𝑟𝑒𝜙𝑘
𝑤𝑡
𝑦𝑡
=𝑚𝑜𝑑𝑒𝑙 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑎𝑡 𝑙𝑎𝑔 𝑘=𝑤ℎ 𝑖𝑡𝑒 𝑛𝑜𝑖𝑠𝑒 𝑒𝑟𝑟𝑜𝑟 𝑎𝑡 𝑡𝑖𝑚𝑒 𝑡; ∼ 𝙽(0, )𝜎2
=𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑡 𝑡𝑖𝑚𝑒 𝑡
=1𝜌0
=𝑝𝑘 𝜙𝑘
=𝑝
𝑡→∞",500,semantic
8d888cb4-6b38-4080-b542-3a1944d07ace,Forecasting And Time Series Analysis.pdf,CSCI_83,50,"The Autoregressive Model
How can we understand the AR model? Consider an AR(2) model
The value of  is a weighted sum of  previous values plus an error term
Illustration of the AR(2) model
𝑦𝑡 𝑘",192,semantic
332d8ed5-0ce3-4b23-9c5a-0e25bdab7e6b,Forecasting And Time Series Analysis.pdf,CSCI_83,51,"The Autoregressive Model
How can we understand the AR model? Model matrix of AR(2) model
AR model is a linear model! For coefﬁcient vector, , solve linear system:
𝐴=
⎡
⎣
⎢⎢⎢⎢⎢⎢⎢⎢⎢
,  , 𝑦𝑡 𝑦1−1 𝑦𝑡−2
,  , 𝑦𝑡−1 𝑦1−2 𝑦𝑡−3
,  , 𝑦𝑡−2 𝑦1−3 𝑦𝑡−4
⋮ ,     ⋮ ,     ⋮ ,     ,    𝑦2 𝑦1 𝑦0
,        0𝑦1 𝑦0
,    0,    0𝑦0
⎤
⎦
⎥⎥⎥⎥⎥⎥⎥⎥⎥
Φ=[ , ,…, ]𝜙1𝜙2 𝜙𝑝
𝑌=𝐴Φ",344,semantic
3c89feb7-a267-4c24-9673-5400c5d83646,Forecasting And Time Series Analysis.pdf,CSCI_83,52,"The Autoregressive Model
We can rewrite the AR(1) model in terms of exceptions:
The AR model is unstable for the roots of the polynomial 
To be a stable AR process, 
Violation of this condition leads to unstable model! 𝔼()= 𝔼( )+𝔼()𝑦𝑡 𝜙𝑡 𝑦𝑡−1 𝜖𝑡𝑜𝑟𝜇=𝑐+𝜇+0𝜙𝑡
𝑡ℎ 𝑒𝑟𝑒𝑓𝑜𝑟𝑒
𝜇= 𝑐1−𝜙2𝑡
1−𝜙2𝑡
≤1𝜙2𝑡",289,semantic
77403a5e-29db-4b64-a8a9-5c30ea0ca7a0,Forecasting And Time Series Analysis.pdf,CSCI_83,53,"The Autoregressive Model
Example of AR(2) time series with coefﬁcients :
Time series looks a bit random
But, notice the statistical properties; ACF, PACF
PACF has 2 non-zero lag values, so 
=(1.0,0.75,0.25)
𝑝=2",210,semantic
53de7b5b-6387-4657-864e-244fd0f4179e,Forecasting And Time Series Analysis.pdf,CSCI_83,55,"The Autoregressive Model
Example model summary for AR(2) model:
Both AR coefﬁcients are statistically signiﬁcant
Variance term is statistically signiﬁcant",154,semantic
b89ba94c-2b08-46ca-bbba-2703142c4522,Forecasting And Time Series Analysis.pdf,CSCI_83,56,"The Moving Average Model
A moving average model of order , , uses the last q error terms or shocks:
An MA process has the following properties:
For autocorrelation,  always
Number of , , 
Shocks die off quickly in MA processes
MA model assumes stationary time series
𝑞𝑀𝐴(𝑞)
=𝜇+ + + +…+𝑦𝑡 𝜖𝑡 𝜃1𝜖𝑡−1 𝜃2𝜖𝑡−2 𝜃𝑞𝜖𝑡−𝑞
𝑤ℎ 𝑒𝑟𝑒𝜃𝑘
𝑦𝑡
𝜖𝑡
=𝑚𝑜𝑑𝑒𝑙 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑎𝑡 𝑙𝑎𝑔 𝑘=𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑡 𝑡𝑖𝑚𝑒 𝑡=𝑖𝑛𝑛𝑜𝑣𝑎𝑡𝑖𝑜𝑛 𝑜𝑟 𝑒𝑟𝑟𝑜𝑟 𝑎𝑡 𝑡𝑖𝑚𝑒 𝑡; ∼ 𝙽(0, )𝜎2
=1𝜌0
≠0𝜌𝑘 =𝑞𝑘>0",434,semantic
5ebc8783-500a-4fd3-91c6-e7647de5d904,Forecasting And Time Series Analysis.pdf,CSCI_83,57,"The Moving Average Model
How can we understand the MA model? Model matrix of MA(2) model
MA model is a linear model! But, the value of  dependents on 
The s are unobservable!",174,semantic
5255ce4d-5c22-47dc-946c-3278137b3a7a,Forecasting And Time Series Analysis.pdf,CSCI_83,57,"So, ﬁtting requires nonlinear iteratively rewieighted least squares
𝐴=
⎡
⎣
⎢⎢⎢⎢⎢⎢⎢⎢⎢
,  , 𝑦𝑡 𝜖1−1 𝜖𝑡−2
,  , 𝑦𝑡−1 𝜖1−2 𝜖𝑡−3
,  , 𝑦𝑡−2 𝜖1−3 𝜖𝑡−4
⋮ ,     ⋮ ,     ⋮ ,     ,    𝑦2 𝜖1 𝜖0
,        0𝑦1 𝜖0
,    0,    0𝑦0
⎤
⎦
⎥⎥⎥⎥⎥⎥⎥⎥⎥
𝜖𝑡 [ , ,..., ]𝜖𝑡−1𝜖𝑡−2 𝜖𝑡−𝑞
𝜖𝑘",256,semantic
14d884df-98f0-48b3-9171-3740aecf2986,Forecasting And Time Series Analysis.pdf,CSCI_83,58,"The Moving Average Model
Example of an MA(1) model with coefﬁcients 
The time series looks fairly random
The ACF has 1 statistically signiﬁcant nonzero lag value
(1,−0.75)",171,semantic
1d6d5b84-51da-4eb1-82e5-26616148afdc,Forecasting And Time Series Analysis.pdf,CSCI_83,60,"The Moving Average Model
Example model summary for MA(1) model:
The MA coefﬁcient is statistically signiﬁcant
Notice that true value is within the conﬁdence interval
Conﬁdence interval is wide",192,semantic
aeb10f31-bf45-47c8-87d6-fc0d2c217997,Forecasting And Time Series Analysis.pdf,CSCI_83,61,"Autoregressive Moving Average Model
We can combine AR and MA terms to create the autoregressive moving average (ARMA) model of order :
Fit ARMA model by solving a nonlinear equatioin:
Can write as polynomial equation in terms of coefﬁcient vectors , :
Model is linear in coefﬁcients
ARMA model assumes stationary time series
(𝑝,𝑞)
= + ,…,+ + + + +…+𝑦𝑡 𝜙1𝑦𝑡−1 𝜙2𝑦𝑡−2 𝜙𝑝𝑦𝑡−𝑝 𝜖𝑡 𝜃1𝜖𝑡−1 𝜃2𝜖𝑡−2 𝜃𝑞𝜖𝑡−𝑞
− − ,…,− = + + +…+𝑦𝑡 𝜙1𝑦𝑡−1 𝜙2𝑦𝑡−2 𝜙𝑝𝑦𝑡−𝑝 𝜖𝑡 𝜃1𝜖𝑡−1 𝜃2𝜖𝑡−2 𝜃𝑞𝜖𝑡−𝑞
Φ=[1, , ,…, 𝑦]𝜙1𝜙2 𝜙𝑝 Θ=[1, , ,…, ]𝜃1𝜃2 𝜃𝑞
(1−Φ)𝑌=Θ𝜖",515,semantic
ed17bc92-df3d-4d13-8baf-3ee8228eab24,Forecasting And Time Series Analysis.pdf,CSCI_83,62,"The ARIMA Model
The integrative model addresses certain non-stationary components of a time series
Random walks
Trends
Based on difference operator
Typically ﬁrst order difference
Can be higher order
Is deterministic, no model coefﬁcient to estimate",249,semantic
595549c1-716d-4d28-a9ae-7c834b61fce7,Forecasting And Time Series Analysis.pdf,CSCI_83,63,"The ARIMA Model
The autoregressive integrative moving average (ARIMA) model includes AR, integrative and MA terms
The order of an ARIMA is speciﬁed as (p,d,q)
p is the AR order
d is the order of differencing
q is the MA order
The integrative term helps transforms trend and random walks to stationary process
Does not account for seasonal effect
For difference values, , formulate as:∇ 𝑦𝑡
(1−Φ)∇ 𝑌=Θ𝜖",400,semantic
81a33db1-5ee9-4832-be84-7bd266a1d7a4,Forecasting And Time Series Analysis.pdf,CSCI_83,64,"Seasonal Models
Several possible seasonal models
Seasonal effects can be periodic or single event (e.g. holiday, game day, etc.)
Linear regression model to ﬁnd effect for each time step in period
STL decomposition
SARIMAX, the S term
Each model requires:
Known period of the cycle or time of seasonal event
Additive or logarithmic transformation",345,semantic
37036c31-96d8-476c-aa92-1f32d2a65ac4,Forecasting And Time Series Analysis.pdf,CSCI_83,65,"SARIMAX Model
The SARIMAX model adds seasonal and exogenous terms
ARIMA terms are same, (p,d,q)
Seasonal terms:
ARIMA model, order (P,D,Q,S)
Must specify period, S
Order of SARIMAX model is speciﬁed as (p,d,q)(P,D,Q,S)
See Statsmodels State Space User Guide for more details and examples",287,semantic
b173f862-03f3-4c36-9042-cacb1179264d,Forecasting And Time Series Analysis.pdf,CSCI_83,66,"SARIMAX Model
The SARIMAX model (with no exogenous variables) is formulated
 and  are the AR polynomials non-seasonal and seasonal terms
 and  are the MA polynomials non-seasonal and seasonal terms
 and  are the non-seasonal and seasonal differencing operators
 is the trend term
(𝑌) ( ) =𝐴(𝑡)+(𝑌) ( )𝜙𝑝 𝜙̃ 𝑃𝑌∗ ∇ 𝑑∇ 𝐷𝑦𝑡 𝜃𝑞 𝜃̃ 𝑄𝑌∗ 
(𝑌)𝜙𝑝 ( )𝜙̃ 𝑃𝑌∗ 
(𝑌)𝜃𝑞 ( )𝜃̃ 𝑄𝑌∗ 
∇ 𝑑 ∇ 𝐷
𝐴(𝑡)",377,semantic
8bd97dfa-0e27-4448-ab5d-182347eb69e3,Forecasting And Time Series Analysis.pdf,CSCI_83,67,"SARIMAX Model
SARIMAX model can include exogenous variables, , leading to a new system of equations”:
Time series model for latent variable, 
 acts as the intercept term for the regression model for 
The coefﬁcient vector, , contains the effect sizes for the exogenous variables
𝑥
= +𝑦𝑡 𝛽𝑡𝑥𝑡 𝜇𝑡
(𝑌) ( ) =𝐴(𝑡)+(𝑌) ( )𝜙𝑝 𝜙̃ 𝑃𝑌∗ ∇ 𝑑∇ 𝐷𝜇𝑡 𝜃𝑝 𝜃̃ 𝑃𝑌∗ 
𝜇𝑡
𝜇𝑡 𝑥𝑡
𝛽",356,semantic
d808d2e3-58ec-4e10-a625-226c1a923be8,Forecasting And Time Series Analysis.pdf,CSCI_83,68,"Evaluating and Comparing Time Series Models
How can we evaluate time series models? RMSE; compare forecast to actual values
Could use log-likelihood; 
Use score function 
But, score will decrease with model complexity
Need to adjust for number of model parameters
We always prefer simpler models; fewer parameters to learn
Akaki Information Criteria (AIC)
Bayes Information Criteria (BIC)
𝑙𝑜𝑔(𝑝(𝑋|𝜃))
𝜃=𝑚𝑜𝑑𝑒𝑙 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠
=−2 𝑙𝑜𝑔(𝑙𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑)=−2 𝑙𝑜𝑔(𝑝(𝑋|𝜃))",455,semantic
ba2e5886-5bd5-4f33-a063-7f6679d434e1,Forecasting And Time Series Analysis.pdf,CSCI_83,69,"Evaluating and Comparing Time Series Models
Akaki Information criteria, AIC
AIC penalizes the score function for the complexity of the model by 
Model with lowest AIC is best
𝐴𝐼𝐶=2 𝑘−2 𝑙𝑛()𝐿̂ 𝑤ℎ 𝑒𝑟𝑒=𝑡ℎ 𝑒 𝑙𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑 𝑔𝑖𝑣𝑒𝑛 𝑡ℎ 𝑒 𝑓𝑖𝑡𝑡𝑒𝑑 𝑚𝑜𝑑𝑒𝑙 𝑝𝑎𝑟𝑚𝑎𝑡𝑒𝑟𝑠 =𝑝(𝑥| )𝐿̂  𝜃̂  𝜃̂ 𝑥=𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑎𝑡𝑎𝑘=𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑚𝑜𝑑𝑒𝑙 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠
2 𝑘",315,semantic
411ef8ae-49c9-4d94-8729-8f1c80feecd4,Forecasting And Time Series Analysis.pdf,CSCI_83,70,"Evaluating and Comparing Time Series Models
Bayes Information criteria, BIC
BIC penalizes the score function for the complexity of the model, 
BIC adjusts for number of samples used to learn the  model parameters
Model with lowest BIC is best
BIC is often preferred to AIC for time series models
𝐵𝐼𝐶=𝑙𝑛(𝑛) 𝑘−2 𝑙𝑛()𝐿̂ 𝑤ℎ 𝑒𝑟𝑒=𝑡ℎ 𝑒 𝑙𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑 𝑔𝑖𝑣𝑒𝑛 𝑡ℎ 𝑒 𝑓𝑖𝑡𝑡𝑒𝑑 𝑚𝑜𝑑𝑒𝑙 𝑝𝑎𝑟𝑚𝑎𝑡𝑒𝑟𝑠 =𝑝(𝑥| )𝐿̂  𝜃̂  𝜃̂ 𝑥=𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑎𝑡𝑎𝑘=𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑚𝑜𝑑𝑒𝑙 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠𝑛=𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛𝑠
𝑘
𝑘",464,semantic
42529490-565c-4c61-ad5c-5a094455ec67,Forecasting And Time Series Analysis.pdf,CSCI_83,71,"Evaluating and Comparing Time Series Models
Can compare and select models using BIC or AIC
Backwards step-wise model selection
1. Start with initial order of the model; e.g. 2.",176,semantic
0e5e5c1f-7c14-403c-ba24-5fe2d9c72b5a,Forecasting And Time Series Analysis.pdf,CSCI_83,71,"Fit (learn) the model parameters
3. compute the BIC, and if reduced consider this a better model
4. Reduce the order of one of the model components
5. Repeat steps 2, 3 and 4 until no further improvement
Tips for comparing models:
BIC and AIC are approximations; small changes (3rd or 4th decimal) are not important
If close tie for best model pick the simpler (lower order) case
Often best to consider integrative terms,  and , separately
(𝑝,𝑑,𝑞)(𝑃,𝐷,𝑄,𝑆)
𝑑 𝐷",460,semantic
86709bea-b4f8-4c84-9c64-d694a611d5a8,Forecasting And Time Series Analysis.pdf,CSCI_83,72,"SARIMAX Example
Example: 3 time series of Australian production",63,semantic
5b3ce8eb-2299-464f-9bc9-1d69de1cd880,Forecasting And Time Series Analysis.pdf,CSCI_83,74,"SARIMAX Example
Use the SARIMAX model to ﬁnd the best ARIMA ﬁt of log(electric production)
Log_electric = CBE.elec_log[:'1989-12-31']
best_model = pm.auto_arima(Log_electric, start_p=1 , start_q=1 ,                             max_p=3 , max_q=3 , m=1 2 ,                             start_P=0 , seasonal=True,
                             d=1 , D=1 , trace=True,                             information_criterion = 'bic',                             error_action='ignore',  # don't want to know if an order does not work                             suppress_warnings=True,  # don't want convergence warnings
                             stepwise=True)  # set to stepwise",670,semantic
a1a941aa-b79a-44d0-9b71-f48045c46e09,Forecasting And Time Series Analysis.pdf,CSCI_83,75,"SARIMAX Example
Example of SARIMAX model of order (0.1.1)(0,1,2,12) for monthly electric production series
Model selected by backwards step-wise method
First order model integrative term and MA(1)
First order model integrative term and MA(1) for period 12 seasonality",267,semantic
6e0464a1-14eb-4538-ba3c-8159b7b731ed,Forecasting And Time Series Analysis.pdf,CSCI_83,76,"SARIMAX Example
Predictions for the last 12 months of the time series
prediction = pd.Series(best_model.predict(n_periods=1 2 ), 
                       index = pd.date_range(start = '1990-01-31', end = '1990-12-31', freq = 'M'))",229,semantic
fbea2386-4bc2-45de-9e36-a12b851ee468,Forecasting And Time Series Analysis.pdf,CSCI_83,78,"SARIMAX Example
Residuals of the predictions
residuals = CBE.elec_log['1990-01-31':] - prediction
fig, ax = plt.subplots(nrows=1 , ncols=1 , figsize=(4 , 4 ))_=ss.probplot(residuals, plot = ax);
plt.show()",205,semantic
8c5a4dc2-04a2-4b03-911c-0c5a07f57e1e,Forecasting And Time Series Analysis.pdf,CSCI_83,79,"Summary
Fundamental elements of time series
Fundamental components which cannot be predicted
White noise
Random walks
Autocorrelation and partial autocorrelation
Trend
Seasonal components
Stationarity properties; Dicky Fuller test",230,semantic
d58bb260-881d-4b75-9a07-e4c15c7e635f,Forecasting And Time Series Analysis.pdf,CSCI_83,80,"Summary
Time series models must account for serial correlation
Exponential models; e.g. Holt-Winters
Second order accounts for trend
Third order accounts for trend and seasonal",176,semantic
a585324c-308c-4ad3-93f4-8e656c5a4e78,Forecasting And Time Series Analysis.pdf,CSCI_83,81,"Summary
Time series models must account for serial correlation
e.g. ARIMA and SARIMAX
AR components for serial correlation of values
MA components for serial correlation of errors
Integrative components for random walk and trend, I
Seasonal, (P,D,Q,S)
Exogenous variables, X",274,semantic
2a755141-dcd7-4a82-b637-b43713c767a1,Forecasting And Time Series Analysis.pdf,CSCI_83,82,"Summary
Evaluation and model comparison
RMSE
AIC and BIC, penalize score function for model complexity
Use BIC (or AIC) to perform backwards step-wise model selection",166,semantic
c05f547a-9b98-4b01-b736-261970be2266,Sampling and Simulation.pdf,CSCI_83,0,"Sampling and Simulation
Steve Elston
09/22/2022",47,semantic
f9edb31a-3384-4c7c-bc8b-61b6126ab85b,Sampling and Simulation.pdf,CSCI_83,1,"Review
Axioms of probability; for discrete distribution
Expectation
0≤𝑃(𝐴)≤1
𝑃(𝑆)= 𝑃()=1∑∈ 𝐴𝑎𝑖
𝑎𝑖
𝑃(𝐴 ∪ 𝐵)=𝑃(𝐴)+𝑃(𝐵)𝑖𝑓 𝐴⊥ 𝐵
E[𝐗]=  𝑝()∑𝑖=1
𝑛
𝑥𝑖 𝑥𝑖",146,semantic
3ec5f980-cbd9-48e8-8221-ef8be08832f0,Sampling and Simulation.pdf,CSCI_83,2,"Review
The Categorical distribution - Discrete multi-variate distribution
For outcome  we one hot encode the results as:
For a single trial the probabilities of the  possible outcomes are expressed:
Probability mass function as:
Multivariate Normal distribution, parameterized by n-dimensional vector of locations,  and  x  dimensional covariance matrix
𝑖
=(0,0,…,1,…,0)𝐞𝐢
𝑘
Π=( , ,…, )𝜋1𝜋2 𝜋𝑘
𝑓(|Π)=𝑥𝑖 𝜋𝑖
𝜇⃗  𝑛 𝑛
𝑓()= 𝑒𝑥𝑝( (− 𝚺(−))𝐱⃗  1(2𝜋|𝚺|)𝑘√ 12𝐱⃗ 𝜇⃗ )𝑇 𝐱⃗ 𝜇⃗",463,semantic
174be24a-d82d-4cb8-a9f5-dfbd55e22e3a,Sampling and Simulation.pdf,CSCI_83,3,"Review
Conditional probability
One random variable depends on another
But not commutable
Mutually exclusivity
Independence
𝑃(𝐴|𝐵)=𝑃(𝐴)⇎𝑃(𝐵|𝐴)=𝑃(𝐵)
𝑃(𝐴|𝐵)=𝑃(𝐴)+𝑃(𝐵)
𝑃(𝐴|𝐵)=𝑃(𝐴)",175,semantic
6d1bdd97-b388-4177-b5c5-e3954c8dda3f,Sampling and Simulation.pdf,CSCI_83,4,"Review
Bayes’ theorem
Marginal distribution
For continuous distributon
For discrete distribution
𝑃(𝐴|𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)𝑃(𝐵)
𝑝()= 𝑝(, ,…, ) 𝑑𝜃2,…,𝑑𝜃1 ∫,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛 𝜃𝑛
𝑝()= 𝑝(, ,…, )𝜃1 ∑,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛",199,semantic
288e0326-2eec-40c5-9fe1-279c57d48e34,Sampling and Simulation.pdf,CSCI_83,5,"Introduction
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases, the standard error decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation
We will take up the topic of resampling later",645,semantic
741c2a0f-b376-45b7-96c0-cac4ce97ede0,Sampling and Simulation.pdf,CSCI_83,6,"Sampling Example
Use Case Sample Population
A/B Testing The users we show either web sites A or B All possible users, past present and future
World Cup Soccer 32 teams which qualify in one season All national teams in past, present and future years
Average height of data science students Students in a data science class All students taking data science classes world wide
Tolerances of a manufactured part Samples taken from production lines All parts manufactured in the past, present andfuture
Numbers of a species in a habitat Population counts from sampled habitats All possible habitats in the past, present and future
In several cases it is not only impractical, but impossible to collect data from the entire population
We nearly always work with samples, rather than the entire population.",799,semantic
03393fcd-7172-459b-a9b8-817b34c2366b,Sampling and Simulation.pdf,CSCI_83,7,"Importance of Random Sampling
All statistical methods rely on the use of randomized unbiased samples
Failure to randomized samples violates many key assumptions of statistical models
An understanding of proper use of sampling methods is essential to statistical inference
Most commonly used machine learning algorithms assume that training data are unbiased and independent and identically distributed(iid)
These conditions are only met if training data sample is randomized
Otherwise, the training data will be biased and not represent the underlying process distribution",572,semantic
22dab002-90d4-4e41-861a-d6041066b060,Sampling and Simulation.pdf,CSCI_83,8,"Sampling Distributions
Sampling of a population is done from an unknown population distribution, 
Any statistic, , we compute for the generating process is based on a sample, 
The statistic is an approximation, of a population parameter
For example, the mean of the population is 
But, sample estimate is 
If we continue to take random samples from the population and compute estimates of a statistic, we generate a sampling distribution
Hypothetical concept of the sampling distribution is a foundation of frequentist statistics
Example, if we continue generating samples and computing the sample means,  for the ith sample
Frequentist statistics built on the idea of randomly resampling the population distribution and recomputing a statistic
In the frequentist world, statistical inferences are performed on the sampling distribution
Sampling process must not bias the estimates of the statistic
 
𝑠  ̂ 
𝜇
𝑥¯
𝑥¯𝑖",915,semantic
5462dd3e-2b2b-4b86-b92f-f5cdc3265034,Sampling and Simulation.pdf,CSCI_83,9,"Sampling Distributions
Sampling of a population is done from an unknown population distribution, 
Any statistic, , we compute for the generating process is an approximation for the population, 
Sampling distribution of unknown population parameter
 
𝑠 𝑠() ̂",257,semantic
917ba4e4-1a1c-4ef1-b25a-b0c5fb30ee93,Sampling and Simulation.pdf,CSCI_83,10,"Sampling and the Law of Large Numbers
The weak law of large numbers is a theorem that states that statistics of independent random samples converge to the populationvalues as more samples are used
Example, for a population distribution, , the sample mean is:
Then by the weak law of large numbers:
This result is reassuring, the larger the sample the more the statistic converges to the population parameter
 (𝜇,𝜎)
𝐿𝑒𝑡  =𝑋¯ 1𝑛∑𝑖=1
𝑛
𝑋𝑖
→𝐸(𝑋)=𝜇𝑋¯ 𝑎𝑠𝑛→∞",451,semantic
5326052d-cae8-47dd-8dc6-434486c4ecff,Sampling and Simulation.pdf,CSCI_83,11,"Sampling and the Law of Large Numbers
The law of large numbers is foundational to statistics
We rely on the law of large numbers whenever we work with samples
Assume that larger samples are more representatives of the population we are sampling
Is foundation of sampling theory, plus modern computational methods; simulation, bootstrap resampling, and Monte Carlo methods
If the real world did not follow this theorem, then much of statistics (along with much of science and technology) would have to be rethought",513,semantic
1ba6a164-d152-4333-8e12-9757cee123fe,Sampling and Simulation.pdf,CSCI_83,12,"Sampling and the Law of Large Numbers
The weak law of large numbers has a long history
Jacob Bernoulli posthumously published the ﬁrst proof for the Binomial distribution in 1713
Law of large numbers is sometimes referred to as Bernoulli’s theorem
A more general proof was published by Poisson in 1837.",302,semantic
8a49d283-daf2-4d4e-95c9-545d7efc4aaa,Sampling and Simulation.pdf,CSCI_83,13,"Sampling and the Law of Large Numbers
A simple example
The mean of fair coin ﬂips (0,1) = (T,H) converges to the expected value with more ﬂips
The mean converges to the expected value of 0.5 for 
## 5  0.80## 50  0.36## 500  0.48## 5000  0.49
𝑛=5,50,500,5000",258,semantic
01ea0583-b8db-4422-aa35-3bf74a215b16,Sampling and Simulation.pdf,CSCI_83,14,"Sampling and the Law of Large Numbers
A simple example; mean of fair coin ﬂips (0,1) = (T,H) converges to the expected value with more ﬂips
Convergance of mean estimates for fair coin",183,semantic
a865e0c5-4d42-4f91-928f-87cb1c9be91d,Sampling and Simulation.pdf,CSCI_83,15,"the Central Limit Theorem (CLT)
Law of large number is almost too obvious, but the CLT is more tricky! Law of large number applied to any statistic, but the CLT applies only to the mean
Let  be a random variable representing the population
 is allowed to have any distribution (not limited to normal), and let  be your true population mean and  the true population standard deviation
Given sample size  thee sampling distribution of is
𝑋
𝑋 𝜇 𝜎
𝑛 𝑋¯
∼ 𝑁(𝜇, )𝑋¯ 𝜎𝑛√",463,semantic
e9269e0c-42af-4d7c-8c9e-e6ec77ca6ebd,Sampling and Simulation.pdf,CSCI_83,16,"importance of CLT
CLT is a sort of guarantee
Sampling distribution of mean estimates do not depend on the population the sample was drawn from
Standard deviation  of the sampling distribution of  converges as 
Only depends on the population’s mean and variance, and on the sample size
CLT is the basis for hypothesis testing
𝑠 𝑥¯ 1/𝑛√",334,semantic
1e68a6a4-7b36-4f62-a961-207eb7ec2573,Sampling and Simulation.pdf,CSCI_83,17,"Example of CLT
Start with a mixture of Normal distributions",59,semantic
f1506131-4806-4188-9a13-b1a19864441b,Sampling and Simulation.pdf,CSCI_83,18,"Example CLT
Sample distribution of the mean of mixture of Normals is Normally distributed! Repetitively random sample the population, 
Compute the mean estimate,  for each sample
𝑠𝑖𝑧𝑒=50
𝑥¯
x_means = np.array([        nr.choice(x, size=5 0 , replace=True).mean()        for i in range(5 0 0 )])
breaks = np.linspace(x_means.min(), x_means.max(), num=4 0 )fig, ax = plt.subplots(1 ,2 , figsize=(1 2 , 5 ) ) _ = ax[0 ].hist(x_means, bins=breaks)
_ = sm.qqplot(x_means, line='s', ax=ax[1 ])",487,semantic
1d959f33-da9f-443a-a4b4-144eefbc1acf,Sampling and Simulation.pdf,CSCI_83,19,"Example CLT
Sample distribution of the mean of mixture of Normals is Normally distributed!",90,semantic
c4f75ebc-f8ca-4ff0-9fac-afbd4f840b3a,Sampling and Simulation.pdf,CSCI_83,20,"Standard Error and Convergence for a Normal Distribution
As we sampled from a Normal distribution, the sample means converges to the population mean
What can we say about the expected error of the mean estimate as the number of samples increases? Population has standard deviation 
This measure is known as the standard error of the sample mean
By the CLT the standard error is deﬁned:
Standard error decreases as the square root of 
Example, if you wish to halve the error, you will need to sample four times as many values. For the mean estimate, , deﬁne the uncertainty in terms of conﬁdence intervals
For 95% conﬁdence interval:
𝜎
𝑠𝑒=±𝜎(𝑛)√
𝑛
𝑥¯
𝐶 =±1.96 𝑠𝑒𝐼95 𝑥¯",667,semantic
0063b04f-ccf5-4cd0-82d5-f0a4f12fe946,Sampling and Simulation.pdf,CSCI_83,21,"Convergence and Standard Errors for a Normal Distribution
Mean estimates for realizations of standard Normal distribution with 95% conﬁdence intervals
Convergance of mean estimates with standard errors",201,semantic
efab3e19-b268-4276-a664-f8de7d3c93be,Sampling and Simulation.pdf,CSCI_83,22,"Sampling Strategies
There are a great number of possible sampling methods. Some of the most commonly used methods
Bernoulli sampling, a foundation of random sampling
Stratiﬁed sampling, when groups with different characteristics must be sampled
Cluster sampling, to reduce cost of sampling
Systematic sampling and convenience sampling, a slippery slope",352,semantic
00cdbe76-2be1-4486-a637-677521d60410,Sampling and Simulation.pdf,CSCI_83,23,"Bernoulli Sampling
Bernoulli sampling is a widely used foundational random sampling strategy
Bernoulli sampling has the following properties:
A single random sample of the population is created
A particular value in the population is sampled based on the outcome of a Bernoulli trial with ﬁxed probability of success, 
Example, a company sells a product by weight
To ensure the quality of a packaging process so few packages are underweight
Impractical to empty and weight the contents of every package
Bernoulli randomly sampled packages from the production line and weigh contents
Statistical inferences are made from sample
𝑝",628,semantic
32232391-086e-4c52-ad07-3729e3c6d2a0,Sampling and Simulation.pdf,CSCI_83,24,"Bernoulli Sampling
An example with synthetic data. - Generate population of 10000 samples from the standard Normal distribution- The realizations are randomly divided into 4 groups with - The probability of a sample being in a group is not uniform, and sums to 1.0. ##         var  group## 0  1.469248      1## 1 -1.150144      2## 2  2.519226      2## 3 -0.082478      2## 4 -0.033601      0## 5 -1.636656      0## 6 -0.412092      2## 7  1.784949      1## 8  0.042383      2## 9 -0.619732      2
𝑝=[0.1,0.3,0.4,0.2]
nr.seed(3 4 5 )population_size = 1 0 0 0 0 
data = pd.DataFrame({""var"":nr.normal(size = population_size),                      ""group"":nr.choice(range(4 ), size= population_size, p = [0 . 1 ,0 . 3 ,0 .",719,semantic
e60e7b0b-48a0-413e-a6f2-3c0e818f9db8,Sampling and Simulation.pdf,CSCI_83,24,"4 ,0 . 2 ])})data.head(1 0 )",28,semantic
ea391c62-db7e-4d8c-a818-b99ff93d7dc9,Sampling and Simulation.pdf,CSCI_83,25,"Bernoulli Sampling
The population of 10000 samples from the standard Normal distribution
The mean of each group should be close to 0.0:1. The sample is divided between 4 groups2. Probability of sample from given group, 3. Summary statistics are computed for each group
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0        102 -0.030849  0.102082  0.169231 -0.230929## 1        297 -0.000648  0.056728  0.110538 -0.111834## 2        399  0.003840  0.050075  0.101986 -0.094306## 3        200 -0.014848  0.071609  0.125506 -0.155202
𝑝=0.1
def count_mean(dat, p=1 . 0 ):    import numpy as np    import pandas as pd    groups = dat.groupby('group') # Create the groups
    n_samples = np.int64(p * groups.size())    se = np.sqrt(np.divide(groups.aggregate(np.var).loc[:, 'var'], n_samples))    means = groups.aggregate(np.mean).loc[:, 'var']
    ## Create a data frame with the counts and the means of the groups    return pd.DataFrame({'Count': n_samples,                         'Mean': means,                        'SE': se,
                        'Upper_CI': np.add(means, 1 . 9 6  * se),                        'Lower_CI': np.add(means, -1 . 9 6  * se)})p = 0 .",1234,semantic
3e4ee82c-c808-48a1-babd-a4e5cda29d08,Sampling and Simulation.pdf,CSCI_83,25,"1 
count_mean(data, p)",22,semantic
a531f9c0-172b-47b7-b134-3f5927e8f8f5,Sampling and Simulation.pdf,CSCI_83,26,"Sampling Grouped Data
Group data is quite common in application
A few examples include:
1. Pooling opinion by county and income group, where income groups and counties have signiﬁcant differences in population
2. Testing a drug which may have different effectiveness by sex and ethnic group
3.",293,semantic
8c479f67-e682-4a46-b035-988b43d40112,Sampling and Simulation.pdf,CSCI_83,26,Spectral characteristics of stars by type,41,semantic
4a298bb2-fccf-4ce9-b617-14d3f78cb465,Sampling and Simulation.pdf,CSCI_83,27,"Stratiﬁed Sampling
What is a sampling strategy for grouped or stratiﬁed data? Stratiﬁed sampling strategies are used when data are organized in strata
Simple Idea: independently sample an equal numbers of cases from each strata
The simplest version of stratiﬁed sampling creates an equal-size Bernoulli sample from each strata
In many cases, nested samples are required
For example, a top level sample can be grouped by zip code, a geographic strata
Within each zip code, people are then sampled by income bracket strata
Equal sized Bernoulli samples are collected at the lowest level",584,semantic
a3844c0b-ebba-482d-a1b1-93e981f74992,Sampling and Simulation.pdf,CSCI_83,28,"Example
Bernoulli sample 100 from each group and compute summary statistics
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0        100 -0.041753  0.097958  0.150244 -0.233750## 1        100 -0.159626  0.092409  0.021495 -0.340748## 2        100 -0.072279  0.097027  0.117893 -0.262451## 3        100  0.129402  0.102523  0.330347 -0.071543
def stratify(dat, p):
    groups = dat.groupby('group') # Create the groups    nums = min(groups.size()) # Find the size of the smallest group    num = int(p * dat.shape[0 ]) # Compute the desired number of samples per group
    if num <= nums:         ## If sufficient group size, sample each group. ## We drop the unneeded index level and return,         ## which leaves a data frame with just the original row index. return groups.apply(lambda x: x.sample(n=num)).droplevel('group')    else: # Oops.",909,semantic
ece56499-9ea2-4eb3-83be-43366632f975,Sampling and Simulation.pdf,CSCI_83,28,"p is to large and our groups cannot accommodate the choice of p. pmax = nums / dat.shape[0 ]
        print('The maximum value of p = ' + str(pmax))
p = 0 . 0 1 stratified = stratify(data, p)
count_mean(stratified)",213,semantic
ec471eaf-f474-4517-8f36-3476d537425c,Sampling and Simulation.pdf,CSCI_83,29,"Cluster Sampling
When sampling is expensive, a strategy is required to reduce the cost
Examples of expensive to collect data:
Surveys of customers at a chain of stores
Door to door survey of homeowners
Sampling wildlife populations in a dispersed habitat
Population can be divided into randomly selected clusters:- Deﬁne the clusters for the population- Randomly select the required number of clusters- Sample from selected clusters- Optionally, stratify the sample within each cluster",485,semantic
669d3f86-304c-4fe4-85a3-19b07d4784b0,Sampling and Simulation.pdf,CSCI_83,30,"Cluster Sampling
As an example, select a few store locations and Bernoulli sample customers at these locations. ##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0         97  0.023343  0.103481  0.226166 -0.179479## 1         97  0.049290  0.104858  0.254812 -0.156232## 2         98  0.056362  0.109000  0.270002 -0.157278## 3        102 -0.146424  0.105291  0.059947 -0.352795## 4        111 -0.025847  0.100979  0.172072 -0.223765## 5        100  0.079969  0.090989  0.258308 -0.098369## 6        104 -0.064531  0.101523  0.134454 -0.263515## 7        111 -0.057377  0.092665  0.124247 -0.239000## 8         87  0.038115  0.103077  0.240147 -0.163916## 9         93  0.033201  0.100668  0.230510 -0.164109
## First compute the clusters
num_clusters = 1 0 num_vals = 1 0 0 0 ## Create a data frame with randomly sampled cluster numbers
clusters = pd.DataFrame({'group': range(num_clusters)}).sample(n = num_vals, replace = True)## Add a column to the data frame with Normally distributed valuesclusters.loc[:, 'var'] = nr.normal(size = num_vals)",1111,semantic
7d7250af-5fae-4af7-a2ac-f0ce34533a31,Sampling and Simulation.pdf,CSCI_83,31,"Cluster Sampling
Randomly select 3 clusters
The sampled clusters
## cluster sampled are:
## 7## 5## 4
Display summary statistics
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 4        111 -0.025847  0.100979  0.172072 -0.223765## 5        100  0.079969  0.090989  0.258308 -0.098369## 7        111 -0.057377  0.092665  0.124247 -0.239000
## Randomly sample the group numbers, making sure we sample from 
## unique values of the group numbers. clusters_samples = nr.choice(clusters.loc[:, 'group'].unique(),                              size = 3 , replace = False)
## Now sample all rows with the selected cluster numbersclus_samples = clusters.loc[clusters.loc[:, 'group'].isin(clusters_samples), :]",766,semantic
3b9f91c5-e41b-477e-88a1-db8f9b19c9eb,Sampling and Simulation.pdf,CSCI_83,32,"Systematic Sampling
Convenience and systematic sampling are a slippery slope toward biased inferences
Systematic sampling lacks randomization
Convenience sampling selects the cases that are easiest to obtain
Commonly cited example known as database sampling
Example, the ﬁrst N rows resulting from a database query
Example, every k-th case of the population",357,semantic
ea29c70b-561f-4301-a418-0c0d53d329d5,Sampling and Simulation.pdf,CSCI_83,33,"A Few More Thoughts on Sampling
There are many practical aspects of sampling. Random sampling is essential to the underlying assumptions of statistical inference
Whenever you are planning to sample data, make sure you have a clear sampling plan
Know the number of clusters, strata, samples in advance
Don’t just stop sampling when your desired result is achieved: e.g. error measure!",383,semantic
8d6ff321-f699-4cbe-8c1c-4b9b83399dd3,Sampling and Simulation.pdf,CSCI_83,34,"Introduction to Simulation
Simulation enables data scientists to study the behavior of stochastic processes with complex probability distributions
Most real-world processes have complex behavior, resulting in complex distributions of output values
Simulation is a practical approach to understanding these complex processes
Two main purposes of simulation can be summarized as:
Testing models: If data simulated from the model do not resemble the original data, something is likely wrong
Understand processes with complex probability distributions: In these cases, simulation provides a powerful and ﬂexible computational technique to understand behavior",654,semantic
52f50bb9-faff-4e52-beb9-06cdf8d08490,Sampling and Simulation.pdf,CSCI_83,35,"Introduction to Simulation
As cheap computational power has become ubiquitous, simulation has become a widely used technique
Simulations compute a large number of cases, or realizations
The computing cost of each realization must be low in any practical simulation
Realizations are drawn from complex probability distributions of the process model
In many cases, realizations are computed using conditional probability distributions
The ﬁnal or posterior distribution of the process is comprised of these realizations",517,semantic
8739a737-4a26-4e4d-9c7e-00070399ef9d,Sampling and Simulation.pdf,CSCI_83,36,"Representation as a Directed Acyclic Graphical Model
When creating a simulation with multiple conditionally dependent variables it is useful to draw a directed graph; a directed acyclic graphical model orDAG
The graph is a communications device showing which variables are independent and which are conditionally dependent on others with the shapes usedrepresenting the type of nodes
Probability distributions of the variables are shown as ellipses
Distributions have parameters which must be estimated
Decision variables are deterministic and are shown as rectangles
Decisions are determined by variables
Setting decision variables can be performed either manually or automatically
Utility nodes, proﬁt in this case, are shown as diamonds
Nodes represent a utility function given the dependencies in the graph
Utility calculations are deterministic given the input values
Directed edges show the dependency structure of the distributions
Arrows point to child nodes which are dependent on parent nodes
Child node conditional on parent nodes",1041,semantic
96001fc0-e91a-4134-aa46-0e219bbadfa2,Sampling and Simulation.pdf,CSCI_83,37,"Sandwich Shop Simulation
The sandwich shop simulation can be represented by a DAG
Directed graph of the distributions for proﬁt simulation",138,semantic
c94d5653-5317-43f4-987b-f21ba4bc89cd,Sampling and Simulation.pdf,CSCI_83,38,"Sandwich Shop Simulation
Interpreting the DAG
The DAG is a shorthand description of the simulation model
Leaves of the DAG are independent distributions
Parameters must be known or estimated
Can be useful to vary the parameters
Child distributions are conditional on their parents
Parameters must be known or estimated
Resulting distribution can be quite complex
Decision variables deterministicly change the model parameters
Utility node uses a ﬁxed deterministic formula to compute the value for each realization of the simulaiton",532,semantic
932f3c8e-6970-45f4-b602-0eaa1253e2b0,Sampling and Simulation.pdf,CSCI_83,39,"Tips on Building Simulations
Creating, testing and debugging simulation software can be tricky given the stochastic nature of simulation
Build your simulation as a series of small, easily tested, chunks
Test each small functional unit individually, including at least testing some typical cases, as well as boundary or extreme cases
Test your overall simulation each time you add a new functional component - avoid big bang integration! Simulations are inherently stochastic, set a seed before you begin tests so they are repeatable",532,semantic
216ba496-cdec-422b-b26e-d60fb92cb537,Sampling and Simulation.pdf,CSCI_83,40,"Summary
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation",633,semantic
08aa94ef-5ab1-4bac-a8a5-685c230f7c0d,Introduction to Bayesian Models.pdf,CSCI_83,0,"Introduction to Bayesian Models
Steve Elston
10/13/2022",55,semantic
bc05bc85-7bab-4151-a730-c93fd536102c,Introduction to Bayesian Models.pdf,CSCI_83,1,"Review
The concept of likelihood and maximum likelihood estimation (MLE) have been at the core of much of statistical modeling for about 100 years
In 21st Century, likelihood and MLE ideas continue to be foundational
The principles we can easily visualize in a few dimensions apply in high dimenstions
An understanding of these principles helps you understand modern large scale methods
Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods
Likelihood is a measure of how well a model ﬁts data
MLE is a generic methods for parameter estimation
MLE used widely for machine learning models, including some deep learning models",697,semantic
17973849-711c-493e-9060-67a84d7c368f,Introduction to Bayesian Models.pdf,CSCI_83,2,"Review
Statistical inference seeks to characterize the uncertainty in statistical point estimates
Statistics are estimates of population parameters
Inferences using statistics must consider the uncertainty in the estimates
Conﬁdence intervals quantify uncertainty in statistical estimates
Two-sided conﬁdence intervals: express conﬁdence that a value is within some range around the point estimate
One-sided conﬁdence intervals: express conﬁdence that the point estimate is greater or less than some range of values",515,semantic
cc352652-2c74-4b80-ab6c-e2dc642d92ab,Introduction to Bayesian Models.pdf,CSCI_83,3,"Review
Nonparametric bootstrap estimation is widely useful and requires minimal assumption
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conﬁdence interval is computed from the bootstrap distribution",633,semantic
85b56351-1f16-4893-8ee6-09c0519a8aa3,Introduction to Bayesian Models.pdf,CSCI_83,4,"Review
There are several variations of the basic nonparametric bootstrap algorithm
One sample bootstrap
Inference on single statistic
Two sample bootstrap
Inference on different statistic
Special cases
Correlation coefﬁcients - part of your assignment",251,semantic
88af1919-50f6-440e-9d4b-9a05c9bbbb81,Introduction to Bayesian Models.pdf,CSCI_83,5,"Review
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the re-sampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Be suspicious of overly optimistic conﬁdence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
𝑥¯ 𝜇",596,semantic
bf354843-8eb2-436f-9d17-5ab4b805cf7a,Introduction to Bayesian Models.pdf,CSCI_83,6,"Introduction Baysian Models
Despite the long history, Bayesian models have not been used extensively until recently
Two traditions in statistics
Frequentist we have been working with previously
Bayesian statistics
Limited use is a result of several difﬁculties
Rarely taught for much of the 20th Century
The need to specify a prior distribution has proved a formidable intellectual obstacle
Modern Bayesian methods are often computationally intensive and have become practical only with cheap computing
Recent emergence of improved software and algorithms has resulted in wide and practical access to Bayesian methods",617,semantic
968220ec-2b25-4f3e-b917-7ac3fa03f11e,Introduction to Bayesian Models.pdf,CSCI_83,7,"Introduction
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics is to compute a point estimate and conﬁdence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
Update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ﬁnding the maximum a postiori (MAP) value and a credible interval",740,semantic
0614b5c0-36d6-41f9-958f-a9ab8d5d092c,Introduction to Bayesian Models.pdf,CSCI_83,8,"Bayesian Model Use Case
Bayesian methods made global headlines with the successful location of the missing Air France Flight 447
Aircraft had disappeared in little traveled area of the South Atlantic Ocean
Conventional location methods had failed to locate the wreckage; potential search area too large
Bayesian methods rapidly narrowed the prospective search area
Used ‘prior information’ on aircraft heading and time of sattelite transmisison
Posterior distribution of locations of Air France 447",498,semantic
659a260a-8d90-4161-b34a-91d36d2abde4,Introduction to Bayesian Models.pdf,CSCI_83,9,"Bayesian Model Use Case
Kratzke, Stone and Frost developed an optimal search missing planner using Baysian model
Search areas concentrate on high posterior probability regions
Model accounts for current, wind, etc. Screen shot from USCG search planner",251,semantic
fc3e880b-1c1b-4f88-ad87-33e0b2f05fe5,Introduction to Bayesian Models.pdf,CSCI_83,10,"Bayesian vs. Frequentist Views
With greater computational power and general acceptance, Bayes methods are now widely used
Among pragmatists
Some problems are better handled by frequentist methods
Some problems with Bayesian methods
Bayes models allow us to express prior information
Models that fall between these extremes are also in common use
Methods include the so-called empirical Bayes methods.",400,semantic
a42c5a05-7133-4522-ad50-d6751efd9638,Introduction to Bayesian Models.pdf,CSCI_83,11,"Bayesian vs. Frequentist Views
Can compare the contrasting frequentist and Bayesian approaches
Comparison of frequentist and Bayes methods",138,semantic
4638013a-76ac-419e-8b91-0b11ae7f86d5,Introduction to Bayesian Models.pdf,CSCI_83,12,"Review of Bayes Theorem
Bayes’ Theorem is fundamental to Bayesian data analysis. Start with:
We can also write:
Eliminating 
And ﬁnally, Bayes theorem! 𝑃(𝐴∩𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)
𝑃(𝐴∩𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)
𝑃(𝐴∩𝐵):
𝑃(𝐵)𝑃(𝐴|𝐵)=𝑃(𝐴)𝑃(𝐵|𝐴)
𝑃(𝐴|𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)𝑃(𝐵)",239,semantic
b551b943-823a-441d-a5df-9de5b722ae22,Introduction to Bayesian Models.pdf,CSCI_83,13,"Bayes Theorem
Bayes Theorem!",28,semantic
bc62031d-44c6-4cc0-a6a1-680db70c8106,Introduction to Bayesian Models.pdf,CSCI_83,14,"Marginal Distributions
In many cases we are interested in the marginal distribution
Example, it is often the case that only one or a few parameters of a joint distribution will be of interest
In other words, we are interested in the marginal distribution of these parameters
The denominator of Bayes theorem, , can be computed as a marginal distribution
Consider a multivariate probability density function with  variables, 
Marginal distribution is the distribution of one variable with the others integrated out. Integrate over all other variables  the result is the marginal distribution, :
- But computing this integral is not easy! 𝑃(𝑑𝑎𝑡𝑎)
𝑛 𝑝(, ,…, )𝜃1 𝜃2 𝜃𝑛
{ ,…, }𝜃2 𝜃𝑛 𝑝( )𝜃1
𝑝()= 𝑝(, ,…, ) 𝑑𝜃2,…,𝑑𝜃1 ∫,…,𝜃2 𝜃𝑛
𝜃1 𝜃2 𝜃𝑛 𝜃𝑛",731,semantic
8488b818-392d-4773-aa8b-49fe940a7257,Introduction to Bayesian Models.pdf,CSCI_83,15,"Marginal Distributions
For discrete distributions compute the marginal by summation
Or, for discrete samples of a continuous distribution
Example, need to know (un-normalized) posterior distribution of parameter , a marginal distribution:
Now we have the marginal distribution of 
Or, we need to ﬁnd the denominator for Bayes theorem to normalize our posterior distribution, a marginal distribution:
We can compute  from samples without ever directly computing the marginal
𝜃
𝑝(𝜃)= 𝑝(𝜃|𝐗) 𝑝(𝐗)∑𝑥∈ 𝐗
𝜃
𝑝(𝐗)= 𝑝(𝐗|𝜃)𝑝(𝜃)∑𝜃∈ Θ
𝑝(𝐗)",527,semantic
fc8333a2-d48a-4998-a25e-57cce7e03dda,Introduction to Bayesian Models.pdf,CSCI_83,16,"Interpreting Bayes Theorem
How can you interpret Bayes’ Theorem? For model parameter estimation problem:
Or, Bayes’ theorem in terms of model parameters:
Summarized as:
𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 𝐷𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛=𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑∙𝑃𝑟𝑖𝑜𝑟 𝐷𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒
𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎)=𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) 𝑃𝑟𝑖𝑜𝑟(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑑𝑎𝑡𝑎)
𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎)=𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) 𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑑𝑎𝑡𝑎)",383,semantic
04ed8614-b7d2-4a37-8534-59bffd783aea,Introduction to Bayesian Models.pdf,CSCI_83,17,"Interpreting Bayes Theorem
What do these terms actually mean? 1. Posterior distribution of the parameters given the evidence or data, the goal of Bayesian analysis
2.",166,semantic
4e55f368-2d15-4bd7-a39a-21bbd6a1a32c,Introduction to Bayesian Models.pdf,CSCI_83,17,"Prior distribution is chosen to express information available about the model parameters apriori
3. Likelihood is the conditional distribution of the data given the model parameters
4. Probabiltiy of Data or evidence is the distribution of the data and normalizes the posterior
Relationships can apply to the parameters in a model; partial slopes, intercept, error distributions, lasso constants, etc",400,semantic
032548a8-e43c-434f-98dc-c7fce710f22b,Introduction to Bayesian Models.pdf,CSCI_83,18,"Applying Bayes Theorem
We need a tractable formulation of Bayes Theorem for computational problems
We must avoid directly summing all of the possibilities to compute the denominator, 
In many cases, computing this denominator directly is intractable
Some interesting facts about conditional probabilities:
Where, , and the marginal distribution, , can be written:
𝑃(𝐵)
𝑃(𝐵∩𝐴)=𝑃(𝐵|𝐴)𝑃(𝐴)𝐴𝑛𝑑𝑃(𝐵)=𝑃(𝐵∩𝐴)+𝑃(𝐵∩ )𝐴¯
=𝑛𝑜𝑡 𝐴𝐴¯ 𝑃(𝐵)
𝑃(𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)+𝑃(𝐵│)𝑃()𝐴¯ 𝐴¯",453,semantic
0be28a1a-f2a8-4c39-82cd-bc21864e3f3b,Introduction to Bayesian Models.pdf,CSCI_83,19,"Applying Bayes Theorem
Using the foregoing relations we can rewrite Bayes Theorem as:
Computing the denominator requires summing all cases in the subsets  and 
This is a bit of a mess! And, the siguation is worse if there are multiple alternative hypotheses! Fortunately, we can often avoid computing this denominator by force
Write Bayes Theorem as:
Ignoring the normalization constant :
𝑃(𝐴|𝐵)= 𝑃(𝐴)𝑃(𝐵|𝐴)𝑃(𝐵│𝐴)𝑃(𝐴)+𝑃(𝐵│)𝑃()𝐴¯ 𝐴¯
𝐴 𝑛𝑜𝑡 𝐴
𝑃(𝐴│𝐵)=𝑘∙𝑃(𝐵|𝐴)𝑃(𝐴)
𝑘
𝑃(𝐴│𝐵)∝ 𝑃(𝐵|𝐴)𝑃(𝐴)",480,semantic
e7fa043b-dd9c-417a-b6d6-a3ee41977115,Introduction to Bayesian Models.pdf,CSCI_83,20,"Interpreting Bayes Theorem
Denominator must account for all possible outcomes, or alternative hypotheses, :
Computing this denominator is a formidable problem! Can be inﬁnite number of alternative hypotheses; e.g. continuous random variable
ℎ ′
𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠 | 𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)=𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 | ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠) 𝑝𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠)𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 |  ) 𝑝𝑟𝑖𝑜𝑟()∑∈  𝐴𝑙𝑙 𝑝𝑜𝑠𝑠𝑖𝑏𝑙𝑒 ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑒𝑠ℎ ′ ℎ ′ ℎ ′",409,semantic
12890d9b-e58c-4fc7-9dac-e310ba2a7ef0,Introduction to Bayesian Models.pdf,CSCI_83,21,"Simpliﬁed Relationship for Bayes Theorem
How to we interpret the foregoing relationship? Consider the following relationship:
We can ﬁnd an un-normalized function proportional to the posterior distribution
Sum over  to ﬁnd the marginal distribution 
Approach can transform an intractable computation into a simple summation
𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 𝐷𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛∝ 𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑∙𝑃𝑟𝑖𝑜𝑟 𝐷𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛𝑂𝑟𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎)∝ 𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)
𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) 𝑃(𝐵)",468,semantic
1f53fa84-19d9-42b7-a602-a309aa4e7bc6,Introduction to Bayesian Models.pdf,CSCI_83,22,"Creating Bayes models
The goal of a Bayesian analysis is computing and performing inference on the posterior distribution of the model parameters
The general steps are as follows:
1. Identify data relevant to the research question
2. Deﬁne a sampling plan for the data.",269,semantic
4f932a2c-b2f4-4dfb-b58c-e5c47d422551,Introduction to Bayesian Models.pdf,CSCI_83,22,"Data need not be collected in a single batch
3. Deﬁne the model and the likelihood function; e.g. regression model with Normal likelihood
4. Specify a prior distribution of the model parameters
5. Use the Bayesian inference formula to compute posterior distribution of the model parameters
6. Update the posterior as data is observed
7. Inference on the posterior can be performed; compute credible intervals
8. Optionally, simulate data values from realizations of the posterior distribution. These values are predictions from the model.",538,semantic
d4033e28-849c-4eb1-80f9-005a0a0604e5,Introduction to Bayesian Models.pdf,CSCI_83,23,"Updating Bayesian Models
An advantage of Bayesain model is that it can be updated as new observations are made
In contrast, for frequentist models data must be collected completely in advance
We update our belief by adding new evidence
The posterior of a Bayesian model with no evidence is the prior
The previous posterior serves as a prior for model updates",358,semantic
506fed69-91aa-47d4-af5c-eae4716c519e,Introduction to Bayesian Models.pdf,CSCI_83,24,"How can you choose a prior? The choice of the prior is a difﬁcult, and potentially vexing, problem when performing Bayesian analysis
The need to choose a prior has often been cited as a reason why Bayesian models are impractical
General guidance is that a prior must be convincing to a skeptical audience
Often tend to use vague or less informative priors in practice",367,semantic
d6bc0d44-d7dc-425a-a83d-9f098d2afb80,Introduction to Bayesian Models.pdf,CSCI_83,25,"How can you choose a prior? Some possible approaches to prior selection include:
Use prior empirical information
Apply domain knowledge to determine a reasonable distribution
Use information from prior work
Example, the viable range of parameter values could be computed from physical principles
Example, it could be well know that there is price range for some asset
If there is poor prior knowledge for the problem a non-informative prior can be used
One possibility is a Uniform distribution. But be careful! since a uniform prior is informative because of limits on the values!",581,semantic
61ee3fa6-4d39-44c3-914b-0126f6ffa046,Introduction to Bayesian Models.pdf,CSCI_83,25,Other options include the Jefferys’ prior.,42,semantic
501c98f8-a82c-4a66-a6a1-b23af52955bf,Introduction to Bayesian Models.pdf,CSCI_83,26,"How can you choose a prior? How to use prior empirical information to estimate the parameters of the prior distribution
Deriving a prior distribution in this manner is sometimes called empirical Bayes
Has become more practical with large modern data sets
Method somewhere between Bayesian and frequentist
Empirical Bayes approach is often applied in practice
Some Bayesian theoreticians do not consider this a Bayesian approach at all! Example, need a prior distribution of home prices per square foot by location
Use pooled information to compute distribution of prices for all locations
Use the prior with speciﬁc evidence by locations to compute posteriors by location
Is example of hierarchical model
Typically, a less informative prior distribution is used than the actual empirical distribution so the model is not overly constrained",839,semantic
06403051-48fd-4262-9cd7-2a03bf712a97,Introduction to Bayesian Models.pdf,CSCI_83,27,"Conjugate Prior Distributions
An analytically and computationally simple choice for a prior distribution family is a conjugate prior
When a likelihood function is multiplied by its conjugate distribution the posterior distribution will be in the same family as the conjugate prior
Attractive idea for cases where the conjugate distribution exists
Analytic results can be computed
The posterior is a known distribution
But there are many practical cases where a conjugate prior is not used
We will address more general methods later",531,semantic
760b73bf-f644-4d82-8903-e410798b4f90,Introduction to Bayesian Models.pdf,CSCI_83,28,"Conjugate Prior Distributions
Most commonly used distributions have conjugates, with a few examples:
Likelihood Conjugate
Binomial Beta
Bernoulli Beta
Poisson Gamma
Categorical Dirichlet
Normal - mean Normal
Normal - variance, Inverse Gamma
Normal - inverse variance, Gamma
𝜒2
𝜏",278,semantic
047af5bf-38fa-45c7-8097-a56834bd1cdf,Introduction to Bayesian Models.pdf,CSCI_83,29,"Example using Conjugate Distribution
We are interested in analyzing the incidence of distracted drivers
Randomly sample the behavior of 10 drivers at an intersection and determine if they exhibit distracted driving or not
Data are Binomially distributed, a driver is distracted or not, with likelihood:
Binomial likelihood has one parameter we need to estimate, , the probability of success
𝑃(𝑘)=()⋅ (1−𝜃𝑛𝑘 𝜃𝑘 )𝑛−𝑘
𝜃",416,semantic
5c910689-823c-4144-9c4d-0e24fa29450b,Introduction to Bayesian Models.pdf,CSCI_83,30,"Working with Conjugate Distribution
Our process for example is:
1.",66,semantic
f5eaa91c-fcef-47db-a914-1ef0da58136f,Introduction to Bayesian Models.pdf,CSCI_83,30,"Use the conjugate prior, the Beta distribution with parameters  and  (or a,b)
2. Using the data sample, compute the likelihood
3. Compute the posterior distribution of distracted driving
4. Add more evidence (data) and update the posterior distribution. 𝛼 𝛽",257,semantic
b4a491bb-a1c0-4350-8051-db219c324780,Introduction to Bayesian Models.pdf,CSCI_83,31,"Example using Conjugate Distribution
What are the properties of the Beta distribution? Beta distribution for different parameter values",135,semantic
41c30761-67ef-40bf-b94f-1a41850fee66,Introduction to Bayesian Models.pdf,CSCI_83,32,"Example using Conjugate Distribution
Consider the product of a Binomial likelihood and a Beta prior
Deﬁne the evidence as  trials with  successes
Prior is a Beta distribution with parameters  and , or the vector 
From Bayes Theorem the distribution of the posterior:
𝑛 𝑧
𝑎 𝑏 𝜃=(𝑎,𝑏)
𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(𝜃|𝑧,𝑛)
𝑝(𝜃|𝑧,𝑛)
=𝑙𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑧,𝑛|𝜃) 𝑝𝑟𝑖𝑜𝑟(𝜃)𝑑𝑎𝑡𝑎 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛(𝑧,𝑛)
=𝐵𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝑧,𝑛|𝜃) 𝐵𝑒𝑡𝑎(𝜃)𝑝(𝑧,𝑛)=𝐵𝑒𝑡𝑎(𝑧+𝑎, 𝑛−𝑧+𝑏)",407,semantic
47aeedd0-643a-42cd-aeaa-1c3d5b19201c,Introduction to Bayesian Models.pdf,CSCI_83,33,"Example using Conjugate Distribution
There are some useful insights you can gain from this relationship for (discrete) integer counts:
Posterior distribution is in the Beta family, as a result of conjugacy
Parameters  and  are determined by the prior and the evidence
Parameters of the prior can be interpreted as pseudo counts of successes,  and failures, 
Be careful when creating a prior to add 1 to the successes and failures
The larger the total pseudo counts, , the stronger the prior information
-Evidence is also in the form (actual) counts of successes,  and failure, - The more evidence the greater the inﬂuence on the posterior distribution- Large amount of evidence will overwhelm the prior- With large amount of evidence, posterior converges to frequentist model
𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(𝜃|𝑧,𝑛)=𝐵𝑒𝑡𝑎(𝑧+𝑎, 𝑛−𝑧+𝑏)
𝑎 𝑏
𝑎=𝑝𝑠𝑒𝑢𝑑𝑜 𝑠𝑢𝑐𝑐𝑒𝑠𝑠+1 𝑏=𝑝𝑠𝑒𝑢𝑑𝑜 𝑓𝑎𝑖𝑙𝑢𝑟𝑒+1
𝑎+𝑏
𝑧 𝑛−𝑧",861,semantic
ab619937-84c7-473f-a378-9a76c89cbf35,Introduction to Bayesian Models.pdf,CSCI_83,34,"Example using Conjugate Distribution
Consider example with:- Prior pseudo counts , successes  and failures, - Evidence, successes  and failures, - Posterior is 
Prior, likelihood and posterior for distracted driving
[1,9] 𝑎=1+1 𝑏=9+1=10 =30𝐵𝑒𝑡𝑎(10+2, 40−10+10)=𝐵𝑒𝑡𝑎(12, 40)",273,semantic
b262b7e3-f33f-4484-81cd-3e71990f4212,Introduction to Bayesian Models.pdf,CSCI_83,35,"Sampling the Posterior
How can we ﬁnd an estimate of the poster distribution? 1. We can sample from the analytic solution - if we have a conjugate
2.",149,semantic
ebda028a-14f6-46c7-a279-7582ec1f812e,Introduction to Bayesian Models.pdf,CSCI_83,35,"We can sample the likelihood and prior, take the product and normalize - for any posterior
3. Grid sample or Markov chain Monte Carlo (MCMC) sample",147,semantic
f4eca257-9e3a-46e2-a3e2-a64869618399,Introduction to Bayesian Models.pdf,CSCI_83,36,"Sampling the Posterior
Grid sampling is a naive approach
Compute the probability at each point on a regular gird
Sample over range of interesting values for variables
Posterior if conjugate prior
Prior and likelihood
In principle can work for any number of dimensions
In 1-dimension is just regularly spaced points on a line
Poor scaling to higher dimensions
Sampling grid for bivariate distribution",399,semantic
220d8ec9-d8fb-4421-a28c-18dfce22462b,Introduction to Bayesian Models.pdf,CSCI_83,37,"Sampling the Posterior
Algorithm for grid sampling to compute posterior from likelihood and prior
Procedure CreateGrid(variables, lower_limits, upper_limits): 
    # Build the sampling grid     return sampling_grid       
Procedure SampleLikelihood(sampling_value, observation_values):        return likelihood_function(sampling_value, observation_values)    Procedure Prior(sampling_values, prior_parameter_value):    
    return prior_density_function(sampling_value, prior_parameter_values)        ComputePosterior(variables, lower_limits, upper_limits):    
    # Initialize the sampling grid    Grid = CreateGrid(variables, lower_limits, upper_limits)        # Initialize array to hold sampled posterior values       
    array posterior[range(Grid)]
    # Compute posterior at each sampling value in the grid      for sampling_value in range(lower_limits, upper_limits):   
        likelihood = SampleLikelihood(sampling_value, observation_values)        prior = Prior(sampling_values, prior_parameter_value)           posterior[sampling_value] = likelihood * prior
    # Normalize the posterior           probability_data = sum(posterior[range(Grid)])    posterior = posterior[range(Grid)]/probability_data 
    return posterior",1235,semantic
dea429d1-2c16-4ea8-9769-8de6377d7248,Introduction to Bayesian Models.pdf,CSCI_83,38,"Credible Intervals
How can we specify the uncertainty for a Bayesian parameter estimate? For frequentist analysis we use conﬁdence intervals, but not entirely appropriate
Conﬁdence intervals are based on a sampling distribution
The upper and lower conﬁdence intervals quantiles of the sampling distribution
Bayesian analysis has no sampling distribution uses a prior distribution and likelihood
For Bayesian analysis inference performed on posterior distribution
We use a concept known as the credible interval
A credible interval is an interval on the Bayesian posterior distribution with the highest  proportion of posterior probability𝛼",639,semantic
cf9692d5-4db6-418b-8a32-6cd0f0aed685,Introduction to Bayesian Models.pdf,CSCI_83,39,"Credible Intervals
How can we specify the uncertainty for a Bayesian parameter estimate? Example, the  credible interval encompasses the 90% of the posterior distribution with the highest density
The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval (HPDI)
These names make sense, since we seek the the densest posterior interval containing  probability
For symmetric distributions the credible interval can be numerically the same as the conﬁdence interval
In general, these two quantities can be quite different
𝛼=0.90
𝛼",583,semantic
a0f6211f-aa8c-40c5-bba7-945735709e00,Introduction to Bayesian Models.pdf,CSCI_83,40,"Credible Intervals
What are the 95% credible intervals for ? Probability of distract drivers for next 10 cars
𝐵𝑒𝑡𝑎(12, 40)",122,semantic
c554fafb-1605-4d56-818c-984da49a35b2,Introduction to Bayesian Models.pdf,CSCI_83,41,"Credible Intervals are not Conﬁdence Intervals
How are credible intervals different from the more familiar conﬁdence intervals? Conﬁdence intervials and credible intervals are conceptually quite different
A conﬁdence interval is a purely frequentest concept- Is an interval on the sampling distribution where repeated samples of a statistic are expected with probability - Cannot interpret a conﬁdence interval as an interval on a probability distribution of the value of a statistic! Credible interval is an interval on a posterior distribution of the statistic- Credible interval is exactly what the misinterpretation of the conﬁdence interval tries to be- Credible interval is the interval with highest  probability for the statistic being estimated
For symmetric posterior distributions, the credible interval will be numerically the same as the conﬁdence interval- This need not be the case in general
=𝛼
𝛼",911,semantic
0903d84f-438a-4af5-ac2b-103dbe69329f,Introduction to Bayesian Models.pdf,CSCI_83,42,"Credible Intervals are not Conﬁdence Intervals
Compare conﬁdence interval and credible interval for the case of 10 observations
Credible intervals cross the density function at exactly the same density
Conﬁdence intervals have the same CDF in the tails beyond the interval
Difference between credible and conﬁdence intervals",324,semantic
2303ed96-5f1f-46db-9b4c-4559b7032a68,Introduction to Bayesian Models.pdf,CSCI_83,43,"Simulating from the posterior distribution: predictions
What else can we do with a Bayesian posterior distribution beyond credible intervals? Perform simulations and make predictions
Predictions are computed by simulating from the posterior distribution
Results of these simulations are useful for several purposes, including:
Predicting posterior values
Model checking by comparing simulation results agree (or not) with observations",434,semantic
63a14af2-2c11-4456-a9fd-8f339d9a3f67,Introduction to Bayesian Models.pdf,CSCI_83,44,"Simulating from the posterior distribution: predictions
Example; What are the probabilities of distracted drivers for the next 10 cars with posterior, ? Probability of distract drivers for next 10 cars
𝐵𝑒𝑡𝑎(12, 40)",214,semantic
fffc35d2-b684-45a7-99e8-129d255874f8,Introduction to Bayesian Models.pdf,CSCI_83,45,"Summary
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics is to compute a point estimate and conﬁdence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ﬁnding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution a",807,semantic
229845e7-5a5a-4ad9-9d6c-465d5c7ea4cd,Introduction to Bayesian Models.pdf,CSCI_83,46,"Summary
Bayesian analysis is in contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Frequentist statistics seeks to compute a point estimate and conﬁdence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ﬁnding the maximum a postiori (MAP) value and a credible interval",727,semantic
3165b24f-be1a-4000-b3c4-70dbfe6775e7,Parameter Estimation and Likelihood.pdf,CSCI_83,0,"Parameter Estimation and Likelihood
Steve Elston
09/29/2022",59,semantic
f6779f9d-2f1c-4faa-ba18-2fd09201b3bb,Parameter Estimation and Likelihood.pdf,CSCI_83,1,"Review
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation",632,semantic
6ce20741-3218-49a1-bf9f-4a8dd4142a04,Parameter Estimation and Likelihood.pdf,CSCI_83,2,"Review
Sampling of a population is done from an unknown population distribution, 
Any statistic we compute for the generating process is an approximation for the population, 
Sampling distribution of unknown population parameter
 
𝑠() ̂",236,semantic
fb073469-aef6-4b64-90d0-a14792a46390,Parameter Estimation and Likelihood.pdf,CSCI_83,3,"Review
The law of large numbers is a theorem that states that statistics of independent random samples converge to the population valuesas more samples are used
The law of large numbers is foundational to statistics and sampling theory
Assume that larger samples are more representatives of the population we are sampling
If the real world did not follow this theorem much of statistics, to say nothing of science and technology, would fail badly.",447,semantic
d881b353-191a-4e11-b595-77f9ac7e6243,Parameter Estimation and Likelihood.pdf,CSCI_83,4,"Introduction
The concept of likelihood and maximum likelihood estimation (MLE) have been at the core of much of statistical modeling for about 100 years
In 21st Century, likelihood and MLE ideas continue to be foundational
Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods
Likelihood is a measure of how like a parametric model is to generate the observed data
MLE is a generic methods for parameter estimation
MLE used widely for machine learning models, including some deep learning models",568,semantic
16745d58-a49a-4b95-8e53-55b172f7368f,Parameter Estimation and Likelihood.pdf,CSCI_83,5,"Likelihood and Density Functions
Likelihood is a measure of how like a parametric model is to generate the observed data
Start with a data sample, 
Likelihood of sample from a generating process with a parametric density function, 
 can be either a probability density function (PDF), for continuous distributions, or a probability mass function (PMF), for discrete distributions
The distribution parameter vector, , is ﬁxed
Now, for each observation, , in , the density is just 
𝐗=[ , ,…, ]𝑥1𝑥2 𝑥𝑛
𝑓(𝐗|𝜃)
𝑓(𝐗 | 𝜃)
𝜃
𝑥𝑖 𝐗= , ,…,𝑥1𝑥2 𝑥𝑛 𝑓(| 𝜃)𝑥𝑖",544,semantic
7a78812a-f0c3-4864-874d-91682f37e262,Parameter Estimation and Likelihood.pdf,CSCI_83,6,"Likelihood
Likelihood is a measure of how like a parametric model is to generate the observed data
Likelihood of sample from a generating process with a parametric probability density, 
For the set of observations, , the likelihood is the product of the densities:
In most practical cases, we work with the log likelihood, for observations, , the log likelihood is expressed:
𝑓(𝐗|𝜃)
𝐗=[ , ,…, ]𝑥1𝑥2 𝑥𝑛
 (𝐗| 𝜃)= 𝑓(|𝜃)∏𝑖=1
𝑛
𝑥𝑖
𝐗= , ,…,𝑥1𝑥2 𝑥𝑛
𝑙(𝐗| 𝜃)=𝑙𝑜𝑔( (𝐗| 𝜃))= 𝑙𝑜𝑔(𝑓( | 𝜃))∑𝑖=1
𝑛
𝑥𝑖",485,semantic
c7bf3ffd-65a4-44fa-91fb-b1b22cd856d2,Parameter Estimation and Likelihood.pdf,CSCI_83,7,"Likelihood
Likelihood is a measure of how likely a parametric model is to generate the observed data sample
In most practical cases, we work with the log likelihood, for observations, , the log likelihood is expressed:
Use log-likelihood to work with the sum of log probabilities rather than the product
If the probabilities are small, the sum is numerically stable
The product of many small numbers is a very small number, which can lead to numerical underﬂow even for 64 or 128 bit ﬂoating point arithmetic
𝐗= , ,…,𝑥1𝑥2 𝑥𝑛
𝑙(𝐗| 𝜃)=𝑙𝑜𝑔( (𝐗| 𝜃))= 𝑙𝑜𝑔(𝑓( | 𝜃))∑𝑖=1
𝑛
𝑥𝑖",568,semantic
1ae3731b-513d-4dad-91b8-a862b500bc61,Parameter Estimation and Likelihood.pdf,CSCI_83,8,"Likelihood Example
Likelihood is a measure of how likely a parametric model is to generate the observed data sample
Binomial likelihood for sample sizes  with 
Notice the variability in the likelihood curve for smaller samples
Likelihood has stronger curvature for larger samples - less uncertainty for maximum
Binomial likelihood at different sample sizes
[25,50,100] 𝑝=0.5",374,semantic
8a62a87e-8ba9-4fa5-b76a-1f011ca72bbf,Parameter Estimation and Likelihood.pdf,CSCI_83,9,"Example: The Normal likelihood
The univariate Normal probability density function with parameter vector  for a single observation, :
For n observations, , the likelihood is the product of the densities:
The log-likelihood (log of above) is a lot easiter to deal with:
- The log-likelihood is a function of the parameters, 
𝜃=(𝜇,𝜎) 𝑥𝑖
𝑓(𝑥 | 𝜇, )=− 𝑒𝑥𝑝[− ( −𝜇]𝜎2 1(2𝜋𝜎2)1/2 12𝜎2𝑥𝑖 )2
𝐗= , ,…,𝑥1𝑥2 𝑥𝑛
𝑙(𝐗| 𝜃)=− 𝑒𝑥𝑝[− ( −𝜇]𝑛(2𝜋𝜎2)1/2 12𝜎2∏𝑖=1
𝑛
𝑥𝑖 )2
𝑙(𝐗 | 𝜇,𝜎)=−𝑙𝑜𝑔(2𝜋 )− ( −𝜇𝑛2 𝜎2 12𝜎2∑𝑖=1
𝑛
𝑥𝑖 )2
(𝜇,𝜎)",501,semantic
7e966bb1-6d1e-45a0-b292-0381fee3413d,Parameter Estimation and Likelihood.pdf,CSCI_83,10,"Example: The Normal likelihood
An example to illustrate the foregoing concepts
Plot the likelihood for 5, 10 and 20 samples from a standard Normal distribution
Vary the parameter , and assume the parameter  is ﬁxed and known. The steps are:
A random sample is drawn from a standard Normal distribution
For the random sample the log-likelihood is computed at each location parameter value
Notice that as the number of observations increases so does the curvature of the likelihood. Normal likelihood at different sample sizes
𝜇 𝜎",528,semantic
4cf31d1a-95d8-489b-9457-572656157c1a,Parameter Estimation and Likelihood.pdf,CSCI_83,12,"Example: Binomial Likelihood
Example of log-likelihood for the Binomial distribution
Binomial distribution models discrete events
Range of the single parameter, , restricted to the range 
Binomial distribution has the following probability mass function (PMF) for  successes in  trials:
Log-likelihood is easily found:
Binomial log-likelihood has a strong dependence on both the sample size,  and the number of successes, 
𝜋 0≤𝜋≤1
𝑘 𝑛
𝑓(𝑘,𝑛 | 𝜋)=()(1−𝜋𝑛𝑦𝜋𝑘 )𝑛−𝑘
𝑙(𝑘,𝑛 | 𝜋)=𝑙𝑜𝑔()+𝑘 𝑙𝑜𝑔(𝜋)+(𝑛−𝑘) 𝑙𝑜𝑔(1−𝜋)𝑛𝑘
𝑛 𝑘",508,semantic
e366bf6f-22f1-489a-b139-0bc1825090ab,Parameter Estimation and Likelihood.pdf,CSCI_83,13,"The Maximum Likehihood Estimator
Maximum likelihood estimator (MLE) is a foundational tool for much of statistical inference and machine learning
Given a log-likelihood function, ﬁnd the model parameters which maximize it
Further, knowing the distribution allows us to quantify the uncertainty of the MLE parameter estimates
The model parameter estimates found by MLE is Normal for large samples, a remarkable property
The MLE is a point estimator
An estimate of a single parameter value, or point value, with the highest likelihood",532,semantic
8346ae2f-4d39-41ef-be4a-0dc22a997113,Parameter Estimation and Likelihood.pdf,CSCI_83,14,"The Maximum Likehihood Estimator
The maximum likelihood for the model parameters is achieved when two conditions are met:
Interpret these two conditions:
First derivative of log-likelihood function, or slope, is 0 at either maximum or minimum points
In general,  is a vector of model parameters
Partial derivatives of log-likelihood are a vector - the gradient with respect to the model parameters
Gradient of the log-likelihood are known as the score function
The second derivatives of the log-likelihood indicates the curvature
Maximum has negative curvature
Minimum has positive curvature
=0∂ 𝑙(𝐗 | 𝜃))∂𝜃
<0 𝑙(𝐗 | 𝜃))∂2
∂𝜃2
𝜃⃗",629,semantic
ee7e43c7-1138-4f00-992e-0373f607c3ee,Parameter Estimation and Likelihood.pdf,CSCI_83,15,"Fisher information and properties of MLE
The maximum likelihood estimator has useful and desirable properties
Start with a matrix of second partial derivatives of the log-likelihood function
Matrix is the observed information matrix of the model, . Useful properties of the information matrix
The more negative the values of the second partial derivatives, the greater the curvature of the log-likelihood
log-likelihood likelihood function with more negative values has limbs
Narrow peak implies that the information on parameter values is high
The matrix is symmetric, or information is symmetric around the maximum likelihood point
 ()𝜃⃗ 
 ()=𝜃⃗ 
⎛
⎝
⎜⎜⎜⎜⎜⎜⎜
𝑙(𝐗 | 𝜃)∂2
∂𝜃21
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃1 𝜃2
⋮ 
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃𝑛 𝜃1
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃2 𝜃1
𝑙(𝐗 | 𝜃)∂2
∂𝜃22
⋮ 
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃𝑛 𝜃2
…
…
⋮ 
…
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃𝑛 𝜃1
𝑙(𝐗 | 𝜃)∂2
∂ ∂𝜃2 𝜃𝑛
⋮ 
𝑙(𝐗 | 𝜃)∂2
∂𝜃2𝑛
⎞
⎠
⎟⎟⎟⎟⎟⎟⎟",858,semantic
cdb64b1e-abb0-4c1a-896b-cbeef6393356,Parameter Estimation and Likelihood.pdf,CSCI_83,16,"Fisher information and properties of MLE
Can one consider the information of the MLE before sampling data or performing an experiment? Can use the expected information or Fisher information
Fisher information is the expectation over the second derivative of the observed information
 ()=−𝐄{ ()}=−𝐄{ }𝜃⃗  𝜃⃗   𝑙(𝐗 | 𝜃)∂2
∂𝜃2",323,semantic
37caca8f-91f3-4766-a33e-2e86288b93a3,Parameter Estimation and Likelihood.pdf,CSCI_83,17,"Fisher information and properties of MLE
Fisher information leads to an important relationship
The MLE parameter estimate  is a Normally distributed random variable
Arises from the Taylor expansion of the maximum likelihood estimator
or
: ﬁrst partial derivative given observations 
: second partial derivative given observations 
𝜃̂ 
0= = + (−𝜃)∂ 𝑙(𝐗 | 𝜃)∂𝜃 ∂ 𝑙(𝜃)𝐗
∂𝜃  𝑙(∂2 𝜃̂ )𝐗
∂𝜃2 𝜃̂ 
0=(𝜃 +( (−𝜃)𝑙′ )𝐗 𝑙″𝜃̂ )𝐗𝜃̂ 
(𝜃𝑙′ )𝐗 𝐗
(𝑙″𝜃̂ )𝐗 𝐗",439,semantic
974021fb-9292-472a-adbf-d3e818842aae,Parameter Estimation and Likelihood.pdf,CSCI_83,18,"Fisher information and properties of MLE
Continuing with the simpliﬁed notation, and solving for ;
Fisher information relates to the score function as the inverse variance
For large sample, , take the expectation over all samples, . Assuming ﬁrst and second derivatives exist and continuous, then by the Central LimitTheorem:
𝜃̂ 
=𝜃+𝜃̂  (𝜃 /𝑛𝑙′ )𝐗
−( /𝑛𝑙″𝜃̂ )𝐗
   (0,1/ )∂ 𝑙(𝜃)𝐗
∂𝜃 ∼ ˙  𝜃
𝑛→∞ 𝐗
 (𝜃, )𝜃̂ ∼ ˙ 1𝑛 (𝜃)",414,semantic
1b6a0991-fa70-47b5-8b09-0b53d5373cf8,Parameter Estimation and Likelihood.pdf,CSCI_83,19,"Fisher information and properties of MLE
Relationship shows several important properties
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, .  (0,1/ )∂ 𝑙(𝜃)∂𝜃 ∼ ˙  𝜃
𝜃̂ 
𝑛",466,semantic
d30e1b02-d245-4aca-8664-dfd0d28380aa,Parameter Estimation and Likelihood.pdf,CSCI_83,20,"Example of MLE for Normal distribution
MLE for the Normal distribution
Find derivatives of the log-likelihood function with respect to the model parameters,  and 
Solving above for the estimate of the mean, 
𝜇 𝜎2
( )=( )=()
∂𝑙∂𝜇∂𝑙∂𝜎2
( −𝜇)1𝜎2 ∑𝑗𝑥𝑗
− + ( −𝜇𝑛2𝜎2 12𝜎4 ∑𝑗𝑥𝑗 )2
00
𝑥¯
𝑛( −𝜇)=0∑𝑗=1
𝑥𝑗
→𝜇: 𝑎𝑡 𝑐𝑜𝑛𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒𝑥¯
=𝑥¯ 1𝑛∑𝑗=1
𝑥𝑗",330,semantic
91b2b7dd-f6d7-45a2-ada3-8a662c1e3264,Parameter Estimation and Likelihood.pdf,CSCI_83,21,"Example of MLE for Normal distribution
MLE for the Normal distribution
Find the maximum likelihood estimate of 
( )=( )=()
∂𝑙∂𝜇∂𝑙∂𝜎2
( −𝜇)1𝜎2 ∑𝑗𝑥𝑗
− + ( −𝜇𝑛2𝜎2 12𝜎4 ∑𝑗𝑥𝑗 )2
00
𝜎2
( −𝜇 =𝑛1𝑠2∑𝑗=1
𝑥𝑗 )2
→𝜇: 𝑎𝑡 𝑐𝑜𝑛𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒𝑥¯
= ( −𝑠2 1𝑛∑𝑗=1
𝑛
𝑥𝑗 𝑥¯)2",245,semantic
73d9720b-1c74-464b-9e2e-0f6b716988ae,Parameter Estimation and Likelihood.pdf,CSCI_83,22,"Example of MLE for Normal distribution
The simpliﬁcation results from the fact that  and  in the limit of a large sample from the law of large numbers. There are some aspects of these relationships which make the MLE method attractive:
The curvature of the MLE for both parameters increases with the number of samples 
The peak of the log-likelihood function becomes better deﬁned as  increases. The maximum likelihood estimates of the parameters,  and  are independent. The off-diagonal terms are .",499,semantic
97ced58c-8b33-42a5-a047-7e880cd393c0,Parameter Estimation and Likelihood.pdf,CSCI_83,22," ()𝜃⃗ =−𝐄⎛
⎝
⎜⎜
𝑙∂2
∂𝜇2
𝑙∂2
∂𝜇 ∂𝜎2
𝑙∂2
∂𝜇 ∂𝜎2
𝑙∂2
∂(𝜎2)2
⎞
⎠
⎟⎟
=−𝐄( )
−𝑛𝜎2
−(−𝜇)𝑛𝜎4 𝑥¯
−(−𝜇)𝑛𝜎4 𝑥¯
− + ( −𝜇𝑛2𝜎4 1𝜎6 ∑𝑗𝑥𝑗 )2
=( )
𝑛𝜎2
0
0
𝑛2𝜎4
→𝐄()=𝜇𝑥¯ 𝐱𝐣 →𝐄{( −𝜇}=𝑠2 𝑥𝑗 )2 𝜎2
𝑛
𝑛
𝜇 𝜎2 0",186,semantic
d7d59354-98ae-4881-8ec1-d293ff07947b,Parameter Estimation and Likelihood.pdf,CSCI_83,23,"Fisher information and properties of MLE
Example of Fisher Information for Normal distribution
As sample size increases, Fisher Information decreases
Variance of MLE decreases with decreasing Fisher Information
Fisher Information for Normal distribution at different sample sizes",279,semantic
2534ace2-0cb2-4d67-9f3f-0766eef04a3c,Parameter Estimation and Likelihood.pdf,CSCI_83,25,"Finding Solutions Without a Closed Form
How do we generalize the MLE method beyond cases with a closed form solution
For example, logistic regression has a nonlinear log likelihood function
An approximate solution can be found by numerical optimization methods or root ﬁnding methods
Two widely used methods
Gradient descent, a ﬁrst-order method
Newton’s method, quadratic approximation",386,semantic
51572586-f670-4ecb-9412-488c4b07ceb6,Parameter Estimation and Likelihood.pdf,CSCI_83,26,"Gradient descent methods
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
Start with the gradient of the log-likelihood function with respect to the parameters, 
The gradient is the vector of partial derivatives with respect to each of the parameters
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
𝜃
𝑔𝑟𝑎𝑑(𝑙())=  𝑙()=𝜃⃗  ∇ 𝜃 𝜃⃗ 
⎛
⎝
⎜⎜⎜⎜⎜⎜
∂𝑙(𝐗 | 𝜃)∂𝜃1
∂𝑙(𝐗 | 𝜃)∂𝜃2
⋮ ∂𝑙(𝐗 | 𝜃)∂𝜃𝑛
⎞
⎠
⎟⎟⎟⎟⎟⎟
𝜃̂ 𝑛 𝜃̂ 𝑛+1
= +𝛾   𝑙()𝜃̂ 𝑛+1 𝜃̂ 𝑛 ∇ 𝜃 𝜃̂",554,semantic
510dbd3d-90dc-4cee-a72f-e474b737a348,Parameter Estimation and Likelihood.pdf,CSCI_83,27,"Gradient descent methods
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
Example: contour plot of Normal log-likelihood
Gradient descent following negative (minus) gradient to maximum of log-likelihood
Maximum gradient direction is perpendicular to coutour lines - steepest uphill path
Notice difference between gradients (and information) between locaiton and scale!",430,semantic
a746dde1-c247-4ca4-b94a-7b269adacc21,Parameter Estimation and Likelihood.pdf,CSCI_83,28,Contours of Normal Log-Likelihood,33,semantic
5344fa23-91ab-4516-a7c7-fbca99003203,Parameter Estimation and Likelihood.pdf,CSCI_83,29,"Gradient descent methods
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Determining a learning rate can have a signiﬁcant effect on the performance of the gradient
This hyperparameter can be chosen manually, often with by a search of the hyperparameter space
Using a ﬁxed  is far from optimal
The gradient changes toward the maximum point the optimal step size changes
More sophisticated algorithms use an adaptive method to determine an optimal step at each step
Adaptive algorithm ﬁnds step size dynamically using a line search procedure
Algorithm converges when the norm of the gradient is approximately 0
This is the stopping condition
Express this condition as
𝜃̂ 𝑛 𝜃̂ 𝑛+1
= +𝛾   𝑙()𝜃̂ 𝑛+1 𝜃̂ 𝑛 ∇ 𝜃 𝜃̂ 
𝛾
𝛾
||  𝑙()||≤𝑡𝑜𝑙𝑙𝑒𝑟𝑎𝑛𝑐𝑒∇ 𝜃 𝜃⃗",964,semantic
cf623280-6be1-408c-b881-a0f83e15e2e6,Parameter Estimation and Likelihood.pdf,CSCI_83,30,"Gradient descent methods
Gradient with respect to model parameters is computed for each dimension of the model parameter vector
proceedure compute_expected_grad(parameters, cases):     for each dimension:      grad[dimension] = grad(parameters, cases)  return grad
The forgoing procedure can be vectorized or parallelized",321,semantic
5f1c7ae3-a010-4c46-b788-e30a97507d7c,Parameter Estimation and Likelihood.pdf,CSCI_83,31,"Gradient descent methods
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
The gradient is always perpendicular to the contours
Gradient descent on countour plot of log-likelihood",240,semantic
c2ea15f3-e96a-414c-a189-f9c91bdc1209,Parameter Estimation and Likelihood.pdf,CSCI_83,32,"Stochastic gadient descent (SGD)
The simple gradient descent algorithm and Newton’s method have limited scalability
Basic algorithms require computing and summing the entire gradient vector
Calculation must be done as a single batch in memory, or batch gradient descent
Computing the full gradient at each step limits scalability
Stochastic gradient decent (SGD) algorithm computes the approximate expected gradient using a mini-batch
The mini-batch is a limited-size Bernoulli sample from the full set of cases
These gradient approximations are inherently noisy or stochastic, giving rise to the method’s name
Using mini-batches greatly increases scalability
While gradient estimates are less accurate, these estimates can be computed very quickly
SGD is highly scalable and the workhorse of many large-scale statistical methods
Mini-batch optimization is often referred to as online optimization since updates as cases arrive.",928,semantic
cf89f6e8-3015-4d69-b6b5-783a6bdff017,Parameter Estimation and Likelihood.pdf,CSCI_83,33,"Stochastic gadient descent (SGD)
The basic idea of stochastic optimization is using a Bernoulli random sample of the data to estimate the expected update of the model weights
where,  is the expected value of the gradient given the Bernoulli sample of the data . Choosing batch size can require some tuning
If the batch is too small, the gradient estimate will be and, hardware resources may not be fully utilized
Large batches require signiﬁcant memory and slow the calculation
Empirically, SGD has good convergence properties
This behavior arises since stochastic gradient samples provide a better exploration of the loss function space
For very large data sets, the SGD algorithm often converges before the ﬁrst pass through the data is completed
= +𝛾  [ 𝐽()]𝜃𝑡+1 𝜃𝑡 𝐸𝑑𝑎𝑡𝑎𝑝̂  ∇ 𝜃 𝜃𝑡
[]𝐸𝑑𝑎𝑡𝑎𝑝̂  𝑑𝑎𝑡𝑎𝑝̂",802,semantic
f00e284d-2a1b-4291-8582-8cf262eabeca,Parameter Estimation and Likelihood.pdf,CSCI_83,34,"Stochastic gadient descent (SGD)
The pseudo code for the SGD algorithm is:
define learning_rateproceedure update_weights(weights, grad):     weights = weights + learning_rate * grad   return weights
Random_sort(cases)          while(grad > stopping_criteria):          mini-batch = sample_next_n(cases)         grad = compute_expected_grad(weights, mini_batch)          weights = update_weights(weights, grad)`   
If the sampling continues for more than one cycle through the cases, the samples are biased
In practice, this small bias does not seem to mater much",562,semantic
0ed38f31-d6b5-4a38-9e6a-6352cd1250a2,Parameter Estimation and Likelihood.pdf,CSCI_83,35,"Newton’s method
Newton’s method, and related methods, employ a quadratic approximation to optimization. For MLE, Newton’s method uses both the ﬁrst andsecond derivatives of the log-likelihood function. Consider a nonlinear log-likelihood function, . We use a Taylor expansion to ﬁnd the tangent point of the log-likelihood, .",325,semantic
545abfdb-c7f0-402b-a4a3-96625361bac7,Parameter Estimation and Likelihood.pdf,CSCI_83,35,"The Taylorexpansion is:
Setting this expansion to 0, we have:
It is simple to solve for :
𝑙(𝜃 |𝐗) = +𝛿𝜃𝜃𝑛 𝜃𝑘
𝑙( +𝛿𝜃)=𝑙( )+( )𝛿𝜃+ ( )𝛿𝜃𝜃𝑘 𝜃𝑘 𝑙′ 𝜃𝑘 12𝑙″𝜃𝑘
0
0
= (𝑙( )+( )𝛿𝜃+ ( )𝛿 )𝑑𝑑 𝛿𝜃 𝜃𝑘 𝑙′ 𝜃𝑘 12𝑙″𝜃𝑘 𝜃2
=( )+( )𝛿𝜃𝑙′ 𝜃𝑘 𝑙″𝜃𝑘
𝛿𝜃
𝛿𝜃= ( )𝑙′ 𝜃𝑘
( )𝑙″𝜃𝑘",247,semantic
d5bf1127-f5c9-45f2-a43a-f9a863f502f2,Parameter Estimation and Likelihood.pdf,CSCI_83,36,"Newton’s method
At each step in the iteration, the update  is given by the following relationship:
 is a learning rate of step size. Newton’s method has a quadratic form
The quadratic form is not just a mathematical curiosity
Newton’s method exhibits convergence quadratic in the number of iterations
Compared to the approximate linear convergence for gradient descent methods
𝑥𝑛+1
𝜃𝑛+1= +𝛾𝜃𝑛 ( )𝑙′ 𝜃𝑘
( )𝑙″𝜃𝑘
= +𝛾 𝛿𝜃𝜃𝑛
𝛾",421,semantic
bbce18d1-da6b-4e06-bb8a-cb1793d2d085,Parameter Estimation and Likelihood.pdf,CSCI_83,37,"Newton’s method
Newton’s method in higher dimensions
Use the gradient,  for the ﬁrst derivatives of the likelihood
Second derivative is represented as a matrix, , known as the Hessian
The quadratic update is:
Requires computing the inverse Hessian matrix with practical difﬁculties
The inverse of the Hessian may not exist as this matrix may be singular
With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive
Computing the full gradient and Hessian requires summing over all observations
For large scale problems quasi-Newton methods are used
Use an approximation to avoid computing the full inverse Hessian
Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm the most widely used quasi-Newton method
∇ 𝑙()𝜃⃗ 
𝑙()∇ 2𝜃⃗ 
= +𝛾| 𝑙() 𝑙()𝑥𝑛+1 𝑥𝑛 ∇ 2𝜃 𝜃⃗ |−1∇ 𝜃 𝜃⃗",850,semantic
0790ea36-dff2-4c9c-b2ec-971a9bd60fc0,Parameter Estimation and Likelihood.pdf,CSCI_83,38,"Limitations of MLE
The maximum likelihood estimator has a number of important limitations, including
Incorrect model for complex distributions
Parameter near limits
High dimensional problems
Correlated features",210,semantic
60676866-3a26-44fe-bf32-cc9989125dd0,Parameter Estimation and Likelihood.pdf,CSCI_83,39,"Incorrect model and complex distributions
In many real-world problems the distribution are not simple
Problematic for maximum likelihood methods
Consider a likelihood function that is only approximately correct
Population being modeled has a different distribution
Outliers in the form of erroneous samples
Example; maximum likelihood estimator for population with a mixture of simple distributions
Mixture has multiple modes
One mode is the maximum
Mode found by any gradient-based algorithm is dependent on the starting point
MLE algorithm will ﬁnd the nearest local maximum
No guarantee of ﬁnding the actual maximum likelihood point",635,semantic
09765e5a-9850-4cc9-be6a-8685e4b2f8de,Parameter Estimation and Likelihood.pdf,CSCI_83,40,"Incorrect model and complex distributions
Example; univariate Normal maximum likelihood estimator for mixture of 3 Normal distributions",135,semantic
62050c95-3bfc-40a3-9a7d-7e6a7b86cf56,Parameter Estimation and Likelihood.pdf,CSCI_83,41,"Parameter near limits
For many distributions, parameter values have limits
The log-likelihood function may have an extremely high gradient near the limit
Results can be poorly determined parameter estimates and slow convergence
Fisher information drops rapidly near these limits, indicating poorly determined gradient
Examples:
Variance estiamtes near 0; variables with low variance
Binomial parameter estimates near  and ; case with either very few successes or failure𝑝=0 𝑝=1",477,semantic
16c45ae8-801d-43b5-809d-1ea200646a88,Parameter Estimation and Likelihood.pdf,CSCI_83,42,"Parameter near limits
Example; Binomial likelihood and Fisher information with Extreme gradients and low information near limits,  and 
𝑝=0.5𝑝=0 𝑝=1",148,semantic
38977815-d11e-40b6-9c32-902a338a72f3,Parameter Estimation and Likelihood.pdf,CSCI_83,43,"High dimensional problems
The MLE method often ﬁnds poor solutions to problems in high dimensions
High dimensions means a large numbers of parameters
The likelihood function has corresponding high dimensionality; one dimension for each parameter
For high dimensional problems, it is often the case that the gradient and Hessian are not well determined
Uncertainty in the variables can lead to considerable uncertainty in determining the gradient in high dimensions
MLE algorithms may not converge or converge to results with a large uncertainty
Even with few parameters, MLE methods can have convergence problems
Example; difﬁculties ﬁtting the variance parameter while ﬁtting the location parameter for a univariate Normal distribution
We will discuss this problem later in the course",785,semantic
d6773d38-2239-45fb-95d8-8c7c18fccd7e,Parameter Estimation and Likelihood.pdf,CSCI_83,44,"Correlated features
Theory assumes that the variables used for MLE are independent
In reality, never truly the case
Some variables have a high correlation to each other
MLE algorithm can breakdown since gradients will not be well determined
We will discuss this problem later in the course",289,semantic
ca87c474-3e16-4bd7-8f4d-c4f24c9347e1,Parameter Estimation and Likelihood.pdf,CSCI_83,45,"Summary
Likelihood is a measure of how well a parametric model ﬁts a data sample
In most practical cases, we work with the log likelibood
 (𝐗| 𝜃)= 𝑓(|𝜃)∏𝑖=1
𝑛
𝑥𝑖
𝑙( | 𝐗)=𝑙𝑜𝑔( ( | 𝐗))= 𝑙𝑜𝑔(𝑓( | ))𝜃⃗  𝜃⃗  ∑𝑗
𝑥𝑗 𝜃⃗",211,semantic
b2f4dc96-c8b0-4873-810b-ccd37bef4d40,Parameter Estimation and Likelihood.pdf,CSCI_83,46,"Summary
Matrix is the observed information matrix of the model, . The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood
Fisher information or expected information is the expectation over the second derivative of the observed information
Fisher information relates to the score function as its variance
For large sample, , take the expectation over :
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, 
 ()𝜃⃗ 
 ()=−𝐄{ ()}=−𝐄{ }𝜃⃗  𝜃⃗   𝑙(𝐗|𝜃)∂2
∂𝜃2
   (0,1/ )∂ 𝑙(𝜃)∂𝜃 ∼ ˙  𝜃
𝑛→∞ 𝐗
 (𝜃, )𝜃̂ ∼ ˙ 1𝑛 (𝜃)
𝜃̂ 
𝑛",858,semantic
ec9bad19-ad52-4e1d-960c-8eb0e8bf9b1c,Parameter Estimation and Likelihood.pdf,CSCI_83,47,"Summary
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
The maximum likelihood for the model parameters is achieved when two conditions are met:
Gradient of the log-likelihood is known as the score function
=0∂ 𝑙(𝐗 | 𝜃))∂𝜃
<0 𝑙(𝐗 | 𝜃))∂2
∂𝜃2
𝑔𝑟𝑎𝑑(𝑙())=  𝑙()=𝜃⃗  ∇ 𝜃 𝜃⃗ 
⎛
⎝
⎜⎜⎜⎜⎜⎜
∂𝑙(𝐗 | 𝜃)∂𝜃1
∂𝑙(𝐗 | 𝜃)∂𝜃2
⋮ ∂𝑙(𝐗 | 𝜃)∂𝜃𝑛
⎞
⎠
⎟⎟⎟⎟⎟⎟",395,semantic
1e5165ca-1315-45f3-a02d-6d7dd0e1aa8d,Parameter Estimation and Likelihood.pdf,CSCI_83,48,"Summary
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Stochastic optimization uses a Bernoulli random sample of the data to estimate the expected update of the model weights
Where,  is the expected value of the gradient given the Bernoulli sample of the data, . Empirically, SGD has good convergence properties
𝜃̂ 𝑛 𝜃̂ 𝑛+1
= +𝛾   𝑙()𝜃̂ 𝑛+1 𝜃̂ 𝑛 ∇ 𝜃 𝜃̂ 
𝛾
= +𝛾  [ 𝐽()]𝜃𝑡+1 𝜃𝑡 𝐸𝑑𝑎𝑡𝑎𝑝̂  ∇ 𝜃 𝜃𝑡
[]𝐸𝑑𝑎𝑡𝑎𝑝̂  𝑑𝑎𝑡𝑎𝑝̂",635,semantic
00d7f549-3400-4665-b282-f9ef5d76cd77,Parameter Estimation and Likelihood.pdf,CSCI_83,49,"Summary
Newton’s method, and related methods, employ a quadratic approximation to optimization. Newton’s method exhibits convergence quadratic in the number of iterations
Compared to the approximate linear convergence for gradient descent methods
The quadratic update is:
 is a learning rate of step size
Requires computing the inverse Hessian matrix with practical difﬁculties
The inverse of the Hessian may not exist as this matrix may be singular
With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive
For large scale problems quasi-Newton methods are used
Use an approximation to avoid computing the full inverse Hessian
Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm the most widely used quasi-Newton method
= +𝛾| 𝑙() 𝑙()𝑥𝑛+1 𝑥𝑛 ∇ 2𝜃 𝜃⃗ |−1∇ 𝜃 𝜃⃗ 
𝛾",851,semantic
475a0986-ca8f-4658-b207-06eb2eb8cac4,Parameter Estimation and Likelihood.pdf,CSCI_83,50,"Summary
The maximum likelihood estimator has a number of important limitations, including
Incorrect model and complex distributions
Parameter near limits
High dimensional problems
Correlated features",199,semantic
349f5bde-b65b-4b22-975f-8a498463c362,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,0,"When One Thing Depends on Another; Conditional Probability
Steve Elston
09/15/2022",82,semantic
62d81b10-ecfe-40ab-97c5-b35eb43c50b4,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,1,"Introduction
Many real-world random variables depend on other random variables
Statistical models of complex processes invariably require the use of conditional probability distributions
Conditional probability is the probability that event A occurs given that event B has occurred
Write the conditional probability of A given B as:
Example: Model of the probability of contracting the infectious disease, depends on other variables
In more technical terms, the probability of contracting the disease is conditional on other random variables. Age, contact with people carrying the disease, immunity, etc. 𝑃(𝐴|𝐵)",611,semantic
325cb9dc-33fd-4e1f-929c-1ce10fe00990,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,2,"Properties of Conditional Probability
Example:
Example of conditional probability of discrete events; credit, Wikipedia commons
Sample space is the space of all possible events in the set 
Sample space is divided into several subspaces or subsets, ,  and 
Intersection is where the two sets overlap occur in both  and 
𝑆
𝐴𝐵 𝐶
𝐴 𝐵",329,semantic
dfc235cb-fc59-4197-ad9a-81612e31c690,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,3,"Properties of Conditional Probability
Example:
Example of conditional probability of discrete events; credit, Wikipedia commons
Intersection is where the two sets overlap occur in both  and 
First, ﬁnd the relationship between conditional probability and the intersection between the sets, 
The probability of the intersection is the product of two probabilities:
1. since B must be true to be in this intersection. 2.",418,semantic
2b3e4f6d-9f8f-4565-937f-3d8056d533f7,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,3,"since A must also occur when B is occurring
The result is:
𝐴 𝐵
𝑃(𝐴∩ 𝐵)
𝑃(𝐵)
𝑃(𝐴|𝐵)
𝑃(𝐴∩ 𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)",101,semantic
014d82d3-75f5-494d-9c13-4ead1d710467,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,4,"Properties of Conditional Probability
Rearranging terms we get the following:
We could have, just as well, written the last equation as:
Now, the probability of an identical event in the same intersection:
Factorization of a probability function is a key tool: notice that the factorization of a conditional probability distribution in not unique
𝑃(𝐴∩ 𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)
𝑃(𝐴|𝐵)=𝑃(𝐴∩ 𝐵)𝑃(𝐵)
= = =
210410
24 12
𝑃(𝐵∩ 𝐴)=𝑃(𝐵|𝐴)𝑃(𝐴)
𝑃(𝐴∩ 𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)=𝑃(𝐵∩ 𝐴)",460,semantic
0ce2f933-d9b4-4225-b86c-dd1a5e21caeb,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,5,"Set Operations and Probability
Set operations are applied to probability problems
1.",84,semantic
6110b77a-8cc7-4334-bb22-b509894cb301,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,5,"Intersection:
2. Union: is the sum of the probabilities of the sets minus the intersection between the sets:
3. Negation: Example, compute the probability of an event being in subset  but not in :
Example: We can apply De Morgan’s Laws:
𝑃(𝐴∩ 𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)
𝑃(𝐴∪ 𝐵)=𝑃(𝐴)+𝑃(𝐵)−𝑃(𝐴∩ 𝐵)
𝐴 𝐵
𝑃(𝐴 𝑎𝑛𝑑 ¬𝐵)=𝑃(𝐴)−𝑃(𝐵∩ 𝐴)
𝑃(¬(𝐴∪ 𝐵))𝑃(¬(𝐴∩ 𝐵))=𝑃(¬𝐴 ∩ ¬𝐵)=𝑃(¬𝐴 ∪ ¬𝐵)",353,semantic
cc919f8c-9d9c-4fae-849f-1ebd6b3d72b8,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,6,"Independence and Mutual Exclusivity
The factorization of probability distributions can be simpliﬁed if events are either independent or mutually exclusive
At ﬁrst glance, these concepts may seem similar
Are quite different
Very different implications
Independence of sets  and  means the occurrence of an event in , does not have any dependency on an event in 
Mutual exclusivity means events cannot occur in both the sets  and 
𝐴 𝐵 𝐴 𝐵
𝐴 𝐵",440,semantic
7b270e28-4f1c-46ce-bd15-55a567fa432c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,7,"Independence
Express independence of random variables, , mathematically:
But independence of A given B does not imply independence of B given A:
In other words, we need to pay attention to if A is independent of B or B is independent of A- One or the other could be true - Both could be true
𝐴⊥ 𝐵
𝑃(𝐴 ∩ 𝐵)𝑃(𝐴 ∪ 𝐵)𝑃(𝐴|𝐵)𝑃(𝐴|¬𝐵)
=𝑃(𝐴|𝐵)𝑃(𝐵)=𝑃(𝐴)𝑃(𝐵)=𝑃(𝐴)+𝑃(𝐵)−𝑃(𝐴)𝑃(𝐵)=𝑃(𝐴)=𝑃(𝐴)
𝑃(𝐴|𝐵)=𝑃(𝐴)⇎𝑃(𝐵|𝐴)=𝑃(𝐵)",400,semantic
234a14bb-8194-4976-bfcd-aa01cc06c108,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,8,"Mutual Exclusivity
If the intersection between events is an empty set:
Then, events in A are mutually exclusive of events in B:
And,  mutually exclusive of B, implies B is mutually exclusive of A
𝐴∩ 𝐵=∅
𝑃(𝐴∪ 𝐵)𝑃(𝐴|𝐵)
𝑃(𝐴|¬𝐵)
=𝑃(𝐴)+𝑃(𝐵)=0
= 𝑃(𝐴)1−𝑃(𝐵)
𝐴",252,semantic
dbb08a49-ffc1-4cf4-b205-ed1ec81c3321,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,9,"Conditional Distributions and Bayes’ Theorem
Bayes’ theorem, also known as Bayes’ rule, is a powerful tool to think about and analyze conditional probabilities
We can derive Bayes Theorem starting with the following relationships:
Now:
Which leads to:
Which is Bayes’ theorem! 𝑃(𝐴∩ 𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵)𝑃(𝐵∩ 𝐴)=𝑃(𝐵|𝐴)𝑃(𝐴)
𝑃(𝐴∩ 𝐵)=𝑃(𝐵∩ 𝐴)
𝑃(𝐴|𝐵)𝑃(𝐵)
𝑃(𝐴|𝐵)
=𝑃(𝐵|𝐴)𝑃(𝐴)
=𝑃(𝐵|𝐴)𝑃(𝐴)𝑃(𝐵)",375,semantic
3f148c5a-d4c7-4a0e-82ea-2bbf2ee0909a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,10,"Interpreting Bayes Theorem
How can we interpret Bayes’ theorem in a useful way? Consider an example using Bayes Theorem for an hypothesis test given some data or evidence
We must make an assertion of our prior probability that the hypothesis is true
We also must choose a likelihood function of the evidence given the hypothesis
𝑝𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠)
𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 | ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠)",385,semantic
cf1de5fc-8f7c-411c-b7e6-67c14c7ff4b3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,11,"Interpreting Bayes Theorem
Now, we can think of Bayes’ theorem in the following terms:
We discuss selection of prior probability distributions and likelihood functions in subsequent lectures
The denominator  or partition function is problematic
Required to normalize the posterior distribution to range 
𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠 | 𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)=𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 | ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠) 𝑝𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠)𝑃(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)
𝑃(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)
0≤𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠 | 𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)≤1",457,semantic
5ae9bf6b-f543-4017-b392-e225ee9b9d2e,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,12,"Interpreting Bayes Theorem
Denominator must account for all possible outcomes, or alternative hypotheses, :
This is a formidable problem! ℎ ′
𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠 | 𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒)=𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 | ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠) 𝑝𝑟𝑖𝑜𝑟(ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠)𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑(𝑒𝑣𝑖𝑑𝑒𝑛𝑐𝑒 |  ) 𝑝𝑟𝑖𝑜𝑟()∑∈  𝐴𝑙𝑙 𝑝𝑜𝑠𝑠𝑖𝑏𝑙𝑒 ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑒𝑠ℎ ′ ℎ ′ ℎ ′",306,semantic
2859b95c-05d0-4216-8159-c2ebfe04ba62,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,13,"Bayes Theorem Example
Hemophilia is a serious genetic condition expressed on any X chromosome
Women have two X chromosomes and are unlikely to exhibit hemophilia
One X chromosome inherited from each parent
Must inherit hemophilia from both parents
Men have one X chromosome and one Y chromosome
Inherit Y chromosome from the father
Inherit X chromosome, and possibly hemophilia, from the mother
Say a woman has a brother who exhibits hemophilia
X chromosome expression is  - brother has hemophilia with 
Woman’s father does not exhibit hemophilia,  - father has hemophilia with 
Our prior probability that she carries the genetic marker for hemophilia  - Woman’s X chromosome has P = 0.5 that it is from mother, who carried the marker with 
𝜃=1.0 𝑃=1.0
𝜃=0 𝑃=0.0
𝜃=0.5 𝑃=1.0",774,semantic
b0cd05a2-1ffa-4149-a9c4-d74a489bfca1,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,14,"Bayes Theorem Example
As evidence the woman has two sons (not identical twins) with no expression of hemophilia
What is the likelihood for the two sons  not having hemophilia? Two possible cases here
Case where woman caries one X chromosome with hemophilia expression, , and probability of passing to son = 0.5
Case where woman does not carry an X chromosome with hemophilia expression, 
Note: we are neglecting the possibility of a mutations in one of the sons
𝑋=( , )𝑥1𝑥2
𝜃=1
𝜃=0
𝑝( =0, =0|𝜃=1)=0.5∗ 0.5=0.25𝑥1 𝑥2
𝑝( =0, =0|𝜃=0)=1.0∗ 1.0=1.0𝑥1 𝑥2",548,semantic
156d661d-712c-439d-a6f0-63034b4fec5c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,15,"Bayes Theorem Example
Use Bayes theorem to compute probability woman carries an X chromosome with hemophilia expression, 
Where:
The evidence of two sons without hemophilia causes us to update our belief that the probability of the woman carrying the disease
𝜃=1
𝑝(𝜃=1|𝑋)= 𝑝(𝑋|𝜃=1)𝑝(𝜃=1)𝑝(𝑋|𝜃=1)𝑝(𝜃=1)+𝑝(𝑋|𝜃=0)𝑝(𝜃=0)
= =0.200.25∗ 0.50.25∗ 0.5+1.0∗ 0.5
𝐿𝑖𝑘𝑒𝑙𝑖ℎ 𝑜𝑜𝑑𝑃𝑟𝑖𝑜𝑟 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦𝑃𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 ℎ 𝑦𝑝𝑜𝑡ℎ 𝑒𝑠𝑖𝑠 𝑛𝑜 𝑚𝑎𝑟𝑘𝑒𝑟
=𝑝(𝑋|𝜃=1)=0.50.5=0.25=𝑝(𝜃=1)=0.5=𝑝(𝜃=0)=1−𝑝(𝜃=1)=0.5",470,semantic
e90977e3-ebdb-43f0-92cf-bdb706c686c3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,16,"Marginal Distributions
In many cases we are interested in the marginal distribution
Example, it is often the case that only one or a few parameters of a joint distribution will be of interest
In other words, we are interested in the marginal distribution of these parameters
The denominator of Bayes theorem, , can be computed as a marginal distribution
Consider a multivariate probability density function with  variables, 
Marginal distribution is the distribution of one variable with the others integrated out. Integrate over all other variables  the result is the marginal distribution, :
For discrete distribution the above is a summation
𝑃(𝑑𝑎𝑡𝑎)
𝑛 𝑝(, ,…, )𝜃1𝜃2 𝜃𝑛
{,…, }𝜃2 𝜃𝑛 𝑝()𝜃1
𝑝()= 𝑝(, ,…, ) 𝑑𝜃2,…,𝑑𝜃1 ∫,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛 𝜃𝑛
𝑝()= 𝑝(, ,…, )𝜃1 ∑,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛",770,semantic
9d0d69b7-d9e3-4568-b636-a4579bca556d,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,17,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = 0.014143391305686877  variance = 0.9422459154327474## For y mean = -0.030639573892598113  variance = 0.9834781862781161
𝜇=[0,0] 𝜎=[ ]1.00.0 0.01.0",310,semantic
aab53b5d-56c8-4773-a2e2-55444bbe0a3c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,18,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = -0.03097095972118977  variance = 0.9515116745292175## For y mean = -0.0618435252254139  variance = 1.0206970889966356
𝜇=[0,0] 𝜎=[ ]1.00.5 0.51.0",308,semantic
d7c1f405-97e5-43a2-9e27-caf4cd55b50a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,19,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = -0.03902993462783489  variance = 0.9970254226866206## For y mean = -0.06192143862563938  variance = 0.9812738103164559
𝜇=[0,0] 𝜎=[ ]1.00.9 0.91.0",309,semantic
daa0fba2-4fb2-4cfb-8c66-2583c0f090a7,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,20,"Conditional Probability Example
A simple and widely used example of using conditional probabilities to work out the chance of having a rare disease. Sickle Cell Anemia is a serious, but fairly rare, disease
The probability that a given patient, drawn at random from the population of all people in the United States, has the disease is. We can describe the possible events in diagnosing this condition as:
 a patient has the disease. a patient does not have the disease.",470,semantic
aa07a437-9a12-46a5-86b2-1f57a3752344,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,20,"patient tests positive. a patient tests negative. 𝑃(𝑆)= =0.000312513200
𝑆⇒
⇒𝑆′
⊕ ⇒
−⇒",85,semantic
f4fbeb71-7d15-4124-b9f7-ec657941c0ae,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,21,"Conditional Probability Example
What if a medical company claims that it has developed a test that is 99% accurate? We can write:
On the surface, it seems that a 99% reliable test is rather good
On average, 99 people out of 100 who have the disease will be identiﬁed and treated
But, dig into the conditional probabilities and make sure! 𝑃(𝑆|⊕ )=0.99
𝑃( |−)=0.99𝑆′",364,semantic
b7494453-5813-489a-819a-596a182d842d,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,22,"Conditional Probability Example
From the root the directed acyclic graphical model (DAG) deﬁnes a conditional dependency structure
Goal: Evaluate the medical test as a decision rule for treatment
Summarize the conditional probabilities for these outcomes:
: Conditional probability the test correctly identiﬁes patient with disease
: Conditional probability of a negative test for a patient with the disease; Type II Error or False Negative
: Conditional probability that a patient with no disease tests positive; Type I Error or False Positive
: Conditional probability of a negative test for a patient with no disease
Graph showing dependency of conditional distribution
𝑃(⊕ |𝑆)
𝑃(−|𝑆)
𝑃(⊕ | )𝑆′
𝑃(−| )𝑆′",706,semantic
51eb3fe1-eb02-4511-8e0b-fa2d015afdd3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,23,"Conditional Probability Example
Four possible outcomes shown using a confusion matrix or truth table
Table shows conditional probabilities of each outcome
Positive TestNegative Test
DiseaseTrue Positive RateFalse Negative Rate
No DiseaseFalse Positive RateTrue Negative Rate
Tip: Make sure the numbers in your confusion matrix sum to 1.0",337,semantic
88a791f2-bc68-4de1-b81b-e8fa20154487,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,24,"Summary
Conditional probability
One random variable depends on another
But not commutable
Mutually exclusivity
Independence
𝑃(𝐴|𝐵)=𝑃(𝐴)⇎𝑃(𝐵|𝐴)=𝑃(𝐵)
𝑃(𝐴|𝐵)=𝑃(𝐴)+𝑃(𝐵)
𝑃(𝐴|𝐵)=𝑃(𝐴)",176,semantic
46ec78b0-97b9-46a2-80d4-f2435cb3eb0a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,25,"Summary
Bayes’ theorem
Marginal distribution
𝑃(𝐴|𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)𝑃(𝐵)
𝑝()= 𝑝(, ,…, ) 𝑑𝜃2,…,𝑑𝜃1 ∫,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛 𝜃𝑛
𝑝()= 𝑝(, ,…, )𝜃1 ∑,…,𝜃2 𝜃𝑛
𝜃1𝜃2 𝜃𝑛",147,semantic
d9417461-20f5-4d62-ac52-a9f0263e8aae,Bayes MCMC Models.pdf,CSCI_83,0,"Bayes MCMC Models
Steve Elston
10/17/2022",41,semantic
c077bfac-7806-4523-a56d-27fcff21b8e4,Bayes MCMC Models.pdf,CSCI_83,1,"Review
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics with objective to compute a point estimate and conﬁdence interval from a sample
Bayesian models allow expressing prior information as a prior distribution
The posterior distribution is said to quantify our current belief
Belief is based on the posterior distribution
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ﬁnding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution",780,semantic
5e9ed4cd-ceb3-43c9-9d9c-8fcbbfef8d47,Bayes MCMC Models.pdf,CSCI_83,2,"Review
Two functions must be deﬁned to compute the posterior distribution
The likelihood for the model being used
The likelihood function includes the parameters for the model
Example; for Binomial likelihood is the probability of success
Example; for Normal likelihood is the mean,  and variance. The prior distribution encodes the information we have in advance about the model parameters
For simple cases is the conjugate distribution
The posterior distribution is in the conjugate family
Example, for binomial likelihood, the conjugate is the 
Example, for Normal likelihood, the normal distribution is the conjugate for the mean, 
𝜇 𝜎2
𝐵𝑒𝑡𝑎(𝛼,𝛽)
𝜇",652,semantic
4dd23382-c943-4911-9cda-69b35027676e,Bayes MCMC Models.pdf,CSCI_83,3,"Introduction
How can we extend Bayes models to more complex problems? For simple problems we can use a conjugate prior and posterior
Unlikely posterior distribution will be a simple conjugate
Need to perform sampling to compute approximation of complex posterior
We need highly efﬁcient sampling methods for complex problems",324,semantic
bdcdb1a7-bcfb-4cfb-a54d-62fe37ec5b7d,Bayes MCMC Models.pdf,CSCI_83,4,"Grid Sampling Cannot Scale! Real-world Bayes models have large numbers of parameters, into the millions
Naive approach is simple grid sampling
Sample across dimensions of the parameter space
Consider this thought experiment, sampling dimension 100 times:
1-parameter model:  samples
2-parameter model:  samples
3-parameter model:  samples
100-parameter model:  samples
Computational complexity of grid sampling has exponential scaling with dimensionality
Need a better approach! 100
=100001002
=1003 105
=100100 10102",517,semantic
0b14d29f-4f61-4c90-88dc-c8b6eb1f9c4c,Bayes MCMC Models.pdf,CSCI_83,5,"Scaling Bayesian models
How can we scale Bayesian models to 1000s of parameters? Variational methods
Based on Variational calculus
A very effective and efﬁcient method for some problems
But, proves difﬁcult us use as a general solution
Details beyond our scope here
Markov Chain Monte Carlo (MCMC)
A simple and reliable method for Bayesian inference
Our focus here",364,semantic
692e8a10-bbbb-43db-8d9c-53e760a0de9b,Bayes MCMC Models.pdf,CSCI_83,6,"Introduction to Markov Chain Monte Carlo
Large-scale Bayesian models need highly efﬁcient sampling methods
Markov chain Monte Carlo (MCMC) sampling is efﬁcient and scalable
Rather than sampling on a grid MCMC methods sample distributions efﬁciently
Sample higher density regions of posterior with higher probability
Requires effort to understand how the algorithm works
Must carefully evaluate how well algorithm converged
What to do when things go wrong",454,semantic
f7c5f9ba-9766-4cf0-9e63-e5f77fade621,Bayes MCMC Models.pdf,CSCI_83,7,"What is a Markov process? MCMC sampling uses a Markov processes sampling chain
A Markov process is a stochastic process that transitions from a current state, , to some next state, , with probability 
No dependency on past states
Summarize properties of a Markov process:
Probability of state transition is parameterized by a matrix of probabilities, , of dim N X N for N possible states
 only depends on the current state, 
Transition can be to current state. 𝑥𝑡 𝑥𝑡+1 Π
Π
Π 𝑥𝑡",477,semantic
0f949dac-ea07-4ef0-9083-51e59bbfc83b,Bayes MCMC Models.pdf,CSCI_83,8,"What is a Markov process? Since Markov transition process depends only on the current state a Markov process is memoryless
We can express the sequence of a Markov transition processes as:
The Markov process is memoryless
Transition probability only depends on the current state, 
No dependency on any previous states, . 𝑝( | =, , ,…, )=𝑝( | )𝑋𝑡+1𝑋𝑡 𝑥𝑡𝑥𝑡−1 𝑥𝑡−2 𝑥0 𝑋𝑡+1𝑥𝑡
𝑥𝑡
{ , ,…, }𝑥𝑡−1 𝑥𝑡−2 𝑥0",395,semantic
682e1d13-199a-4f52-8563-01703a796400,Bayes MCMC Models.pdf,CSCI_83,9,"What is a Markov process? For system with  possible states we can write the transition probability matrix, :𝑁 Π
Π=
⎡
⎣
⎢⎢⎢⎢
𝜋1,1
𝜋2,1
…𝜋𝑁,𝑖
𝜋1,2
𝜋2,2
…𝜋𝑁,2
…………
𝜋1,𝑁
𝜋2,𝑁
…𝜋𝑁,𝑁
⎤
⎦
⎥⎥⎥⎥
𝑤ℎ 𝑒𝑟𝑒=𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛 𝑓𝑟𝑜𝑚 𝑠𝑡𝑎𝑡𝑒 𝑗 𝑡𝑜 𝑠𝑡𝑎𝑡𝑒 𝑖𝜋𝑖,𝑗
𝑎𝑛𝑑=𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑠𝑡𝑎𝑦𝑖𝑛𝑔 𝑖𝑛 𝑠𝑡𝑎𝑡𝑒 𝑖𝜋𝑖,𝑖
𝑓𝑢𝑟𝑡ℎ 𝑒𝑟≠  𝑖𝑛 𝑔𝑒𝑛𝑒𝑟𝑎𝑙𝜋𝑖,𝑗 𝜋𝑗,𝑖",319,semantic
b07b4615-67f8-498f-bd3f-c75fe0a1c215,Bayes MCMC Models.pdf,CSCI_83,10,"Example of a Markov Process
To make the foregoing more concrete let’s construct a simple example. We will start with a system of 3 states, . The transition matrix is:
Some key points to notice
The probabilities of transition from a state is given in a column
The probabilities in each column must add to 1.0
The probabilities of a transition to the same state are along the diagonal of the matrix
Some transitions not possible have a probability of 0.0
Example, ; cannot transition from state 2 to 1
Example, ; cannot remain in state 3
{, , }𝑥1𝑥2𝑥3
Π= =⎡
⎣
⎢⎢⎢
𝜋1,1
𝜋2,1
𝜋3,1
𝜋1,2
𝜋2,2
𝜋3,2
𝜋1,3
𝜋2,3
𝜋3,3
⎤
⎦
⎥⎥⎥
⎡
⎣
⎢⎢
0.50.20.3
0.00.30.7
0.60.40.0
⎤
⎦
⎥⎥
=0𝜋2,1
=0𝜋3,3",671,semantic
807ec47d-302c-48be-a0a5-63e4ab717465,Bayes MCMC Models.pdf,CSCI_83,11,"Example of a Markov Process
Let’s apply a probability matrix to a set of possible states
The state vector represents being in the ﬁrst state at time step ; 
After a state transition, we compute the probability of being in each of the three possible states at the next time step, :
𝑡 =[1,0,0]𝑥𝑡→
𝑡+1
=Π  = =𝑥⃗ 𝑡+1 𝑥⃗ 𝑡
⎡
⎣
⎢⎢
0.50.20.3
0.00.30.7
0.60.40.0
⎤
⎦
⎥⎥
⎡
⎣
⎢⎢
100
⎤
⎦
⎥⎥
⎡
⎣
⎢⎢
0.50.20.3
⎤
⎦
⎥⎥",403,semantic
914d3199-c5c9-49b3-9904-8adebf934989,Bayes MCMC Models.pdf,CSCI_83,12,"From Markov process to Markov chain
The foregoing is a single step of a Markov process
What happens when there is a series of transitions? A sequence of such transitions is known as a Markov chain
There are two major behaviors observed with Markov Chains
Episodic Markov chains have a terminal state
The terminal state can only transition to itself
Once the system is in the terminal state, we say that the episode has ended
Episodic processes are not of direct interest here
Continuous Markov chains have no terminal state
Continue indeﬁnitely, at least in principle
Continuous Markov chains sample probability distribution
Are ideal for estimating Bayesian posterior distributions",682,semantic
aa18a0f0-b978-4f74-9141-60e72ca26fba,Bayes MCMC Models.pdf,CSCI_83,13,"From Markov process to Markov chain
Markov chain comprises a number of state transitions
Chain of  state transitions, 
Each transition has the probabilities given by the state transition matrix, 
To estimate the probabilities of being in the states we use a special case known as a stationary Markov chain
We will skip the technical mathematical details here
Over a large number of time steps the number of times the states are visited is proportional to the state probabilities
𝑛 {, , ,…, }𝑡1𝑡2𝑡3 𝑡𝑛
Π",502,semantic
f642e098-19f5-45e6-8f77-f1eb3e874885,Bayes MCMC Models.pdf,CSCI_83,14,"From Markov process to Markov chain
Start with initial state,  for a continuous Markov chain:
We can ﬁnd the probabilities of the states without knowing the values of the transition matrix, ! As long as we can repeatedly sample the stochastic Markov process, we can estimate the state probabilities
This is the key to Markov Chain Monte Carlo sampling
𝑥⃗ 0
Π Π Π …Π  =  𝑥⃗ 𝑡 Π𝑛𝑥⃗ 𝑡−→−−𝑛→∞𝑝(𝑥)→
Π",395,semantic
5cfa39b8-6873-4b80-9ec5-c7fcaaa3de2e,Bayes MCMC Models.pdf,CSCI_83,15,"MCMC and the Metropolis-Hastings Algorithm
The ﬁrst MCMC sampling algorithm developed is the Metropolis-Hastings (M-H) algorithm; often referred to as Metropolis algorithm or the M-Halgorithm. The M-H algorithm uses the following steps to estimate the posterior density:
Pick a starting point in the parameter space
Sample the posterior distribution according to the model, the product of the likelihood  and prior, 
Choose a nearby point in parameter space randomly and evaluate the posterior at this point
Use the following decision rule to accept or reject the new sample:
If the likelihood, , of the new point is greater than the current point, accept new point
If the likelihood of the new point is less than your current point, only accept with probability according to the ratio:
. If the sample is accepted, compute the posterior density at the new sample point
Repeat sampling steps many times, until convergence
𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) 𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)
𝑝( 𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) 
𝐴𝑐𝑐𝑒𝑝𝑡𝑎𝑛𝑐𝑒 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 = 𝑝(𝑑𝑎𝑡𝑎|𝑛𝑒𝑤 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑝(𝑑𝑎𝑡𝑎|𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)",1050,semantic
d25cced2-c6d3-49c0-9ebf-bb5a7e7c498f,Bayes MCMC Models.pdf,CSCI_83,16,"MCMC and the Metropolis-Hastings Algorithm
Eventually, the M-H algorithms converges to the posterior distribution
M-H random sampling algorithm is far more sample efﬁcient than naive grid sampling
Consider that the M-H algorithm probabilistically samples the parameter space
Preferentially sample high density areas
Not every point on a grid
Important properties of the Metropolis-Hastings MCMC algorithm include:
The M-H algorithm is guaranteed to eventually converge to the underlying distribution, but convergence can be quite slow
High serial correlation from one sample to the next in chain gives slow convergence",618,semantic
f5fb7f8c-4080-4fb7-867e-6bb1a03fcae6,Bayes MCMC Models.pdf,CSCI_83,17,"MCMC and the Metropolis-Hastings Algorithm
Poor convergence arises from low sample efﬁciency
Algorithm must be ‘tuned’ to ensure sample efﬁciency
Tuning ﬁnds a good dispersion parameter value for the state sampling distribution
Dispersion parameter determines the size of the jumps the algorithm makes
Example, for Normal distribution pick the variance 
 is too small, the chain only slowly searches
 is too big, chain has are large jumps which slows convergence
𝜎2
𝜎2
𝜎2",471,semantic
5f441804-5ba2-4fdd-8125-0e29f94f033a,Bayes MCMC Models.pdf,CSCI_83,18,"M-H algorithm example
Let’s try a simple example, ﬁnd an estimate of the posterior density of a bivariate Normal distribution
Random samples from a bivariate Normal distribution
𝜇=[ ].5.5
𝐶𝑜𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒=[ ]1,.6.6,1",210,semantic
a15529cd-b0b5-4852-8536-d53b3fc5e5c4,Bayes MCMC Models.pdf,CSCI_83,19,"M-H algorithm example
And, the marginal distributions of the variables
Marginal distributions of bivariate Normal samples",121,semantic
b11f300b-27a6-4a96-91a1-64b2b54e7f96,Bayes MCMC Models.pdf,CSCI_83,20,"M-H algorithm example
Now, we are ready to sample these data using the M-H MCMC algorithm
The algorithm is
1. Compute the bi-variate Normal distribution likelihood
2. Initialize the chain
3.",190,semantic
a9528ca5-0c69-4a1e-9c2a-6359d02260cf,Bayes MCMC Models.pdf,CSCI_83,20,"Initialize some hyperparameters statistics
4. Sample the likelihood of the data using the M-H algorithm. MCMC samples from bivariate Normal distribution",152,semantic
202d7b6d-5650-4c7c-a871-d640a7ab2f98,Bayes MCMC Models.pdf,CSCI_83,21,"M-H algorithm example
Plot the ﬁrst 500 samples
First 500 MCMC samples from bivariate Normal distribution
Notice the long ‘tail’ on the sampled distribution from the burn-in period.",181,semantic
6584a330-7614-4850-b423-3343e8751136,Bayes MCMC Models.pdf,CSCI_83,22,"M-H algorithm example
Marginal distributions of the MCMC samples, less ﬁrst 500
First 500 MCMC samples from bivariate Normal distribution
These marginals are similar to the original distribution",194,semantic
d98d5854-fa52-4285-9015-c72050e553f8,Bayes MCMC Models.pdf,CSCI_83,23,"Convergence and sampling efﬁciency of MCMC
How can we understand the onvergence properties of the M-H MCMC sampler
MCMC sampling generally convergences to the underlying distribution, but can be slow
In some pathological cases, convergence may not occur at all
The acceptance rate and rejection rate are key convergence statistics for the M-H algorithm
Low acceptnce or high rejection rate are signs of poor convergence
Too few rejections, indicate that the algorithm is not exploring the parameter space sufﬁciently
Trade-off between these statistics is controlled by the dispersion of the sampling distribution
Unfortunately, there are few useful rules of thumb one can use
For our example these statistics are fairly good:
𝐴𝑐𝑐𝑒𝑝𝑡𝑎𝑛𝑐𝑒 𝑟𝑎𝑡𝑒=0.81
𝑅𝑒𝑗𝑒𝑐𝑡𝑖𝑜𝑛 𝑟𝑎𝑡𝑒=0.19",766,semantic
ed9b7d51-761c-4b4d-8da4-8beb89cf2670,Bayes MCMC Models.pdf,CSCI_83,24,"Evaluation of MCMC sampling
Trace plot of the samples displays the sample value of the parameter with sample number
Trace plots for two variables from the M-H algorithm
The traces show sampling around the highest density values of the parameters, indicating good convergence",274,semantic
45390f16-0d19-453d-a7f9-e3265e94557a,Bayes MCMC Models.pdf,CSCI_83,25,"Evaluation of MCMC sampling
An autocorrelation plot shows how a sample value is related to the previous samples
The autocorrelation of the Markov chain sampling
Autocorrelation of Markov chain for the two parameters
Notice that the autocorrelation dies off fairly quickly with lag
Low auto correlation indicates good sampling efﬁciency",335,semantic
e179bc9b-4554-41c1-94d0-d039d89bb6a4,Bayes MCMC Models.pdf,CSCI_83,26,"Evaluation of MCMC sampling
We can relate sampling efﬁciency to the autocorrelation of the samples
Intuitively, uncorrelated samples provide maximum information on the distribution sampled
With signiﬁcant autocorrelation, the new information gathered per-sample is reduced
Effective sample size or ESS.is the ratio between the number of samples adjusted for the autocorrelation and the hypothetical number ofuncorrelated samples, N
ESS close to N indicates low autocorrelation and high sample efﬁciency
𝐸𝑆𝑆= 𝑁1+2 𝐴𝐶𝐹(𝑘)∑𝑘",521,semantic
3a2f9d39-b6b6-4dc0-859a-42dfe3509e41,Bayes MCMC Models.pdf,CSCI_83,27,"Other MCMC Sampling Algorithms
A number of other powerful MCMC sampling algorithms have been developed
The M-H can suffer from slow convergence for several reasons
Generally has fairly high serial correlation and low ESS
Must ‘tune’ the state selection probability distribution
As a result of these limitations, other MCMC sampling methods have been proposed in a quest to improve sample efﬁciency including:
Gibbs sampling
No U turn sampling (NUTS)",449,semantic
d04601f2-ca50-4af2-a148-9de17d38e340,Bayes MCMC Models.pdf,CSCI_83,28,"Gibbs sampling
Gibbs sampler is an improved MCMC sampler which speeds convergence
Named for the 19th Century physicist Josiah Willard Gibbs; inspired by statistical mechanics
Gibbs sampler samples each dimension of the parameter space sequentially in a round-robin manner
M-H algorithm attempts jumps across all dimensions of the parameter space. Compared to the M-H, Gibbs sampling reduces serial correlation through round-robin sampling
Update along each dimension approximately orthogonal to the preceding sampled dimension
There are no tuning parameters since sampling is based on the marginals of the likelihood.",617,semantic
38b8d2ba-4b10-42c6-96e9-2e4c2cf39bb2,Bayes MCMC Models.pdf,CSCI_83,29,"Gibbs sampling
The basic Gibbs sampler algorithm has the following steps:
1. For an N dimensional parameter space, , ﬁnd a random starting point
2. In order, , assign the next dimension to sample, starting with dimension ; actual order not important
3.",252,semantic
94fb0959-f8c8-452b-96c2-9e07fc6eac3e,Bayes MCMC Models.pdf,CSCI_83,29,"Sample the marginal distribution of the parameter given the observations, , and other parameter values: 
4. Repeat steps 2 and 3 until convergence
{, ,…, }𝜃1𝜃2 𝜃𝑁
{1,2,3,…,𝑁} 1
𝐷 𝑝(|𝐷, , ,…, )𝜃1 𝜃2𝜃3 𝜃𝑁",202,semantic
a503a779-d084-4f46-8c3d-6169d7dcb132,Bayes MCMC Models.pdf,CSCI_83,30,"Hamiltonian MCMC
The Hamiltonian sampler was proposed in 1987 (Duane, et.al.) and uses a simple idea from classical mechanics
Attempt to improve convergence of Metropolis-Hastings algorithm
M-H algorithm is slow converging random walk
Leads to low ESS
Hamiltonian MCMC attempts to create a better directed sampling path
Uses an analogy with classical physics
Constrains the search to favor high-density parts of the posterior distribution
Imagine that the posterior density is like a hilly landscape
We want to sample around the high spots, the maximum density points
We call density potential
We roll an imaginary ball around the landscape to the highest potential energy points
Ball has position and velocity in the space
These high density points attract the ball - a ‘ﬁeld of attraction’",791,semantic
bcf2320c-e7ae-4925-81e4-b1b2a4ec0448,Bayes MCMC Models.pdf,CSCI_83,31,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
Ball has potential energy and kinetic energy
Potential energy determined by probability density
Kinetic energy determined by rate of change of position, velocity
Hamiltonian arise from the principle of conservation of energy
Where:
 momentum vector
 position vector
 kinetic energy
 potential energy
The Hamiltonian of the system must remain constant over the sample space
𝙷(𝑞,𝑝)=𝙺(𝑝,𝑞)+𝚅(𝑞)
𝑝=
𝑞=
𝙺(𝑝,𝑞)=
𝚅(𝑞)=",496,semantic
355b1f51-3b74-4083-99a4-7a5cb0659c5b,Bayes MCMC Models.pdf,CSCI_83,32,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
We can relate position and momentum to the density we want to sample
The Boltzmann distribution of the Hamiltonian
Where:
 normalization or partition coefﬁcient
 temperature
𝑝(𝑞,𝑝)=𝑒−𝙷(𝑞,𝑝)𝑘𝑇
𝑘=
𝑇=",282,semantic
86e1d9ed-781f-4429-b66e-7e7e3c35b59d,Bayes MCMC Models.pdf,CSCI_83,33,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
Hamiltonian arise from the principle of conservation of energy
Solve for velocity and momentum as system of coupled differential equations:
Notice that HMCMC only works for distributions with ﬁnite derivatives
Works for nearly all continuous distributions
Cannot be applied to discrete distributions
The above is rather intimidating, and we skip the details! 𝙷(𝑞,𝑝)=𝙺(𝑝,𝑞)+𝚅(𝑞)
𝑑 𝑞𝑑 𝑡𝑑 𝑝𝑑 𝑡
= = +∂𝙷∂𝑝 ∂𝙺∂𝑝 ∂𝚅∂𝑝
= = +∂𝙷∂𝑞 ∂𝙺∂𝑞 ∂𝚅∂𝑞",515,semantic
ef693bbf-febc-490a-9b4f-16456ee7271c,Bayes MCMC Models.pdf,CSCI_83,34,"Hamiltonian MCMC
Hamiltonian MCMC is an extension of the M-H algorithm with steps:
1. Sample 
2. Simulate  and  for a  time steps to time , using the coupled differential equations
3.",183,semantic
dc39d3a4-54f7-478a-b935-3c2f74731aca,Bayes MCMC Models.pdf,CSCI_83,34,"Get  as the new proposed state
4. Apply the M-H acceptance criteria to 
Must ﬁnd good value of  to tune the algorithm
 too small, search is only local
 too large, search makes large jumps
Hard to ﬁnd good hyperparameter value in practice
𝑝∼ 𝙽(0,𝜎)
𝑞𝑡 𝑝𝑡 𝐿 𝑇
𝑞𝑇
𝑞𝑇
𝐿
𝐿
𝐿",269,semantic
56fbc46f-23d7-4a7a-88ee-c16718101dee,Bayes MCMC Models.pdf,CSCI_83,35,"No U-Turn Sampler
NUTS represents the state of the art in MCMC samplers: Hoffman and Gelman 2014
PyMC3 package uses the NUTS MCMC algorithm. NUTS improves on HMC
Run time forward and backward when solving coupled differential equations
Find equilibrium at the no U-turn point
Equilibrium point determines  automatically
Eliminates tuning
Why even discuss other samplers when we have the NUTS
NUTS works well in many common continuous distribution cases, it is not guaranteed to converge
In some cases a Gibbs sampler works better
Use M-H for discrete distributioins
𝐿",567,semantic
4b853435-2459-40e6-9d26-ee62c28b2c31,Bayes MCMC Models.pdf,CSCI_83,36,"Hamiltonian MCMC
Hamiltonian MCMC is a complex algorithm - some resources for additional details
A Conceptual Introduction to Hamiltonian Monte Carlo, Michael Betancourt, 2017
MCMC using Hamiltonian dynamics, Radford Niel, 2012
Hamiltonian Monte Carlo explained, Alex Rogozhnikov, 2016",285,semantic
ffd47a73-5229-420d-bf21-5cb2f431a05a,Bayes MCMC Models.pdf,CSCI_83,37,"Key Steps for MCMC Bayes Modeling
Focus of modern Bayes modeling is no evalaution
1. Deﬁne the problem
2. Collect the dataset
3.",128,semantic
cf217a74-1044-4bda-9d48-4ba8fb629353,Bayes MCMC Models.pdf,CSCI_83,37,"Deﬁne a model, including priors
4. MCMC sample the model
5. Verify MCMC sampling convergence and sufﬁciency
6. Prior predictive checks - are the priors reasonable for the problem? 7. Posterior predictive checks - Do the posterior predictions agree with the observed response? Packages like ArviZ are dedicated to evaluation of Bayes MCMC models. Multiple examples diagnostics for the breakdown of MCMC sampling can be found in an online Appendix to a seminal paper Vehtari et.al., 2019",485,semantic
16f29771-de58-4542-88f1-d54e051041fb,Bayes MCMC Models.pdf,CSCI_83,38,"Constructing an MCMC Model with PyMC
Deﬁne and construct a simple linear regression model
Simple dataset:
Dependent variable 
Independent variables , 
Data for regression example
𝑌
𝑥1𝑥2",185,semantic
2dbc9f39-22e7-4ace-8d57-49165d179fa2,Bayes MCMC Models.pdf,CSCI_83,39,"Constructing an MCMC Model with PyMC
Deﬁne and construct a simple linear regression model
Model has 4 parameters
Deﬁne prior distributions for these parameters:
 is the intercept term
 are the partial slopes
 is the standard deviation
, indicates a half Normal distribution
𝛽0
𝛽1
𝛽2
𝜎
∼ 𝙽(0,2)∼ 𝙽(0,2)∼ 𝙽(0,2)∼ |𝙽(0,1)
𝛽0
,𝛽1𝛽2
𝜎
|𝙽(.,.)",337,semantic
672d28ca-9ab4-40b1-9319-27a130cacfbb,Bayes MCMC Models.pdf,CSCI_83,40,"Constructing an MCMC Model with PyMC
Deﬁne and construct a simple linear regression model
Model has 4 parameters, 
Likelihood model:
Start with independent variables, ,
Sample posterior of parameters
Posterior distribution (likelihood) of observations, :
Expected value of the response,  is computed deterministically:
[, , ,𝜎]𝛽0𝛽1𝛽2
,𝑥1𝑥2
𝑌
𝑌∼ 𝙽(𝜇,𝜎)
𝜇
𝜇= + ∗ + ∗ 𝛽0 𝛽1 𝑥1 𝛽2 𝑥2",379,semantic
b29a8faf-52d8-441f-ab85-23b3bb750671,Bayes MCMC Models.pdf,CSCI_83,41,"Constructing an MCMC Model with PyMC
Deﬁne and construct a simple linear regression model
Use Python PyMC package to construct the model
with pymc3.Model() as regression_model:    # Priors for unknown model parameters
    betas = pymc3.Normal(""betas"", mu=0 , sigma=2 , shape=3 )    sigma = pymc3.HalfNormal(""sigma"", sigma=1 )
    # Deterministic expected value of outcome
    mu = betas[0 ] + betas[1 ] * x1 + betas[2 ] * x2
    # Likelihood (sampling distribution) of observations
    Y_obs = pymc3.Normal(""Y_obs"", mu=mu, sigma=sigma, observed=Y)",547,semantic
48a7903a-2c73-42b5-a600-233c0da91dc5,Bayes MCMC Models.pdf,CSCI_83,42,"Prior Predictive Checks
Prior predictive checks to verify prior distribution choice
Prior predictive checks verify that choices of prior distributions are at least reasonable
Compare the distribution of the response variable with the posterior distribution generated by priors. Is the posterior distribution in the absence of evidence (data)
Prior predictive response vs response variable distribution
Example looks promising
Distribution shape is similar
No noticeable anomalies
Higher dispersion of prior check distribution is desired to not restrict the solution",565,semantic
bbbddde2-5f5c-4c74-81d5-92aaca9e76bc,Bayes MCMC Models.pdf,CSCI_83,43,"Prior Predictive Checks
Prior predictive checks to verify prior distribution choice
Prior predictive checks verify that choices of prior distributions are at least reasonable
Verify that we are happy with the density of the  and  priors
Prior density of Betas and sigma
Shape of these distributions seems reasonable
Range of possible values will not restrict the solution
No anomalies in the shape of the distribution
𝛽 𝜎",421,semantic
6f31d442-4a0f-4c92-9a9c-6f800c355650,Bayes MCMC Models.pdf,CSCI_83,44,"Sampling and Inference with Model
Evaluation the sampled posterior and perform inference
We sample the posterior using one or more Markov chains
Examine the traces (path of chains)
Traces and posterior density of MCMC traces
Traces and density of chains are similar
No divergence between traces
2 s have limited uncertainty
 has high uncertainty as expected
 has limited uncertainty
𝛽
𝛽2
𝜎",389,semantic
35896eb3-8a1a-441f-8556-64e0797ca70b,Bayes MCMC Models.pdf,CSCI_83,45,"Sampling and Inference with Model
Evaluation the sampled posterior and perform inference
There are a number of ways MCMC sampling can breakdown and not converge;
Example from Vehtari et.al., 2021, a seminal paper on evaluation of MCMC",234,semantic
dcb89e0b-eef0-4ca2-be4a-9ce23f609e4c,Bayes MCMC Models.pdf,CSCI_83,46,"Sampling and Inference with Model
We must verify that the samples are representative
Table of MCMC convergence statistics
A lot of information in this summary table
1. The mean MCMC error
Estimated error tells us how much error the MCMC sampling has introduced
2.",263,semantic
0706e4e9-6735-41c9-a323-4a6aa4cd0114,Bayes MCMC Models.pdf,CSCI_83,46,"The standard deviation (sd) of the mean error of the posterior distribution
3. Metrics of effective sample size (ESS)
ESS of bulk (middle) portion of posterior distributions
ESS for tails ESS of posterior distributions
4. The Gelman-Rudin statistic () measures the ratio of the variance shrinkage between chains to the variance shrinkage withinchains
Gelman-Rudin statistic should converge to 1.0
If all chains converge, the reduction in variance between chains and within the chains should is same
𝑅̂",501,semantic
ed885822-af6e-4e21-a56c-beea3f6b09f2,Bayes MCMC Models.pdf,CSCI_83,47,"Sampling and Inference with Model
We must verify that the samples are representative
Plot the MCSE by quantile for each model parameter
MCSE by quantile for each model parameter
Ideally want MCSE to be small and uniform with quantile
Notice the MCSE is higher for outer quantiles
Not an unusual situation since less sampling in thin tails of distributions
Overall MCSE is low, even in tails",390,semantic
a2ebd9b1-be40-4bf6-ba27-81e14b6afdae,Bayes MCMC Models.pdf,CSCI_83,48,"Sampling and Inference with Model
We must verify that the samples are representative
Plot the ESS locally and by quantile for each model parameter
ESS locally and by quantile for each model parameter
Ideally want ESS to be large and uniform with quantile
Local ESS is fairly uniform with quantile
Notice the quantile ESS is lower for outer quantiles
Not an unusual situation since less sampling in thin tails of distributions
Overall ESS is reasonably high, even in tails",471,semantic
69dd44aa-7b1a-4341-b2b6-3ad0b47a44fc,Bayes MCMC Models.pdf,CSCI_83,49,"Sampling and Inference with Model
Evaluate the sampled posterior and perform inference
Examine HDIs of model parameters by trace with tree plot
HDIs of model parameters by trace
HDIs are similar for each parameter by trace
HDIs conﬁrm inferences on uncertainty of the model parameters",284,semantic
36e48d28-5e97-4352-bc39-b5cbe4267e2d,Bayes MCMC Models.pdf,CSCI_83,51,"Sampling and Inference with Model
Evaluate the sampled posterior and perform inference
How can we interpret the results? Posterior distribution of parameters with HDIs
Interpret s by examining HDIs
 is the mean response for the centered independent variable
 is partial slope with respect to 
 coefﬁcient large compared to  with high uncertainty
 has small value with limited uncertainty
𝛽
𝛽0
𝛽1 𝑥1
𝛽2 𝛽1
𝜎",406,semantic
52932fa4-7350-49f0-bbe5-67580dbd90d0,Bayes MCMC Models.pdf,CSCI_83,52,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Does the posterior distribution look like the distribution of the observed response? Can simply compare density plots
Apply a test statistic 
Bayesian p-value
Bayesian u-value
Others
𝑇",273,semantic
eb6b63ae-0136-449e-95f4-6fc7a7d4c901,Bayes MCMC Models.pdf,CSCI_83,53,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Base statistic on posterior predictive distribution
Approximate PPD by resampling from the posterior distribution:
Where: realization drawn from the posterior distribution observation draw from posterior distribution with parameters  posterior distribution of model parameters, 
We can sample the posterior as many times as required to get better estimates of 
𝑝( |𝑦)= 𝑝( |𝜃)𝑝(𝜃|𝑦)𝑦𝑟𝑒𝑝 ∑𝑖
𝑦𝑟𝑒𝑝
=𝑦𝑟𝑒𝑝
𝑦=𝑝( |𝜃)=𝑦𝑟𝑒𝑝 𝜃𝑝(𝜃|𝑦)= 𝜃
𝑝( |𝑦)𝑦𝑟𝑒𝑝",524,semantic
f764eeaa-cc43-4553-811c-ed1de239b3cd,Bayes MCMC Models.pdf,CSCI_83,54,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive distribution is compared to the distribution of observations
Posterior predictive distribution vs. observed responses
Agreement is generally good between posterior predictive and actual observations
Average predictive is heavier in the tails",351,semantic
1b6d7fce-1f6f-44e5-a63d-a7deee61a103,Bayes MCMC Models.pdf,CSCI_83,55,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Bayesian p-value is a test statistic, , for the distribution differences between observed and predicted responses:
Intuitively, for representative posterior,  over many draws of 
 half the time
 half the time
Distribution should be symmetric
𝑇
𝑝=𝑝( <𝑦|𝑦)𝑦𝑟𝑒𝑝
𝑝=0.5 𝑦𝑟𝑒𝑝
<𝑦|𝑦𝑦𝑟𝑒𝑝
>𝑦|𝑦𝑦𝑟𝑒𝑝",376,semantic
c4637a09-0545-4b8e-9fd7-e05bf570eadd,Bayes MCMC Models.pdf,CSCI_83,56,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive Bayesian p-value
Distribution of Bayesian p-values
Plot is centered on 0.5
Distribution is symmetric",210,semantic
5892cf62-36d6-4bbf-9511-25193b59c06c,Bayes MCMC Models.pdf,CSCI_83,57,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive Bayesian u-value is also known as the marginal p-value:
U-value computed for speciﬁc (ordered) observation 
 should be centered on 1.0
 should be close to uniformly distributed
Distribution of Bayesian u-values
The values are close to 1.0 across all values of the observatons
=𝑝( <|𝑦)𝑝𝑖 𝑦𝑟𝑒𝑝𝑖 𝑦𝑖
𝑦𝑖
𝑝𝑖
𝑝𝑖",414,semantic
9346f735-3ab0-4e54-9824-01dfb5dc7645,Bayes MCMC Models.pdf,CSCI_83,58,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Toward the end of 19th Century was growing public awareness of the rising coal mine deaths
Mine disaster deﬁned as incident leading to 6 or more deaths
As early as 1886, Royal Commission published recommendations for safety practices
Adoption of these practices remained voluntary
British Coal Mines Act of 1911 ﬁnally codiﬁed mine safety procedures into law.",443,semantic
42dcc440-94ab-4dba-9619-2aa7bf8f02cc,Bayes MCMC Models.pdf,CSCI_83,59,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Famous dataset from Kaggle, and other sources
Goal to create a model to quantify the differences in the rate of mine disasters before and after the introduction of safety practices
Example of a point process model
Point process model has an intensity or rate of occurrence of events, e.g. mine disasters
Number events over a time period for this type of point process is iid
The number of events per time period are a iid draw from some probability distribution
Because of the iid nature of the point process, no autoregressive (AR) effect",623,semantic
2fbbb38a-8161-4146-aa6f-fdd2855e6311,Bayes MCMC Models.pdf,CSCI_83,60,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
iid nature of the point process does not mean the distribution of the process cannot change in time
Change points or switch points in time are quite common in the real-world
Rates changes at change point
Rate constant betwen change point
Identifying these change points is often a challenging problem",384,semantic
0e51b9f4-7550-4ad9-b945-f033710545f3,Bayes MCMC Models.pdf,CSCI_83,61,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Number of disasters per year 1851-1962
Two related questions we can ask about these data
At what point in time was there a signiﬁcant change in intensity, the switch point? Not obvious, changes in safety practice were adopted over about 25 years
How signiﬁcant was the reduction in the rate?",375,semantic
d9f94529-eeb4-4756-b031-032687016fec,Bayes MCMC Models.pdf,CSCI_83,62,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Poisson likelihood model with switch switch point , rate before,  and after :
Prior distributions of parameters, uniform and exponential:
𝑠 𝑒 𝑙
∼ 𝑃𝑜𝑖𝑠(),  {𝐷𝑡 𝑟𝑡 𝑟𝑡 𝑒, 𝑡<𝑠𝑙, 𝑡≥𝑠
𝑠𝑒𝑙
∼ 𝑢𝑛𝑖𝑓( , )𝑡𝑚𝑖𝑛𝑡𝑚𝑎𝑥
∼ 𝑒𝑥𝑝(2)∼ 𝑒𝑥𝑝(2)",302,semantic
502c8c7b-251c-4d1a-b1bd-4dc335a4ab36,Bayes MCMC Models.pdf,CSCI_83,63,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Deﬁne model using PyMC
with pymc3.Model() as disaster_model:    # Uniform prior on the switch point  
    switchpoint = pymc3.DiscreteUniform(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max())    # More informative prior based on domain knowledge    #    switchpoint = pymc3.Triangular(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max(), c=1900)
    switchpoint = pymc3.DiscreteUniform(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max())
    # Priors for pre- and post-switch rates 
    early_rate = pymc3.Exponential(""early_rate"", 2 . 0 )    late_rate = pymc3.Exponential(""late_rate"", 2 . 0 )
    # Poisson rate switch for years before and after current
    rate = pymc3.math.switch(switchpoint >= disaster_data.Year, early_rate, late_rate)
    disasters = pymc3.Poisson(""disasters"", rate, observed=disaster_data.Count)",986,semantic
1d5de063-24da-4f91-bf26-6171f26bad87,Bayes MCMC Models.pdf,CSCI_83,64,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Distributions and traces from sampling
HDI of parameters by trace
Good agreement between traces",179,semantic
792f72f6-fa59-4a21-8609-afe23cadb4d8,Bayes MCMC Models.pdf,CSCI_83,65,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Examine the posterior HDI of the parameters for each trace",142,semantic
ac06969f-c2fd-467e-876c-a0a39d6e0e84,Bayes MCMC Models.pdf,CSCI_83,66,"HDI of parameters by trace
The HDIs are consistent across traces",64,semantic
64f8ba80-1573-4672-bfe4-15e10c13dffd,Bayes MCMC Models.pdf,CSCI_83,67,"See notebook for MCMC, Prior and Posterior checks",49,semantic
75a31ffa-3632-48be-ab04-45c19768e508,Bayes MCMC Models.pdf,CSCI_83,68,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Examine the posterior HDI of the parameters
Posterior distribution of parameters with HDI
HDI of switch point is in reasonable range
Signiﬁcant difference in HDI of rates",254,semantic
16df5d65-aa6a-4e02-8f25-d09178531d3b,Bayes MCMC Models.pdf,CSCI_83,69,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Posterior Predictive checks
Posterior predictive checks",139,semantic
11fd2382-f2b9-4e5e-ab90-327741f3e240,Bayes MCMC Models.pdf,CSCI_83,70,"Summary
For complex Bayesian models we need a computationally efﬁcient aproximation
Grid sampling is inefﬁcient
MCMC sampling is based on Markov chains
Markov process is memoryless
Sampling converges to probability distribution
Several MCMC sampling methods have been developed
Metropolis-Hastings (M-H) algorithm uses random sampling with acceptance probability
Gibbs sampling round robins on the dimensions of the parameter space
NUTS
NUTS is the state of the art MCMC algorithm
Uses a ﬁeld of attraction to the highest density parts of the parameter space
No hyperparameters to tune",585,semantic
a95ff823-a744-415e-ba39-63e2d750ce71,Bayes MCMC Models.pdf,CSCI_83,71,"Summary
Performance metrics of MCMC sampling
Sample efﬁciency
Serial correlation reduces sample efﬁciency
ESS
Convergence
Multiple chains should converge to the similar distributions
Want between chain and within chain variance to be the same",242,semantic
5ef3055a-76d4-48ff-bff2-18019bbe62b7,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,0,"Introduction to Inference and Conﬁdence Intervals
Steve Elston
10/06/2022",73,semantic
fcd18f02-853c-4ed8-83e7-882bfcf9f6e0,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,1,"Review
Likelihood is a measure of how well a parametric model ﬁts a data sample
In most practical cases, we work with the log likelibood
The maximum likelihood for the model parameters is achieved when two conditions are met:
Gradient of the log-likelihood is known as the score function
 (𝐗| 𝜃)= 𝑓(|𝜃)∏𝑖=1
𝑛
𝑥𝑖
𝑙(𝐗| 𝜃)=𝑙𝑜𝑔( (𝐗| 𝜃))= 𝑙𝑜𝑔(𝑓( | ))∑𝑗
𝑥𝑗 𝜃⃗ 
=0∂ 𝑙(𝐗| 𝜃)∂𝜃
<0 𝑙(𝐗| 𝜃)∂2
∂𝜃2",385,semantic
c63f1107-2610-4953-a520-ca85b12383ed,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,2,"Review
Matrix is the observed information matrix of the model, . The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood
Fisher information or expected information is the expectation over the second derivative of the observed information
Fisher information relates to the score function as its variance
For large sample, , take the expectation over :
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, 
 ()𝜃⃗ 
 ()=−𝐄{ ()}=−𝐄{ }𝜃⃗  𝜃⃗   𝑙(𝐗|𝜃)∂2
∂𝜃2
   (0,1/ )∂ 𝑙(𝜃)∂𝜃 ∼ ˙  𝜃
𝑛→∞ 𝐗
 (𝜃, )𝜃̂ ∼ ˙ 1𝑛 (𝜃)
𝜃̂ 
𝑛",857,semantic
571a0288-a268-4b7e-8ccc-9d8c4759769f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,3,"Review
The gradient descent method ﬁnds the maximum of the log-likelihood function by following the gradient ‘uphill’
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Stochastic optimization uses a Bernoulli random sample of the data to estimate the expected update of the model weights
Where,  is the expected value of the gradient given the Bernoulli sample of the data, . Empirically, SGD has good convergence properties
𝜃̂ 𝑛 𝜃̂ 𝑛+1
= +𝛾   𝑙()𝜃̂ 𝑛+1 𝜃̂ 𝑛 ∇ 𝜃 𝜃̂ 
𝛾
= +𝛾  [  𝑙()]𝜃̂ 𝑛+1 𝜃̂ 𝑛 𝐸𝑑𝑎𝑡𝑎𝑝̂  ∇ 𝜃 𝜃̂ 
[]𝐸𝑑𝑎𝑡𝑎𝑝̂  𝑑𝑎𝑡𝑎𝑝̂",640,semantic
623f0236-a269-4131-b02a-540419bec4c4,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,4,"Review
The maximum likelihood estimator has a number of important limitations, including
Incorrect model and complex distributions
Parameter near limits
High dimensional problems
Correlated features",198,semantic
1ebf5bd6-a335-4686-a1d1-636ae57034f2,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,5,"Introduction to Statistical Inference
All statistical inference has uncertainty
Characterization of uncertainty is a goal of statistical inference
Any model using real-world data has inherent uncertainty
Statistical inference seeks to avoid being fooled by randomness; A catchy title of a book
A very few examples of false inference from randomness
Hypothesis Fooled by Randomness
How certain are you that eating fast food improves your health? Some of my friends are doing great on this diet
How much conﬁdence should we have that a marketing campaign increased sales?Sales are up in the last month since the campaign started
How effective is a certain stock trading strategy for actually improving returns? Traders using this strategy have made money recently
How good are the model parameter estimates?",805,semantic
c6b88b6b-9e48-4b32-ba22-03f71063a4c8,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,5,The model has made accurate predictions so far,46,semantic
98b56c63-3031-425a-8a3c-a5a3af984836,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,6,"Applications of Statistical Inference
Confusingly, the term statistical inference is applied in many contexts
Some applications of statistical inference include:
Inference on differences in distributions: Are samples drawn from the same distribution or not? - A null hypothesis is the hypothesis that thedistributions are the same
Inference for model parameters and model selection: Statistical models, including machine learning models, have unknown parameters for whichthe values must be estimated- Compute uncertainty in model parameter estimates- Compare model performance
Inference for prediction: In recent decades the distinction between prediction and classical inference has blurred to the point of being irrelevant- A common machine learning example, classiﬁcation, uses decision rules determine the most likely class for each case of input values- inference also used to determine the conﬁdence in predictions",920,semantic
08221610-4afd-4cd1-a352-8ce9cd20027f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,7,"Types of Hypothesis Tests
Different tests for one sample, two samples or more
Parametric test uses assumptions about the distribution of the data
Non-parametric tests make no distribution assumptions
Select test for discrete or continuous, or numeric or categorical, ordinal, interval, or ratio variables
Tests can compare effect size (averages), variance, goodness of ﬁt, distribution assumptions, or Independence",414,semantic
dc4d736b-ba3f-45b9-9d7f-c4f799eaf033,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,8,"Inference for Hypotheses
Hypothesis testing seeks to answer the question, are the differences in a statistic statistically signiﬁcant?",134,semantic
3d33485e-add9-4703-8d0d-34ea98b0c081,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,8,"Statistical signiﬁcance must not be confused importance to the problem being addressed? In 1922, Ronald A. Fisher warned:
“I believe that no one who is familiar, either with mathematical advances in other ﬁelds, or with the range of special biological conditions to be considered, would everconceive that everything could be summed up in a single mathematical formula, however complex.”",386,semantic
f19f26a6-5def-4ae0-bf60-b94b91ac35f1,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,9,"Inference for Hypotheses
Many statisticians would say that a statistically signiﬁcant result indicates a relationship worthy of further investigation
Examples of further investigation include:
Gather more data, either observational or experimental
Find other variables which might illuminate the relationship under investigation
Consider the theoretical basis of the relationship. E.G., can known science add understanding?",423,semantic
4e472565-6ae8-48b0-815e-59359c5dc083,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,10,"Conﬁdence Intervals; the Key to Inference
In frequentist statistics uncertainty of an inference is expressed in terms of a conﬁdence interval
A conﬁdence interval is deﬁned as the expected range of a statistical point estimate
A point estimate is the best estimate of a statistic
For example, the maximum likelihood estimate of a model parameter given the data
Two types of of conﬁdence intervals:
Two-sided conﬁdence intervals: express conﬁdence that a value is within some range around the point estimate
One-sided conﬁdence intervals: express conﬁdence that the point estimate is greater or less than some range of values",624,semantic
43222f1e-516d-44a3-8eb5-ffdbd779334b,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,11,"Conﬁdence Intervals; the Key to Inference
Can understand two sided conﬁdence interval by looking at the  and  quantiles of a distribution
Conﬁdence interval corresponds to the span of the distribution between quantiles
Express a two-sided conﬁdence interval for a random variable, , in terms of the probability as:
Example: For Normal distribution the right and left tail probabilities are :
𝛼/2 1−𝛼/2
𝐱
1−𝛼=𝑃(𝐿𝑜𝑤𝑒𝑟 𝐶𝐼≤𝐱≤𝑈𝑝𝑝𝑒𝑟 𝐶𝐼)
𝛼/2
1−𝛼/2=𝑃(−∞≤𝑥≤𝐿𝑜𝑤𝑒𝑟 𝐶𝐼),𝑎𝑛𝑑,1−𝛼/2=𝑃(𝑢𝑝𝑝𝑒𝑟 𝐶𝐼≥𝑥≥∞)",483,semantic
64b6f027-d613-4a35-9d78-fc94465c3c46,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,12,"Conﬁdence Intervals; the Key to Inference
Can understand one sided conﬁdence interval by looking at either  or  quantiles of a distribution
Characterize uncertainty of maximum or minimum value of a random variable
For lower one-sided CI:
For upper one-sided CI:
Example: one-sided CIs of Normal distribution, starting with the lower CI:
Or for the upper conﬁdence interval,
Or,
𝛼 1−𝛼
𝛼=𝑃(𝑥≤𝐿𝑜𝑤𝑒𝑟 𝐶𝐼)
1−𝛼=𝑃(𝑥≤𝑈𝑝𝑝𝑒𝑟 𝐶𝐼)
𝛼=𝑃(−∞≤𝑥≤𝑙𝑜𝑤𝑒𝑟 𝐶𝐼)
1−𝛼=𝑃(−∞≤𝑥≤𝑢𝑝𝑝𝑒𝑟 𝐶𝐼)
𝛼=𝑃(𝑢𝑝𝑝𝑒𝑟 𝐶𝐼≥𝑥≥∞)",475,semantic
3cdd8ae6-fd49-4890-b9d8-796a13427f51,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,13,"Example; conﬁdence intervals of the Normal distribution
Illustrate the concept of conﬁdence intervals with an example
The cumulative distribution function (CDF) of a standard Normal distribution, 
Double ended arrows with annotation are plotted to illustrate the two-sided 95% conﬁdence interval on the CDF
Horizontal double arrow shows the range of the conﬁdence interval
Vertical double arrow shows the part of the distribution within the conﬁdence intervals
## Confidence Interval, lower: -1.96, upper:  1.96
𝑁(0,1)",518,semantic
f06d4909-e4bb-45f9-90d0-586cd444bbee,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,14,"Example, Inference for the mean
Where,-  is the standard Normal distribution evaluated at conﬁdence level, -  is the maximum likelihood estiamte of the mean- Standard error is given by -  is the standard deviation estimate-  is the number of samples
𝐶𝐼(𝑚𝑒𝑎𝑛)=𝐶𝐼()=𝑀𝐿𝐸(𝜃|𝐗) ± 𝐗¯ 𝑠𝑛√𝑍𝛼
𝑍𝛼 𝛼𝑀𝐿𝐸(𝜃|𝐗) 𝑠/𝑛√𝑠𝑛",303,semantic
1203de73-4163-41c5-874f-caa9bd7957fd,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,15,"Interpretation of Conﬁdence Intervals
How can we interpret the conﬁdence interval? Conﬁdence intervals are with respect to the the sampling distribution of a statistic 
CIs are a measure of variation from sampling alone with probability  - the basis of hypothesis testing! With probability  the sample statistic values computed from resamples of the population, are within the CI
Conﬁdence intervals do not indicate the probability  is in a range
Sampling distribution of unknown population parameter
𝑠() ̂ 
𝛼
𝛼  ̂ 𝑖
𝑠() ̂",522,semantic
9c972e92-286d-4827-91ba-416e7482558f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,16,"Conﬁdence and Hypothesis Testing
Purpose of a statistical test is determining if the value of a chosen test statistic exceeds a cutoff value
Select the cutoff value based on the conﬁdence we wish to have in the test result
Example, by specifying a cutoff of 0.05 we are saying that the probability that the test statistic exceeding the cutoff from random variation alone is 0.05
Once the cutoff value has been set and the test statistic computed, interpret the results:
If the test statistic does not exceed the cutoff value, fail to reject the null hypothesis
If the test statistic exceeds the cutoff reject the null hypothesis",628,semantic
7539aef7-1a06-42df-b2b4-d3a25bf84e55,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,17,"Cutoffs for Hypothesis Tests
Examples of one-sided and two-sided cut-offs
Cutoff is probability, , that variation this great or greater arises from random sampling alone
In other words, cutoff is the conﬁdence we have in rejecting a null hypothesis. A p-value is the probability of the statistic this extreme or more extreme arising from random sampling (random variation)alone
## (0.0, 1.0, 0.0, 1.0)
𝛼",403,semantic
d6df208c-020c-4bf0-8ce2-fcafcbc8df88,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,18,"Hypothesis testing steps
The steps required to perform a formal two-sample hypothesis test
State population assumptions of null hypothesis: 
State alternative hypothesis: 
Typically stated in terms of a treatment group vs. control group
Treatment is the factor that differentiates the population
Decide on a signiﬁcance level (probability cutoff or Type I error threshold): e.g. 0.1, 0.05, and 0.01
Data is collected for the different treatments
Treatment used for comparison is the control
Compute the test statistic and evaluate based on the cutoff value
𝐻0
𝐻𝑎",562,semantic
a4747308-da9e-46ae-94ba-8282b2e73a3b,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,19,"Hypothesis testing steps
Test statistic used a a decision rule
Only two possible outcomes
Reject the null-hypothesis: This is not the same as accepting the alternative hypothesis
Fail to reject the null hypothesis: This is not the same as accepting the null hypothesis
Failing to rejecting the null hypothesis can occur for several reasons
The alternative hypothesis was false to begin with
There is not enough evidence for the effect size
Roughly speaking, the effect size is the difference of the test statistic in populations under the different treatments",559,semantic
a7bd01eb-aaad-4f91-a6bd-ca1b23f8d660,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,20,"the p-value
P-value is the probability, under , that we get a statistic as extreme or more extreme than the one we got
Extreme depends on whether the test is one-tailed or two-tailed
Derive the p-value by computing the statistic(s) and evaluate the quantile of the null distribution, 
Low p-value means our evidence against  is too strong to ignore
How strong it needs to be is controlled by our choice of 
Smaller  means we need stronger evidence to reject ; a more conservative test
Together the p-value and the signiﬁcance threshold  determine whether we reject or fail to reject 
The decision rule of the hypothesis test
𝐻0
𝐻0
𝐻0
𝛼
𝛼 𝐻0
𝛼 𝐻0",645,semantic
60bbec66-a59b-46a5-8dea-5b099f0ded52,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,21,"Summary
Statistical inference seeks to characterize the uncertainty in estimates
Statistics are estimates of population parameters
Inferences using statistics must consider the uncertainty in the estimates
Conﬁdence intervals quantify uncertainty in statistical estimates
Two-sided conﬁdence intervals: express conﬁdence that a value is within some range around the point estimate
One-sided conﬁdence intervals: express conﬁdence that the point estimate is greater or less than some range of values
Hypothesis tests based on the conﬁdence we have that variation in statistic arrises from sampling variation alone",612,semantic
da459734-2bb1-4838-a642-97a1d134850f,Review of Common Probability Distributions.pdf,CSCI_83,0,"Review of Common Probability Distributions
Steve Elston
09/15/2022",66,semantic
dad62bd4-bfc9-4cdb-b66e-11a0f33b2075,Review of Common Probability Distributions.pdf,CSCI_83,1,"Importance of Probability Theory
Probability theory is the basis of statistics, machine learning, and much AI
An understanding of probability theory is an important foundation to understand these methods
In this lesson we will review some basic concepts
Properties of probability distributions
Some commonly used probability distributions - focus on difﬁcult to understand properties
Many texts provide comprehensive introductions to probability theory",452,semantic
57651744-2ee1-44d2-b0e8-d7e296a591c6,Review of Common Probability Distributions.pdf,CSCI_83,2,"Probability Theory Has a Long History
First probability textbook by Jacob Bernoulli, published posthumously in 1713
First probability textbook Credit, Wikipedia commons
Probability theory is still an active area of research",223,semantic
6ee71cd2-100a-4a17-8068-9830b9311042,Review of Common Probability Distributions.pdf,CSCI_83,3,"Probability Distributions
Probability distributions are models for uncertainty of random variables
A random variable is any mapping, , from from some outcome of a random event, , to a real number, :
Example: The mapping can be a count
Example: A function which transforms  to a real number, 
This concept appears abstract at ﬁrst glance, but is fundamental to the theory of probability
We will see many examples in this course
𝑋 𝜔 ℝ
𝑋(𝜔)→ℝ
𝜔 ℝ",443,semantic
aa0bff53-edee-4b6f-a4fe-5262d98c7d3d,Review of Common Probability Distributions.pdf,CSCI_83,4,"Two Types of Probability Distributions
Discrete
Model countable events
Examples; people making a purchase, number of patients with disease,…. Characterized by a probability mass function (PMF)
Continuous
Examples; temperature, velocity, price,……. Characterized by a probability density function (PDF)",300,semantic
859afe88-0ede-4166-8ee6-5c8aad2d2005,Review of Common Probability Distributions.pdf,CSCI_83,5,"Axioms of Probability
For discrete distributions, we can speak of a set of events within the sample space of all possible events
1. Probability for any set of events, A, is greater than 0 and less than or equal to 1
2. The sum of the probability mass functions over the sample space must add to 1
3.",299,semantic
3aca3ee5-e018-454b-9e8d-cc2d2e45fcb3,Review of Common Probability Distributions.pdf,CSCI_83,5,"If sets of events A and B are mutually exclusive, then the probability of either A and B is the probability of A plus the probability of B
0≤𝑃(𝐴)≤1
𝑃(𝑆)= 𝑃()=1∑∈ 𝐴𝑎𝑖
𝑎𝑖
𝑃(𝐴 ∪ 𝐵)=𝑃(𝐴)+𝑃(𝐵)𝑖𝑓 𝐴⊥ 𝐵",194,semantic
dc6580d5-c765-4234-94c5-00d5c17fc65b,Review of Common Probability Distributions.pdf,CSCI_83,6,"Axioms of Probability
From these three axioms we can draw some useful conclusions
Events which cannot occur have probability 0
Events that must occur have probability 1
Events must have a probability mass function between 0 and 1",229,semantic
c8835b2a-b6d1-4687-8826-90c64795b5b4,Review of Common Probability Distributions.pdf,CSCI_83,7,"What do you expect: discrete distributions
What value we should expect to ﬁnd when we sample a random variable? This is the expected value or simply the expectation
For  samples, , of a random variable probability mass function  the expected value is:
How can we interpret expectation? Expectation is a probability weighted sum of the sample of the random variable 
By the second axiom of probability the weights must sum to 1.0
𝑛 𝐗= , ,…,𝑥1𝑥2 𝑥𝑛 𝑝()𝑥𝑖
E[𝐗]=  𝑝()∑𝑖=1
𝑛
𝑥𝑖 𝑥𝑖
𝐗",477,semantic
ba2355af-c5e6-4007-9c50-5dcc378ab798,Review of Common Probability Distributions.pdf,CSCI_83,8,"Properties of Expectation
Useful properties of expectation
1. The relationship is linear in probability
2. The expectation of the sum of two random variables,  and , is the sum of the expectations:
3.",200,semantic
3aaffbfc-7559-4493-b73a-b777c7426c4d,Review of Common Probability Distributions.pdf,CSCI_83,8,"The expectation of an afﬁne transformation of a random variable, , is an afﬁne transformation of the expectation:
𝑋 𝑌
E[𝐗,𝐘]=E[𝐗]+E[𝐘]
𝑋
E[𝐚 𝐗+𝐛]=𝑎 E[𝐗]+𝑏",154,semantic
b55c6150-31c2-4def-b122-7258793e3758,Review of Common Probability Distributions.pdf,CSCI_83,9,"Axioms of probability for continuous distributions
Axioms of probability for continuous probability density function, 
1. On the interval, , , must be bounded by 0 and 1:
Note: if  the integral is 0
2. The area under the entire PDF over the limits must be equal to 1:
Note: many distributions lower =  or  and upper = 
3.",321,semantic
4eb24dba-3af7-4e7c-8ef6-ebbc422f6cf1,Review of Common Probability Distributions.pdf,CSCI_83,9,"If events A and B are mutually exclusive:
𝑓(𝑥)
{, }𝑥1𝑥2 𝑃(𝑥)
0≤ 𝑓(𝑥)𝑑𝑥 ≤1∫
𝑥2
𝑥1
=𝑥1 𝑥2
𝑓(𝑥)𝑑𝑥=1∫
𝑢𝑝𝑝𝑒𝑟
𝑙𝑜𝑤𝑒𝑟
0 −∞ ∞
𝑃(𝐴 ∪ 𝐵)=𝑃(𝐴)+𝑃(𝐵) 𝑖𝑓 𝐴⊥ 𝐵",143,semantic
3f19db65-c6c5-4e2c-b562-48ae254085da,Review of Common Probability Distributions.pdf,CSCI_83,10,"What do you expect: continuous distributions
Expected value with PDF , over the interval, :
Values  are weighted by the PDF
By the second axiom of probability presented above, PDF must equal 1.0 integrated over the entire range of 
Transformation of expectation is same as for discrete random variables
𝑓(𝑥) {𝑎,𝑏}
E[𝐗]= 𝑥 𝑓(𝑥) 𝑑𝑥∫
𝑏
𝑎
𝑥
𝑥",338,semantic
1cd80833-0e81-4041-9be8-dd6ca5ac9aaa,Review of Common Probability Distributions.pdf,CSCI_83,11,"Bernoulli and Binomial Distributions
Bernoulli distributions model the results of a single trial or single realization with a binary outcome
For outcomes , or , with probability  of success:
Model the number of successful outcome in  trials with the Binomial distribution
Binomial distribution is product of multiple Bernoulli trials:
Product of Bernoulli trials is normalized by the Binomial coefﬁcient
{0,1} {𝑓𝑎𝑖𝑙𝑢𝑟𝑒,𝑠𝑢𝑐𝑐𝑒𝑠𝑠} 𝑝
𝑃(𝑥 | 𝑝)
𝑜𝑟𝑃(𝑥 | 𝑝)
={ 𝑝 𝑖𝑓 𝑥=1(1−𝑝) 𝑖𝑓 𝑥=0
= (1−𝑝  𝑥∈ 0,1𝑝𝑥 )(1−𝑥)
𝑁
𝑃(𝑘 | 𝑁,𝑝)=()(1−𝑝𝑁𝑘 𝑝𝑘 )(𝑁−𝑘)",529,semantic
d81a2113-458f-4393-a3a8-41b9c3c897ab,Review of Common Probability Distributions.pdf,CSCI_83,12,"Distributions for Multiple Outomes; the Categorical and MultinomialDistribution
Many real-world cases have many possible outcomes
In these cases need a probability distribution for multiple outcomes
Categorical distribution models multiple outcomes
Categorical Distribution is the multiple-outcome extension of the Bernoulli distribution, and is sometimes call the Multinoulli distribution.",390,semantic
4fd290f5-110e-464f-b329-d57539b718e2,Review of Common Probability Distributions.pdf,CSCI_83,13,"The Categorical distribution
Sample space of  possible outcomes, . For each trial, there can only be one outcome
For outcome  we can encode the results as:
Only  value  has a value of ; one hot encoding
For a single trial the probabilities of the  possible outcomes are expressed:
𝑘  =(, ,…, )𝑒1𝑒2 𝑒𝑘
𝑖
=(0,0,…,1,…,0)𝐞𝐢
1 𝑒𝑖 1
𝑘
Π
𝑤𝑖𝑡ℎ  ∑𝑖
𝜋𝑖
=( , ,…, )𝜋1𝜋2 𝜋𝑘
=1",363,semantic
ed557567-b483-4441-9caa-3b8aa54128c9,Review of Common Probability Distributions.pdf,CSCI_83,14,"The Categorical distribution
And consequently, we can write the simple probability mass function as:
For a series  of trials we can estimate each of the probabilities of the possible outcomes, :
Where  is the count of outcome . 𝑓(|Π)=𝑥𝑖 𝜋𝑖
𝑘 ( , ,…, )𝜋1𝜋2 𝜋𝑘
=𝜋𝑖 # 𝑒𝑖
𝑘
# 𝑒𝑖 𝑒𝑖",277,semantic
144d990f-988c-4a33-bb57-75996580304d,Review of Common Probability Distributions.pdf,CSCI_83,15,"The Categorical distribution
For the case of  you can visualize the possible outcomes of a single Categorical trial
Each discrete outcome must fall at one of the corners of a simplex
The probabilities of of each outcome is . Simplex for 
𝑘=3
( , , )𝜋1𝜋2𝜋3
𝑀𝑢𝑙𝑡3",261,semantic
5002ecd7-696f-47c7-9dbb-ca0b412438a1,Review of Common Probability Distributions.pdf,CSCI_83,16,"Normal distribution
The Normal distribution or Gaussian distribution is one of the most widely used probability distributions
The distribution of mean estimates of observations of a random variable drawn from any distribution converge to a Normal distribution by thecentral limit theorem (CLT)
Many physical processes produce Normal measurement values
Normal distribution has tractable mathematical properties",409,semantic
e84fe523-7d02-4457-b23e-3e995680440a,Review of Common Probability Distributions.pdf,CSCI_83,17,"Normal distribution
For a univariate Normal distribution we can write the density function as:
The parameters can be interpreted as:
𝑃(𝑥)= exp12𝜋𝜎2√ −(𝑥−𝜇)2
2𝜎2
𝜇𝜎𝜎2
=𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟=𝑚𝑒𝑎𝑛=𝑠𝑐𝑎𝑙𝑒=𝑠𝑡𝑎𝑛𝑑𝑎𝑟𝑑 𝑑𝑒𝑣𝑖𝑎𝑡𝑖𝑜𝑛=𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒",224,semantic
0996e41d-08d9-4b01-b0a1-9abf0563dcca,Review of Common Probability Distributions.pdf,CSCI_83,18,"Multivariate Normal
Many practical applications have an -dimensional parameter vector in , requiring multivariate distributions
Multivariate Normal distribution, parameterized by:
n-dimensional vector of locations, 
The vector(multi) valued version of univariate location
 x  dimensional covariance matrix, 
The multi-dimensional version of univariate variance 
 is the determinant of the covariance matrix. Along the diagonal the values are the  variances of each dimension, 
Off-diagonal terms describe the dependency between the  dimensions of the distribution. 𝑛 ℝ𝑛
𝜇⃗ 
𝑛 𝑛 𝚺
𝜎2
𝑓()= 𝑒𝑥𝑝( (− 𝚺(−))𝐱⃗  1(2𝜋|𝚺|)𝑛√
12𝐱⃗ 𝜇⃗ )𝑇 𝐱⃗ 𝜇⃗ 
|𝚺|
𝑛 𝜎𝑖,𝑖
𝑛",646,semantic
124a6b7f-0a1c-4763-9e37-f83bd922f031,Review of Common Probability Distributions.pdf,CSCI_83,19,"Multivariate Normal
We can write the covariance matrix:
For a Normally distributed n-dimensional multivariate random variable,  computed from the sample, :
Where  is the inner product operator and  is the mean of . 𝚺=
⎡
⎣
⎢⎢⎢⎢
𝜎1,1
𝜎2,1
⋮ 𝜎𝑛,1
𝜎1,2
𝜎2,2
⋮ 𝜎𝑛,2
……
⋮ …
𝜎1,𝑛
𝜎2,𝑛
⋮ 𝜎𝑛,𝑛
⎤
⎦
⎥⎥⎥⎥
𝜎𝑖,𝑗 𝐗
𝜎𝑖,𝑗=E[( −E[])⋅ ( −E[])]𝑥⃗ 𝑖 𝑥⃗ 𝑖 𝑥⃗ 𝑗 𝑥⃗ 𝑗
=E[( −)⋅ ( −)]𝑥⃗ 𝑖 𝑥¯𝑖 𝑥⃗ 𝑗 𝑥¯𝑗
= ( −)⋅ ( −)1𝑘𝑥⃗ 𝑖 𝑥¯𝑖 𝑥⃗ 𝑗 𝑥¯𝑗
⋅ 𝑥𝑖¯ 𝑥𝑖→",418,semantic
5d526a62-c6e2-4f7f-96b7-91258fea1afb,Review of Common Probability Distributions.pdf,CSCI_83,20,"Multivariate Normal
2-dimensional Normal with  and 
𝜇=[0,0] 𝜎=[ ]1.00.0 0.01.0",78,semantic
06fbc494-5805-46cb-8a7e-b47b25a2742c,Review of Common Probability Distributions.pdf,CSCI_83,21,"Multivariate Normal
2-dimensional Normal with  and 
𝜇=[0,0] 𝜎=[ ]1.00.0 0.00.5",78,semantic
019dfa73-c7e8-44db-9004-4fcbe333bb9e,Review of Common Probability Distributions.pdf,CSCI_83,22,"Multivariate Normal
2-dimensional Normal with  and 
𝜇=[0,0] 𝜎=[ ]1.00.5 0.51.0",78,semantic
5143fab3-44dd-47f8-b0d0-97d26b63797a,Review of Common Probability Distributions.pdf,CSCI_83,23,"Multivariate Normal
2-dimensional Normal with  and 
𝜇=[0,0] 𝜎=[ ]1.0−0.5 −0.51.0",80,semantic
8f32b2fe-c9ee-4233-998e-e871002f7a9c,Review of Common Probability Distributions.pdf,CSCI_83,24,"Multivariate Normal
2-dimensional Normal with  and 
𝜇=[0,0] 𝜎=[ ]1.00.9 0.91.0",78,semantic
770df623-ca6c-4447-920c-bb07024f55c0,Review of Common Probability Distributions.pdf,CSCI_83,25,"Log-Normal distribution
Log Normal distribution is deﬁned for continuous random variables in the range - Examples price, weight, length, and volume
The Log-Normal distribution is based on a log-transformation of the random variable:
0<𝑥≤∞
𝑃(𝑥)= exp1𝑥 1𝜎2𝜋√ −(𝑙𝑜𝑔(𝑥)−𝜇)2
2𝜎2",273,semantic
2912b34e-481b-4718-849a-20ede33897f0,Review of Common Probability Distributions.pdf,CSCI_83,26,"Student t-distribution
Student t-distribution, or simply the t-distribution, is of importance in statistics since it is the distribution of the difference of means of two Normallydistributed random variables
t-distribution has one parameter, the degrees of freedom, denoted as 
The PDF of the t-distribution is a rather complex looking result:
𝜈
𝑃(𝑥 | 𝜈)= (1+Γ( )𝜈+12Γ()𝜈𝜋√ 𝜈2
𝑥2
𝜈)
−𝜈+12
𝑤ℎ 𝑒𝑟𝑒Γ(𝑥)=𝐺𝑎𝑚𝑚𝑎 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛",414,semantic
588d1427-d29f-4f67-9f73-442590c1eda8,Review of Common Probability Distributions.pdf,CSCI_83,27,"The Gamma and  distributions
Gamma family of distributions includes several members which are important in statistics
Gamma distributions are a two-parameter exponential family
PDF is deﬁned in the range 
Gamma family are used in problems, ranging from measurements of physical systems to hypothesis testing
𝜒2
0≤𝑥≤∞",316,semantic
7ab94947-05ea-48b2-b905-43f0d80ce366,Review of Common Probability Distributions.pdf,CSCI_83,28,"The Gamma and  distributions
Gamma family can be parameterized in several ways; we will use:- A shape parameter, , the degrees of freedom- A scale parameter, 
Alternatively, use an inverse scale parameter, . 𝜒2
𝜈𝜎
𝐺𝑎𝑚(𝜈,𝜎)=  𝑥𝜈−1𝑒−𝑥/𝜎
 Γ(𝜈)𝜎𝜈
𝑤ℎ 𝑒𝑟𝑒𝑥≥0, 𝜈>0, 𝜎>0𝑎𝑛𝑑Γ(𝜈)=𝐺𝑎𝑚𝑚𝑎 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛
𝛽=1/𝜎",290,semantic
1dfcd8e2-fa3a-42c6-987c-97d6b585497b,Review of Common Probability Distributions.pdf,CSCI_83,29,"The Gamma and  distributions
Two useful special cases of the Gamma distribution are:
 is the exponential distribution with decay constant , and PDF:
 is the Chi-squared distribution with  degrees of freedom The  distribution has many uses in statistics. Used for estimates of the variance of the Normal distribution
PDF of the  distribution:
𝜒2
𝐺𝑎𝑚(1,1/𝜆) 𝜆
𝑒𝑥𝑝(𝜆)=𝜆𝑒−𝜆𝑥
𝐺𝑎𝑚(𝜈/2,2)=𝜒2𝜈 𝜈 𝜒2𝜈
𝜒2𝜈
=𝜒2𝜈  𝑥𝜈/2−1𝑒−𝑥
 Γ(𝜈/2)𝜎𝜈/2
𝑓𝑜𝑟 𝜈 𝑑𝑒𝑔𝑟𝑒𝑒𝑠 𝑜𝑓 𝑓𝑟𝑒𝑒𝑑𝑜𝑚",448,semantic
c2d6df00-3839-4cda-ac94-c7506d16130c,Review of Common Probability Distributions.pdf,CSCI_83,30,"Odds
Odds are the ratio of the number of ways an event occurs to the number of ways it does not occur
Can say that odds are the count of events in favor of an event vs. the count against the event
Example: Flip a fair coin, odds of getting heads are  (1 in 1)
Example: Roll a single fair die your odds of rolling a 6 are  (1 in 5), or 0.2
1:1
1:5",346,semantic
3f419a73-3838-4ee4-9572-0e360e5a7b45,Review of Common Probability Distributions.pdf,CSCI_83,31,"Odds
What is the relationship between odds and probability of an event? For some event with count  in a set of all outcomes with count , and count of negative outcomes :
Example: For the fair coin ﬂip, the odds are . So we can compute the probability of heads as:
Example In statistics the odds ratio, , used to predict the response variable in logistic regression
𝐴 𝑆 𝐵=𝑆−𝐴
𝑃(𝐴)= = = =  𝐴𝑆 𝐴𝐴+(𝑆−𝐴) 𝐴𝐴+𝐵 𝑐𝑜𝑢𝑛𝑡 𝑖𝑛 𝑓𝑎𝑣𝑜𝑟𝑐𝑜𝑢𝑛𝑡 𝑖𝑛 𝑓𝑎𝑣𝑜𝑟 +𝑐𝑜𝑢𝑛𝑡 𝑛𝑜𝑡 𝑖𝑛 𝑓𝑎𝑣𝑜𝑟𝑤ℎ 𝑖𝑐ℎ  𝑖𝑚𝑝𝑙𝑖𝑒𝑠𝑜𝑑𝑑𝑠=𝐴:(𝑆−𝐴)
1:1
𝑃(𝐻)= =11+1 12
𝑝1−𝑝",504,semantic
829b91ea-fb8e-4ccb-a18b-3a591a36bad8,Review of Common Probability Distributions.pdf,CSCI_83,32,"Summary
Axioms of probability; for discrete distribution
Expectation
0<𝑃(𝐴)≤1
𝑃(𝑆)= 𝑃()=1∑∈ 𝐴𝑎𝑖
𝑎𝑖
𝑃(𝐴 ∪ 𝐵)=𝑃(𝐴)+𝑃(𝐵)𝑖𝑓 𝐴⊥ 𝐵
E[𝐗]=  𝑝()∑𝑖=1
𝑛
𝑥𝑖 𝑥𝑖",147,semantic
5b009eee-693f-4695-be05-e4366def3a1d,Review of Common Probability Distributions.pdf,CSCI_83,33,"Summary
The Categorical distribution
For outcome  we one hot encode the results as:
For a single trial the probabilities of the  possible outcomes are expressed:
probability mass function as:
Multivariate Normal distribution, parameterized by n-dimensional vector of locations,  and  x  dimensional covariance matrix
𝑖
=(0,0,…,1,…,0)𝐞𝐢
𝑘
Π=( , ,…, )𝜋1𝜋2 𝜋𝑘
𝑓(|Π)=𝑥𝑖 𝜋𝑖
𝜇⃗  𝑛 𝑛
𝑓()= 𝑒𝑥𝑝( (− 𝚺(−))𝐱⃗  1(2𝜋|𝚺|)𝑘√ 12𝐱⃗ 𝜇⃗ )𝑇 𝐱⃗ 𝜇⃗",426,semantic
669b0b7c-8f9f-4f26-b2d4-7b1786a153b9,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,0,"Models Categorical Variables and Nonlinear Response
Steve Elston
10/27/2022",75,semantic
6ab7ae20-e2c2-446f-ba92-c77d6205a645,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,1,"Review
Linear models are a ﬂexible and widely used class of models
Fit model coefﬁcients by least squares estimation
Can use many types of predictor variables
We prefer the simplest model that does a reasonable job
The principle of Occam’s razor
Must consider the bias-variance trade-off",287,semantic
04a55c56-3038-462d-a351-0f9265f8d6d8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,2,"Review
When evaluating any machine learning model consider all evaluation methods available
No one method best all of the time
Homoskedastic Normally distributed residuals
Reasonable values , RMSE, etc
Are the model coefﬁcients all signiﬁcant? Different methods highlight different problems with your model
Don’t forget to check that the model must make sense for your application! 𝑅2",384,semantic
4f200aa1-56e2-4c07-ba82-25816cd867d6,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,3,"Review
Representation of machine learning models
The key representation is the model matrix
Column of 1s for intercept
Columns of feature or predictor values
There are two standards for signatures of ML functions
A model matrix  (exogenous-features) and label array  (dependent-endogenous) - Scikit-learn and base Statsmodels
A data frame with all features (predictors) and label (dependent) columns plus a model formula - Statsmodels formula and R
𝐴=
⎡
⎣
⎢⎢⎢⎢⎢⎢
1, , ,…,𝑥1,1𝑥1,2 𝑥1,𝑝
1, , ,…,𝑥2,1𝑥2,2 𝑥2,𝑝
1, , ,…,𝑥3,1𝑥3,2 𝑥3,𝑝
⋮ , ⋮ , ⋮ , ⋮ , ⋮ 1, , ,…,𝑥𝑛,1𝑥𝑛,2 𝑥𝑛,𝑝
⎤
⎦
⎥⎥⎥⎥⎥⎥
𝑋 𝑌",583,semantic
f30229f3-563e-4a0a-a209-c1bcf8cd6c6c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,4,"Review
There are a number of assumptions in linear models that you overlook at your peril! The feature or predictor variables should be independent of one another
This is rarely true in practice
Multi-collinearity between features makes the model under-determined
We assume that numeric features or predictors have zero mean and about the same scale
We do not want to bias the estimation of regression coefﬁcients with predictors that do not have a 0 mean
We do not want to have predictors with a large numeric range dominate training
Example: income is in the range of 10s or 100s of thousands and age is in the range of 10s, but apriori income is no more important than age as a predictor
Values of each predictor or feature should be iid
If variance changes with sample, the optimal value of the coefﬁcient could not be constant
If there serial correlation in the predictor values, the iid assumption is violated - but can account for this such as in time series models",972,semantic
015df549-b977-4efe-930d-bbafb1d04592,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,5,"Review
Scaling of features is required for many machine learning models
Several commonly used approaches
Z-score scaling results in features with zero mean and unit variance
Use Z-score scaling for features approximately normally distributed
Min-max scaling transforms feature values to range 0-1
Use min-max scaling for features with truncated range of values
Effect on model coefﬁcients
Scaling changes model coefﬁcients by the scale factor applied
Can re-scale (unscale) model coefﬁcients before processing unknown cases
Or use same scaling for unknown feature values
When coding categorical variables as binary dummy variables no need to scale - already in range [0-1]",672,semantic
5e649e3c-ec5b-476c-8e36-359dfd45a1a0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,6,"Introduction
Working with categorical variables
One-hot encoding
Working with contrasts
Effects and adjustments
Building models with nonlinear or non-Normal response
Use generalized linear model for nonlinear response
Link function transforms to nonlinear model to linear model
Evaluating Binomial response models
Compare model performance with deviance
Poisson regression",372,semantic
63d142da-5a33-430f-bd89-c399a47f5045,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,7,"Working with Categorical Variables
Linear models, like nearly all machine learning models, use numeric features
How can categorical variables be used in linear models? Need to transform categories to numeric variables with one hot encoding
Each category becomes a binary dummy variable, encoded [0,1]
Only one dummy variable has nonzero value - encodes the category
n categories represented by n-1 dummy variables; all 0s encodes one level
Binary variables are an exception
Represent with a single binary variable. [0,1] values",527,semantic
94531dbd-5cd5-4853-9e1a-a16aa2f6fc45,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,8,"Working with Categorical Variables
Example: Consider a data set with categorical variables
##     id  female  race  ses  schtyp  prog  read  write  math  science  socst## 0   70       0     4    1       1     1    57     52    41       47     57## 1  121       1     4    2       1     3    68     59    53       63     61## 2   86       0     4    3       1     1    44     33    54       58     31## 3  141       0     4    3       1     3    63     44    47       53     56## 4  172       0     4    2       1     2    47     52    57       53     61## 5  113       0     4    2       1     2    44     52    51       63     61## 6   50       0     3    2       1     1    50     59    42       53     61## 7   11       0     1    2       1     2    34     46    45       39     36## 8   84       0     4    2       1     1    63     57    54       58     51## 9   48       0     3    2       1     2    57     55    52       50     51",938,semantic
9dd4b3d9-2e89-476f-91be-8c8356dbf491,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,9,"Working with Categorical Variables
Example: split the data into train and test subsets
## (120, 11)
## (80, 11)
nr.seed(2 3 4 5 )
msk = nr.choice(test_scores.index, size = 1 2 0 , replace=False)test_scores_train = test_scores.iloc[msk,:]print(test_scores_train.shape)
test_scores_test = test_scores.drop(msk, axis=0 ) 
print(test_scores_test.shape)",348,semantic
de826094-ee33-4353-9c00-cdc89311923c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,10,"Working with Categorical Variables
Example: We can encode a categorical variable with the Python patsy package to get the X (model) and Y(label) arrays:
## [[1. 0. 0.]##  [1.",174,semantic
73e74f69-69be-4d3a-af24-eef17d97b95b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,10,"1. 0.]##  [1. 0. 1.]##  [1. 0. 1.]##  [1. 1. 0.]]
## [[57.]##  [61.]##  [31.]##  [56.]##  [61.]]
from patsy import dmatrices
Y, X = dmatrices(""socst ~ C(ses, levels=[1,2,3])"", data=test_scores)print(X[:5 ])
print(Y[:5 ])",220,semantic
1f3961ad-8ce0-4b31-9cb8-9c09bd20faed,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,11,"Working with Categorical Variables
Example: A simple linear model with one categorical variable
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.097## Model:                            OLS   Adj. R-squared:                  0.082## Method:                 Least Squares   F-statistic:                     6.285## Date:                Mon, 31 Oct 2022   Prob (F-statistic):            0.00255## Time:                        10:34:48   Log-Likelihood:                -446.60## No.",721,semantic
f6b81059-c062-4515-aa39-073b22b62757,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,11,"Observations:                 120   AIC:                             899.2## Df Residuals:                     117   BIC:                             907.6## Df Model:                           2                                         ## Covariance Type:            nonrobust                                         ## ===============================================================================##                   coef    std err          t      P>|t|      [0.025      0.975]## -------------------------------------------------------------------------------## Intercept      47.5926      1.949     24.415      0.000      43.732      51.453## C(ses)[T.2]     4.8635      2.366      2.055      0.042       0.177       9.550## C(ses)[T.3]     9.1296      2.579      3.540      0.001       4.023      14.237## ==============================================================================## Omnibus:                        3.392   Durbin-Watson:                   2.023## Prob(Omnibus):                  0.183   Jarque-Bera (JB):                3.413## Skew:                          -0.397   Prob(JB):                        0.182## Kurtosis:                       2.772   Cond. No. 4.53## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
import statsmodels.formula.api as smf 
linear_model = smf.ols(""socst ~ C(ses)"", data=test_scores_train).fit()linear_model.summary()",1515,semantic
16047182-0fa3-4063-8a8f-7dd9784e6ff8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,12,"Working with Categorical Variables
Wait!",40,semantic
1352e7cb-6d33-41b0-ba6b-17151b5e1a71,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,12,"What happened to the coefﬁcient for the ﬁrst level of ses? The intercept is the mean response of the ﬁrst level
The other coefﬁcients are contrasted with respect to the mean of the ﬁrst level. Consider the following possible ways we can encode responses to a categorical variable - often called a treatment
For  treatments, there are mean responses 
The alternative encoding is a treatment with intercept, , at  contrasts, 
The means and contrasts are related:
𝑛 [ , ,…, ]𝜇1𝜇2 𝜇𝑛
𝐼 𝑛−1 [𝐼, ,…, ]𝑐1 𝑐𝑛−1
=
⎡
⎣
⎢⎢⎢⎢
𝐼𝑐2
⋮ 𝑐𝑛
⎤
⎦
⎥⎥⎥⎥
⎡
⎣
⎢⎢⎢⎢
𝜇1
−𝜇2 𝜇1
⋮ −𝜇𝑛 𝜇1
⎤
⎦
⎥⎥⎥⎥",568,semantic
55714835-b47d-4e34-bc7e-8a0ea655d41c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,13,"Working with Categorical Variables
In a linear model we can sometimes relate the coefﬁcient values to an effect size
Start with  treatments  with effect sizes, 
With no intercept term the means represent the effect sizes:
With intercept term compute effect sizes using contrasts:
𝑛 [, ,…, ]𝑡1𝑡2 𝑡𝑛 [, ,…, ]𝑒1𝑒2 𝑒𝑛
=
⎡
⎣
⎢⎢⎢⎢
𝑒1
𝑒2
⋮ 𝑒𝑛
⎤
⎦
⎥⎥⎥⎥
⎡
⎣
⎢⎢⎢⎢
𝜇1
𝜇2
⋮ 𝜇𝑛
⎤
⎦
⎥⎥⎥⎥
=
⎡
⎣
⎢⎢⎢⎢
𝑒1
𝑒2
⋮ 𝑒𝑛
⎤
⎦
⎥⎥⎥⎥
⎡
⎣
⎢⎢⎢⎢
𝐼𝐼+𝑐1
⋮ 𝐼+𝑐𝑛
⎤
⎦
⎥⎥⎥⎥",435,semantic
cc233800-7eae-45ac-a6aa-8d821a873ec1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,14,"Working with Categorical Variables
In a linear model we can sometimes relate the coefﬁcient values to an effect size
Assumes the treatments are orthogonal
In other words, applied on at a time
e.g. a case can only be in one category
Assumes that the model coefﬁcients are statistically independent
Coefﬁcients are dependent in overﬁt model
Often need to adjust for other effects
Other treatments
Levels of other categorical variables
Use partial slope of continuous variables
In other words apply with care! Don’t over-interpret your model
Conditions in real world hard to verify, particularly for observational data",615,semantic
ca68a1a6-137c-4c1a-a1cf-fb868ad4b2f0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,15,"Models with Nonlinear Response
How do we deal with models that do not have nonlinear response variables? Example: binary response variable, ,  distributed
Probability parameter 
A binary classiﬁer
Example: Intensity of an arrival process,  response
 is the average rate or intensity of a point process
Estimate the parameter 
Example: Categorical response variable for  categories, :
 category classiﬁer
Response is probability probability for each category, 
[0,1]𝐵𝑖𝑛(𝜃)
𝜃
𝑝𝑜𝑖𝑠𝑠𝑜𝑛(𝜆)
𝜆
𝜆
𝑛 𝑀𝑢𝑙𝑡𝑖( , ,…, )𝜋1𝜋2 𝜋𝑛
𝑛
Π=[ , ,…, ]𝜋1𝜋2 𝜋𝑛",533,semantic
d4d33d7a-3b7c-49a3-959a-21ea71afd27c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,16,"Models with Nonlinear Response
The generalized linear model (GLM) is a framework for nonlinear response models
Nonlinear response is non-Normally distributed
For each distribution use a link function to transform to a linear model
Linear model has Normally distributed response
Link function transform nonlinear response to Normal distribution
To compute the nonlinear response
Start with a linear model, OLS
Transform response with inverse link function
Works for all exponential family response distributions",510,semantic
1b3e2342-31f2-46cf-9043-7e6a19f60467,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,17,"The Generalized Linear Model
Link functions are available for many distributions
Supported in statsmodels
Supported in Scikit-Learn
Examples:
Gaussian, identity function
Inverse Gaussian
Binomial, logit function
Multinomial
Poisson
Negative Binomial
Gamma
Tweedie",263,semantic
7eeac90b-2c41-48a3-bd96-181c90f2ef4e,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,18,"The Generalized Linear Model
General form for link function, , mapping response variable, , observation vector, , to linear model
Given linear model :
To ﬁnd the value of the response variable we apply the inverse link function:
𝑔() 𝑦 𝑥
= + 𝑥𝜆̂  𝛽0 𝛽1
𝑔(𝙴[| ])= = + 𝑥𝑦𝑖 𝑥𝑖 𝜆̂  𝛽0 𝛽1
𝙴[| ]= ()= ( + 𝑥)𝑦𝑖 𝑥𝑖 𝑔−1 𝜆̂  𝑔−1 𝛽0 𝛽1",323,semantic
98d22c6b-33b8-4a2b-8bb3-028705989680,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,19,"The Generalized Linear Model
OLS has Normal response
What is the link function
Link function for OLS is just unity, or 
Output of linear model directly maps to Normally distributed response
1
+ 𝑥∼ 𝑁( + 𝑥,𝜎)𝛽0 𝛽1 𝛽0 𝛽1",217,semantic
c8c88c11-ea94-4754-993f-2c48f44510a3,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,20,"The Logistic Regression Model
Construct a generalized linear model using a Binomial distribution
Commonly known as logistic regression
Logistic regression widely used as a classiﬁcation model
Logistic regression is linear model, with a binary response or label values, {False, True} or {0, 1}
Response computed as a log likelihood, leading to a Binomial distributed response",374,semantic
67b8dfd8-589f-4f98-b8e3-0cf220886be1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,21,"The Logistic Regression Model
Construct logistic regression as a GLM
Start with a model for the log-odds of response  vs. Probability of success, or , 
Independent variable vector 
Model parameter vector, 
Binary response, 
Deﬁne the link function, know as the or logit function:
1 0
=1𝑦𝑖 𝑝𝑖
𝑥𝑖
𝛽
=[0,1]∼ 𝐵𝑖𝑛()𝑦𝑖 𝑝𝑖
𝑙𝑜𝑔𝑖𝑡(𝙴[| ])=𝑙𝑜𝑔𝑖𝑡(())=𝑙𝑛( )= +𝛽𝑦𝑖 𝑥𝑖 𝑝𝑖 𝑥𝑖 ()𝑝𝑖 𝑥𝑖
1−()𝑝𝑖 𝑥𝑖 𝛽0 𝑥𝑖",383,semantic
6d0bfcd3-fc8d-4597-a89a-9e38f26293f0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,22,"The Logistic Regression Model
Response of linear model is transformed to the binomially distributed random variable through the inverse link function
Known as the inverse logit function, or logistic function, 
After some algebra we can arrive at:
𝑓()𝑥𝑖
𝜆𝑖
()=𝙴[| ]𝑝𝑖 𝑥𝑖 𝑦𝑖 𝑥𝑖
()=𝑓()𝑝𝑖 𝑥𝑖 𝑥𝑖
= +𝛽𝛽0 𝑥𝑖
=𝑓()=𝑙𝑜𝑔𝑖 ()𝜆𝑖 𝑡−1𝜆𝑖
=𝑙𝑜𝑔𝑖 ()= =𝑡−1𝜆𝑖 11+𝑒−𝜆𝑖
11+𝑒−( +𝛽)𝛽0 𝑥𝑖",362,semantic
07495235-7a95-4fc8-97df-beb5206ed859,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,23,"Logistic Regression Model
What does the transformation function look like? Consider a simple 1-dimensional case
The response is bound in the range 
We say the logistic transformation squashes the linear response  to binary, 
Can set a decision threshold for binary response
Default 
=𝑦𝑖 11+𝑒−𝑥𝑖
[0,1]
𝜆=𝐴𝑏⃗  [0,1]
=0.5",318,semantic
3e54a401-f83c-483f-9edc-8e030eee1f8b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,24,"Evaluation of Classiﬁers
How can we evaluate a classiﬁer’s accuracy? Determine proportions of test cases which are classiﬁed as:
True Positives (TP): Are positive and should be positive
True Negatives (TN): Are negative and should be negative
False Positives (FP): Classiﬁed as positive but are actually negative; Type I errors
False Negatives (FN): Classiﬁed as negative but are actually positive; Type II errors
Organize these metrics into a confusion matrix
Classiﬁed NegativeClassiﬁed Positive
Negative TN FP
Positive FN TP",527,semantic
31cd27dd-a808-4aa8-a842-f3cd09e4e737,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,25,"Evaluation of Classiﬁers
The other metrics are deﬁned as follows:
Accuracy = (TP + TN) / (TP + FP + TN + FN)
Selectivity or Precision = TP / (TP + FP)
Precision is the fraction of the relevant class predictions are actually correct
Sensitivity or Recall = TP / (TP + FN)
Recall is the fraction of the relevant class were we able to predict
Is a trade-off between precision and recall
Consider changing the decision threshold
High threshold  higher recall, fewer false negative
Low threshold  higher precision, fewer false positives
→
→",535,semantic
7aef0fab-1e1f-44dc-a3ea-b3b918c95c86,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,26,"Example of Logistic Regression
How well can we predict the type of school given the test scores? ##                  Generalized Linear Model Regression Results                  ## ==============================================================================## Dep. Variable:                 schtyp   No.",305,semantic
56d8c396-eab1-4804-bce9-194591542032,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,26,"Observations:                  200## Model:                            GLM   Df Residuals:                      198## Model Family:                Binomial   Df Model:                            1## Link Function:                  Logit   Scale:                          1.0000## Method:                          IRLS   Log-Likelihood:                -86.978## Date:                Mon, 31 Oct 2022   Deviance:                       173.96## Time:                        10:34:48   Pearson chi2:                     199.## No. Iterations:                     4   Pseudo R-squ. (CS):           0.009511## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          z      P>|z|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept     -3.1718      1.129     -2.809      0.005      -5.385      -0.959## math           0.0283      0.020      1.382      0.167      -0.012       0.068## ==============================================================================
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1 )
## Fit the model
formula = 'schtyp ~ math'logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()
print(logistic_reg_model.summary())",1427,semantic
3daa310f-1ade-42ad-a257-9be7026657ab,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,"Example of Logistic Regression
The data frame now looks like this with the predicted probability and the binary scores:
##      female  race  ses  schtyp  prog  ... math  science  socst  predicted  score## id                                    ... ## 70        0     4    1       0     1  ...",292,semantic
a213296e-b207-4879-869f-aedca8eb5381,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,41       47     57   0.117996      0## 121       1     4    2       0     3  ... 53       63     61   0.158163      0## 86        0     4    3       0     1  ... 54       58     31   0.161968      0## 141       0     4    3       0     3  ... 47       53     56   0.136844      0## 172       0     4    2       0     2  ... 57       53     61   0.173824      0## 113       0     4    2       0     2  ... 51       63     61   0.150772      0## 50        0     3    2       0     1  ... 42       53     61   0.120973      0## 11        0     1    2       0     2  ... 45       39     36   0.130295      0## 84        0     4    2       0     1  ... 54       58     51   0.161968      0## 48        0     3    2       0     2  ... 52       50     51   0.154432      0## 75        0     4    2       0     3  ... 51       53     61   0.150772      0## 60        0     4    2       0     2  ... 51       63     61   0.150772      0## 95        0     4    3       0     2  ... 71       61     71   0.238200      1## 104       0     4    3       0     2  ... 57       55     46   0.173824      0## 38        0     3    1       0     2  ... 50       31     56   0.147184      0## 115       0     4    1       0     1  ... 43       50     56   0.124015      0## 76        0     4    3       0     2  ... 51       50     56   0.150772      0## 195       0     4    2       1     1  ...,1376,semantic
82c51dc4-15fb-4ba1-871c-83b3f10aa479,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,60       58     56   0.186356      1## 114       0     4    3       0     2  ...,80,semantic
8e344610-8943-45b2-b504-33dcbf78ad95,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,"62       55     61   0.195090      1## 85        0     4    2       0     1  ... 57       53     46   0.173824      0## ## [20 rows x 12 columns]
## score the results 
threshold=0 . 1 8 test_scores['predicted'] = logistic_reg_model.predict()test_scores['score'] = [1  if x>threshold else 0  for x in test_scores['predicted']]
test_scores.head(2 0 )",348,semantic
82ef1238-42b7-4615-adf3-ef2b71debcf5,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,28,"Example of Logistic Regression
Now, evaluate the model - the classiﬁer is almost useless - no Kagle awards!:
##                  Confusion matrix##                  Score negative    Score positive## Actual negative       127                41## Actual postitive        22                10## ## Accuracy  0.69##  ##            Negative      Positive## Num case      168            32## Precision    0.85          0.20## Recall       0.76          0.31## F1           0.80          0.24
import sklearn.metrics as sklm  
def print_metrics(labels, scores):    metrics = sklm.precision_recall_fscore_support(labels, scores)    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')    print('                 Score negative    Score positive')    print('Actual negative    %6d' % conf[0 ,0 ] + '             %5d' % conf[0 ,1 ])    print('Actual postitive    %6d' % conf[1 ,0 ] + '             %5d' % conf[1 ,1 ])
    print('')    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))    print(' ')
    print('           Negative      Positive')    print('Num case   %6d' % metrics[3 ][0 ] + '        %6d' % metrics[3 ][1 ])    print('Precision  %6.2f' % metrics[0 ][0 ] + '        %6.2f' % metrics[0 ][1 ])    print('Recall     %6.2f' % metrics[1 ][0 ] + '        %6.2f' % metrics[1 ][1 ])
    print('F1         %6.2f' % metrics[2 ][0 ] + '        %6.2f' % metrics[2 ][1 ])    print_metrics(test_scores['schtyp'], test_scores['score'])",1478,semantic
09fd7438-605a-4dc0-bbf9-b445d402f333,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,29,"Example of Logistic Regression
How can we understand the cut-off value in terms of the CDF of the positive and negative cases? Both CDFs are at 1.0 by about probability = 0.25 - this is a very skewed situation! CDF curves nearly the same = poor model
Positive cases to the left of cut-off are Type II errors
Negative cases to the right of cut-off are Type I errors",364,semantic
8d8cffee-3c4b-44f5-9486-1109a4db20ee,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,30,"What is Deviance? The signiﬁcance of the GLM is expressed in terms of a statistic called deviance
It can be challenging to understand what deviance really means
To further complicate the problem there are several commonly used forms of deviance
OLS regression models are often evaluated based on variance ratios, such as the  metric, or error metrics like RMSE
Given a nonlinear mapping between the linear model and the response, these methods are not suitable
𝑅2",463,semantic
5f6eb21b-b8f3-406c-929e-9cb82a14d419,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,31,"What is Deviance? Focus on one the simplest and easiest to understand forms, known as null deviance
Null deviance is 2 times the square of the log odds ratio between a model and a null model
Intuitively, the null model is informed guessing
Deviance is a measure of how much the model improves accuracy beyond guessing",317,semantic
8a16202c-3003-4ec0-83ac-d00898469070,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,32,"What is Deviance? The deviance statistic is  distributed
Can apply a signiﬁcance test on a model
Model with small deviance is little better that informed guessing
Small 
Not signiﬁcant improvement
Model with large deviance has a large 
Signiﬁcant improvement in accuracy
𝜒2
𝜒2
𝜒2",279,semantic
cd758b44-812a-4e4c-bdc2-eef92f671c35,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,33,"What is Deviance? To understand binomial deviance, start with the expected value of the binomial log-likelihood:
Calculate the binomial probability:
For the null model we set , so the logistic function for a null model is:
And with expected log-likelihood:
(𝑘,𝑛|𝑝)=𝑙𝑜𝑔()+𝑘 𝑙𝑜𝑔(𝑝)+(𝑛−𝑘) 𝑙𝑜𝑔(1−𝑝)𝑙̂  𝑛𝑘
==𝑝𝜙 𝑦¯ 𝑘𝑛
=𝛽0 𝑝𝜙
()= =𝑓𝜙𝑦̂  11+𝑒−𝑝𝜙
11+𝑒−𝛽0
(𝑘,𝑛| )=𝑙𝑜𝑔()+𝑘 𝑙𝑜𝑔( )+(𝑛−𝑘) 𝑙𝑜𝑔(1− )𝑙̂ 𝑝ℎ 𝑖 𝑝𝜙 𝑛𝑘 𝑝𝜙 𝑝𝜙",402,semantic
6ce8b4ef-ada5-4e6d-8182-960663d2a0a8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,34,"What is Deviance? Gain intuitive understand of the behavior of the null model by example
Consider the case where half the values of the response, , are 1s
, so 
For each value of , model randomly selects a 1 or a 0 response with probability of 0.5
This model is random guessing with accuracy of 0.5
In other words, the null model is no better in terms of predictive power than just saying that all values of  are either 0 or all values are 1. 𝑦
𝑘=𝑛/2 =𝑛/2=0.5𝑝𝜙
𝑥𝑖
𝑦",466,semantic
ddd29bfe-f8e2-41ae-a5a0-6af7406c8f04,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,35,"What is Deviance? Form of null deviance of a linear model
Deviance is log of the square expected likelihood ratio:
Expected null log-likelihood, , is ﬁxed by the observed response values
-Therefore, the better the model, and higher the likelihood, - Higher deviance and therefore the value of - Model with large  has greater signiﬁcance and accuracy of predictions
𝑙𝑜𝑔𝑖 (𝑝())=𝑡𝜙 𝑥𝑖 𝛽0
𝐷( +𝛽 )𝛽0 𝑥𝑖 =𝑙𝑛( )𝐿̂ 2
𝐿̂ 2𝜙
=2(−)l̂ l̂ 𝜙
l̂ 𝜙
l̂ 𝜒2
𝜒2",441,semantic
ff819e9f-1a9b-4dc0-8c0d-938b0f0351c1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,36,"What is Deviance? What are some key properties of null deviance? Recall that log-likelihood is a negative number
Higher log-likelihood has smaller negative magnitude
Log-likelihood of null model has large negative magnitude
Deviance  always
If model is no better than null model, 
For model with greater predictive power, 
≥0
l=, 𝐷=0l𝜙
l>, 𝐷>0l𝜙",345,semantic
0ccf3d2c-cbcc-482c-bc67-f483d7862348,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,37,"What is Deviance? Deviance concepts discussed here can be applied to any GLM
In some cases, deviance can be used directly
In other cases, log-likelihood ratio is used directly for model evaluation and signiﬁcance",212,semantic
2a4d23c4-47c8-40f7-99c3-d516a0b36b9a,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,38,"Solving Maximum Likelihood Problem
We have investigated methods for ﬁnding maximum likelihood solutions at large scale
ML algorithms are employed routinely to logistic regression problems on a massive scale
Variations of the stochastic gradient descent (SGD) algorithms
Quasi-Newton’s methods like the limited memory Broyden–Fletcher–Goldfarb–Shanno (l-BFGS) algorithm",368,semantic
badf654d-9841-4681-a337-971fcf0bdb0f,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,39,"Poisson Regression as GLM
Poisson regression is example of a GLM
Poisson regression is example of nonlinear response model
Recall, the Poisson distribution has an exponential form with a single parameter, 
Parameter, , is the expected arrival rate of the process
Predictions, , given the observations,  and the model parameter, :
Above is the Poisson link function and inverse link function
𝜆
𝜆
𝑦𝑖 𝑥𝑖 𝜆𝑖
𝑙𝑜𝑔[𝙴(| )]=𝑥 ⟺ 𝙴(| )=𝑦𝑖 𝑥𝑖 𝜆𝑖 𝑦𝑖 𝑥𝑖 𝑒𝑥𝜆𝑖",444,semantic
acf066c8-335f-4af0-a4e2-80dae637feb0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,40,"Poisson Regression as GLM
Extend relationship using a linear model for 
Expected arrival rate changes with the independent variable
Example, linear model with intercept
, and  dimensional slope parameter vector, 
Estimate of , for a  dimensional observation vector 
𝜆
𝛽0 𝑝 𝛽⃗ 
𝜆𝑖 𝑝 𝐱𝑖
𝑙𝑜𝑔()= + ⟺ =𝜆𝑖 𝛽0 𝐱𝑖𝛽⃗  𝜆𝑖 𝑒( +$)𝛽0 𝐱𝑖 𝛽⃗",326,semantic
30e54a62-21c9-4c98-8e68-7946c89ca153,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,41,"Poisson Regression as GLM
Use formulation of the Poisson distribution
Link function
Link function  Inverse link function:
𝑙𝑜𝑔[𝙴(| )]=𝑦𝑖 𝑥𝑖 𝑥𝑖𝜆𝑖
⟺ 
𝑙𝑜𝑔[𝙴(| )]=𝑦𝑖 𝑥𝑖 𝑥𝑖𝜆𝑖
𝑙𝑜𝑔[𝙴(| )]= +𝑦𝑖 𝑥𝑖 𝛽0 𝐱𝑖𝛽⃗ 
⟺ 𝙴(| )=𝑦𝑖 𝑥𝑖 𝑒𝑥𝑖𝜆𝑖
⟺ 𝙴(| )=𝑦𝑖 𝑥𝑖 𝑒+𝛽0 𝐱𝑖𝛽⃗",240,semantic
217b89e7-2337-4832-8e80-2837584ec677,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,42,"Poisson Regression Example
Data for number of awards for students by program and math score
Count data - suitable for Poisson model
Note exponential decrease in counts with math score and program",195,semantic
86988414-4f47-4799-87b2-0665b2792317,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,43,"Poisson Regression Example
Fit a model and examine result
## Optimization terminated successfully.##          Current function value: 0.913761##          Iterations 6
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                           Poisson Regression Results                          ## ==============================================================================## Dep. Variable:             num_awards   No. Observations:                  200## Model:                        Poisson   Df Residuals:                      196## Method:                           MLE   Df Model:                            3## Date:                Mon, 31 Oct 2022   Pseudo R-squ.:                  0.2118## Time:                        10:34:55   Log-Likelihood:                -182.75## converged:                       True   LL-Null:                       -231.86## Covariance Type:            nonrobust   LLR p-value:                 3.747e-21## =========================================================================================##                             coef    std err          z      P>|z|      [0.025      0.975]## -----------------------------------------------------------------------------------------## Intercept                -4.1633      0.663     -6.281      0.000      -5.462      -2.864## C(prog)[T.General]       -1.0839      0.358     -3.025      0.002      -1.786      -0.382## C(prog)[T.Vocational]    -0.7140      0.320     -2.231      0.026      -1.341      -0.087## math                      0.0702      0.011      6.619      0.000       0.049       0.091## =========================================================================================## """"""",1690,semantic
d5288d69-1005-4b7d-adb8-f9fd0d875c0a,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,44,"Poisson Regression Example
Maximum likelihood regression lines for counts with Poisson model",92,semantic
ce85e315-dde3-46c3-a497-d3303c17a92b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,45,"Summary
Models with nonlinear response have non-Normal distributions
The generalized linear model accommodates nonlinear response distributions
Link function transforms to linear model
Inverse link function transforms from Normal distribution to response distribution
Evaluating Binomial response models
Confusion matrix organizes
Compute metrics from elements of confusion matrix
Use multiple evaluation criteria
Compare model performance with deviance",453,semantic
433aee37-248e-482f-ad49-d4479e5b60ad,Introduction to Linear Models.pdf,CSCI_83,0,"Introduction to Linear Models
Steve Elston
10/20/2022",53,semantic
69e13554-3560-4cd0-af63-d79c41483fd0,Introduction to Linear Models.pdf,CSCI_83,1,"Welcome to the Second Half of CSCI E-83! Plan going forward:
Week 8: Introduction to Linear Models
Week 9: Linear Models Part 2 - Categorical data and nonlinear response models
Week 10: Linear Models Part 3 - Regularization and sparse models
Week 11: Time Series Models
Week 12: Bayes MCMC methods
Week 13: Hierarchical models
Week 14: TBD - More on time series? Dec 17: Submit Graduate Independent Projects
Let me know if you have suggestions to change this schedule",467,semantic
0058eca9-1d4f-4315-b86c-7427887b2277,Introduction to Linear Models.pdf,CSCI_83,2,"Review
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrast with frequentist statistics; computing a point estimate and conﬁdence interval from a sample
Bayesian models allows expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ﬁnding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution a",803,semantic
105e4253-d65d-4bfe-99d4-22cb6e6306d3,Introduction to Linear Models.pdf,CSCI_83,3,"Introduction - Why linear models? Linear models are widely used in statistics and machine learning
Understandable and interpretable
Generalize well, if properly ﬁt
Highly scalable – computationally efﬁcient
Can approximate fairly complex functions
A basis of understanding complex models
Many non-linear models are at locally linear at convergence
We can learn a lot about the convergence of DL and RL models from linear approximations
In this lesson we take a frequentist view of the linear model
Bayesian view is also widely used",531,semantic
45227cfa-2af9-4cb1-bfaa-297a6ee1fd22,Introduction to Linear Models.pdf,CSCI_83,4,Introduction - Why linear models?,33,semantic
6b61ecad-ab9c-4dd3-b334-7523870db51a,Introduction to Linear Models.pdf,CSCI_83,4,"Linear models are readily interpretable! Human interpretability is of critical importance for models used for critical decisions
Health care
Safe operation of autonomous systems
Social justice for applications with human impact
etc. Model coefﬁcients provide information on response sensitivities to changes in variables
Low chance of unexpected output
Complex and nonlinear model result in poor human intuition about expected response
Complex and nonlinear models vulnerable to unexpected output",496,semantic
473c87aa-f1f6-4b66-b8f0-e8efe3c2b4e3,Introduction to Linear Models.pdf,CSCI_83,5,"What is regression? In statistics, regression refers to a family of model that attempt to predict the value of numeric random variable
Regression is a common form of a linear model
Linear regression is a building block of many statistical and ML methods:
multivariate regression and principal component
Analysis of variance (ANOVA)
Polynomial regression
Logistic regression for binary classiﬁcation
Poisson regression
Many time series models
Neural networks (and deep learning)",477,semantic
e9d8361c-d48a-4468-b8e4-17f905449c22,Introduction to Linear Models.pdf,CSCI_83,6,"Terminology
The confusing division in terminology arises from different communities within statistics and machine learning
Machine Learning TerminologyStatistical Terminology
Regression vs classiﬁcation Continuous numeric vs categorical response
Learning algorithm or model Model
Training Fitting
Trained model Fitted model
Supervised learning Predictive modeling",363,semantic
7b52b1fb-d230-40ec-aea7-67e7cd1b5440,Introduction to Linear Models.pdf,CSCI_83,7,"Terminology
Different communities have created different terminology at different times for the variables in a machine learning models
Predicted VariableVariables Used to Predict
y x
Dependent Independent
Endogenous Exogenous
Response Predictor
Response Explanatory
Label Feature
Regressand Regressors
Outcome Design
Left Hand Side Right Hand Side",347,semantic
5e623e5c-2390-4895-a175-e695979f3f4f,Introduction to Linear Models.pdf,CSCI_83,8,"Formulating the Linear Model
The general formulation of a linear model can be written:
 is the vector of  dependent variables or labels we seek to predict
 is the  x  model matrix or design matrix
Deﬁnes the structure of the model
 columns are values of the predictor variables or features
 rows of training cases
 is the vector of  model coefﬁcients
One coefﬁcient for each predictor or feature
Model is ﬁt by ﬁnding an optimal value for each coefﬁcient
 is the vector representing prediction error
The  residuals
Is iid Normally distributed; 
=𝐴+𝑦⃗  𝑏⃗ 𝜖⃗ 
𝑦⃗  𝑛
𝐴 𝑛 𝑝
𝑝
𝑛
𝑏⃗  𝑝
𝜖
𝑛
𝜖∼  (0, )𝜎2",596,semantic
a441f411-4859-4147-948f-a3b5f79bde58,Introduction to Linear Models.pdf,CSCI_83,9,"Single Predictor Regression
Consider a simple case of regression with a single predictor
Only two coefﬁcients ( ) deﬁning a straight line
 the intercept term
Intercept is value of  at 
 model coefﬁcient for the predictor variable
The slope coefﬁcient
Given a variable predictor value , the prediction, , is:
 is the 0 mean Normally distributed residual; 
𝑝=2
=[]𝑏⃗  𝛽0
𝛽1
=𝛽0
𝑦 𝑥=0
=𝛽1
𝑥𝑖 𝑦̂ 𝑖
𝑦̂ 𝑖
𝑦̂ 𝑖
𝜖𝑖
= +𝛽0 𝛽1𝑥𝑖
=𝑦−𝜖𝑖
∼  (0, )𝜎2
𝜖𝑖 𝙴()=0𝜖𝑖",445,semantic
b11ce611-247c-433b-940b-684ea6d1264c,Introduction to Linear Models.pdf,CSCI_83,10,"Example
Let’s start with a simulated data set with one predictor and one response variable 0
The response variable,  is linear in  with additive random noise 
Intercept  and slope 
The ﬁrst 10 rows:
##           x         y## 0  0.000000  1.951736## 1  0.204082  0.627047## 2  0.408163  3.025441## 3  0.612245  1.112869## 4  0.816327  5.225976## 5  1.020408 -0.382646## 6  1.224490  3.969339## 7  1.428571  2.834755## 8  1.632653  1.516411## 9  1.836735  2.958745
𝑦 𝑥 ∼  (0,2)
=0 =1.0",484,semantic
2163b8fe-dfea-411d-82b0-86d9a5c490e3,Introduction to Linear Models.pdf,CSCI_83,11,"Example
Notice the linear trend for these data
How do we compute a best ﬁt model for this relationship",102,semantic
909fd854-6e17-41c1-a148-9f4653582c00,Introduction to Linear Models.pdf,CSCI_83,12,"The Model Matrix
How do we create the model matrix
Start with a data table of  samples with  columns
First column is predictor variable
Second column is the response variable
𝑛 𝑝=2
⎡
⎣
⎢⎢⎢⎢⎢⎢
,𝑥1𝑦1
,𝑥2𝑦2
,𝑥3𝑦3
⋮ ,⋮ ,𝑥𝑛𝑦𝑛
⎤
⎦
⎥⎥⎥⎥⎥⎥",231,semantic
a20ab610-b90f-4384-be0f-756adca6d203,Introduction to Linear Models.pdf,CSCI_83,13,"The Model Matrix
The model matrix for this case, including the intercept term is:
The column of 1’s deﬁne the intercept term
⎡
⎣
⎢⎢⎢⎢⎢⎢
1,𝑥1
1,𝑥2
1,𝑥3
⋮ ,⋮ 1,𝑥𝑛
⎤
⎦
⎥⎥⎥⎥⎥⎥",171,semantic
a1f405ee-3e31-40e1-a029-0d03eeeb7096,Introduction to Linear Models.pdf,CSCI_83,14,"Constructing the Model
For the  data samples and the parameter vector  we can construct the entire model:
For a single prediction:
Or, in matrix notation:
We are assuming that the error, , is all attributable to the dependent variable, 
𝑛 𝑏⃗ 
= []+
⎡
⎣
⎢⎢⎢⎢⎢⎢
𝑦1
𝑦2
𝑦3
⋮ 𝑦𝑛
⎤
⎦
⎥⎥⎥⎥⎥⎥
⎡
⎣
⎢⎢⎢⎢⎢⎢
1,𝑥1
1,𝑥2
1,𝑥3
⋮ ,⋮ 1,𝑥𝑛
⎤
⎦
⎥⎥⎥⎥⎥⎥
𝛽0
𝛽1
⎡
⎣
⎢⎢⎢⎢⎢⎢
𝜖1
𝜖2
𝜖3
⋮ 𝜖𝑛
⎤
⎦
⎥⎥⎥⎥⎥⎥
= + +𝑦𝑖 𝛽0 𝛽1𝑥𝑖 𝜖𝑖
=𝐴+𝑦̂  𝑏⃗ 𝜖⃗ 
𝜖 𝑦",410,semantic
8dde7dc8-4386-4d44-b2b2-4ab7ce07f8f9,Introduction to Linear Models.pdf,CSCI_83,15,"Estimating the Model Parameters
How do we ﬁnd the best value for the coefﬁcients
Need to minimize an error metric
Find  by minimizing the sum of squared errors is known as the least squares method
Given training data, minimize the squared error between the prediction, , and the observed response variable or label, :
 is the ith row of 
𝑏⃗ 
𝑦̂ 𝑖 𝑦𝑖
( − = ( − =min𝑏⃗  ∑𝑖
𝑦𝑖 𝐴𝑖,.𝑏̂ )2 min𝑏⃗  ∑𝑖
𝑦𝑖 𝑦𝑖^)2 min𝑏⃗  ∑𝑖
𝜖2𝑖
𝐴𝑖,. 𝐴",423,semantic
c29a4f92-53dd-41e3-a07e-d889eb988e50,Introduction to Linear Models.pdf,CSCI_83,16,"Estimating the Model Parameters
We could try a naive solution:
Where  is the matrix inverse of 
This might work, BUT
Direct matrix inverse algorithm has complexity , so inefﬁcient
There is no guarantee the inverse exists
The  features can be colinear
= 𝑦𝑏⃗  𝐴−1
𝐴−1 𝐴
𝑂()𝑛3
𝑝",275,semantic
9341d8f4-7c14-41eb-beb9-97f0393ca69c,Introduction to Linear Models.pdf,CSCI_83,17,"Estimating the Model Parameters
We can use the Normal equations
Start with the problem:
Multiply by  and set 
Taking the inverse of  we arrive at the normal equations
 is the covariance matrix for the data set
Is dimension only  x 
For single predictor model this is just dimension 2x2
Much easier to take inverse
But poor scaling for large-scale problems, 
𝑦−𝐴=→0𝑏⃗  𝜖⃗ 
𝐴𝑇 =0𝜖⃗ 
𝐴=𝐴𝑇 𝑏⃗  𝐴𝑇𝑦̂ 
𝐴𝐴𝑇
=( 𝐴𝑏⃗  𝐴𝑇 )−1𝐴𝑇𝑦̂ 
𝐴𝐴𝑇
𝑝𝑝
𝑝>1,000",434,semantic
3d6ce7a7-b3ac-4fce-9eed-a01ed74be9dc,Introduction to Linear Models.pdf,CSCI_83,18,"Estimating the Model Parameters
What is the relationship between the normal equations, least squares and maximum likelihood? First consider how the Normal likelihood can be written in terms of  with  feature vectors of observations  and  labels :
The log-likelihood is then:
For a ﬁxed  one can see that the log-likelihood is maximized by minimizing the squared error:
𝑏⃗  𝑛 𝑥⃗ 𝑖 𝑛 𝑦𝑖
 (, )= 𝑒𝑥𝑝{− ( − }𝑏⃗ 𝜎2 ∏𝑖=1
𝑛 1(2𝜋𝜎2)1/2
12𝜎2𝑦𝑖 𝑥𝑇𝑖𝑏⃗ )2
l(, )=−{𝑛 𝑙𝑜𝑔( )+ ( − }𝑏⃗ 𝜎2 12 𝜎2 1𝜎2∑𝑖=1
𝑛
𝑦𝑖 𝑥𝑇𝑖𝑏⃗ )2
𝜎2
𝑆𝑆()= ( −𝑏⃗  ∑𝑖=1
𝑛
𝑦𝑖 𝑥𝑇𝑖𝑏⃗ )2",534,semantic
4daf8959-0ba5-4fd6-818e-7e57a14e5816,Introduction to Linear Models.pdf,CSCI_83,19,"Estimating the Model Parameters
Minimize sum of square errors to maximize log-likelihood
Set the ﬁrst derivative of sum of square errors to zero
Or, in matrix form:
Solving the above leads to the normal equations:
But still need to compute inverse of covariance matrix, 
=2 ( − )=0∂ 𝑆𝑆()𝑏⃗ 
∂ 𝑏⃗  ∑𝑖=1
𝑛
𝑥𝑇𝑖 𝑦𝑖 𝑥𝑇𝑖𝑏⃗ 
(−𝐴)=0𝐴𝑇𝑦⃗  𝑏⃗ 
=( 𝐴𝑏⃗  𝐴𝑇 )−1𝐴𝑇𝑦̂ 
𝐴𝐴𝑇",357,semantic
72f2d6a6-0859-4e98-b176-3201c1e0338a,Introduction to Linear Models.pdf,CSCI_83,20,"Estimating the Model Parameters
Are there more scalable ways to solve the least squares problems? Inverting the covariance matrix is a big improvement over a naive approach, it still requires taking a large matrix inverse at scale. Can we solve the least squares problem in a more computationally efﬁcient way?",310,semantic
5b1835bd-6e33-45bc-a75d-66bf9b020b47,Introduction to Linear Models.pdf,CSCI_83,20,"Start with the linear equations for maximum likelihood
Eliminate  from both sides
We can ﬁnd the minimum of this linear system with an efﬁcient solver
Stochastic Gradient Descent (SGD) and its relatives
Quasi-Newton methods like L-BFGS
𝑝>1,000
(−𝐴)=0𝐴𝑇𝑦⃗  𝑏⃗ 
𝐴𝑇
−𝐴=0𝑦⃗  𝑏⃗",273,semantic
b5100c4e-8876-4733-b009-241dcf13b865,Introduction to Linear Models.pdf,CSCI_83,21,"Estimating the Model Parameters
What is the relationship between the normal equations, least squares and maximum likelihood? Solving the least problem is equivalent to solving the maximum likelihood estimation problem
The normal equations are a maximum likelihood estimator
Solving the system of linear equations, , results in a least squares maximum likelihood solution
The above applies when residuals are Normally distributed
−𝐴=0𝑦⃗  𝑏⃗",439,semantic
1105780b-8d3b-4bae-a75b-4337dca1c535,Introduction to Linear Models.pdf,CSCI_83,22,"Example - Specifying the Model
How do we specify the model formula with statsmodels? Use the S/R style model formula developed by Chambers and Hastie; Statistical Models in S (1992). Uses the  operator to mean *modeled by**
Example; dependent variable (dv) modeled by two independent variables (var1 and var2):
Example; dependent variable (dv) modeled by independent variables (var1) and its square, uses the  operator to wrap a function:
Example; dependent variable (dv) is modeled by two independent variables (var1 and var2) and the interaction term with no intercept term:
Example; dependent variable (dv) modeled by independent numeric variable (var1) and a categorical variable (var2):
∼ 
𝑑𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡 𝑣𝑎𝑟𝑖𝑎𝑏𝑙𝑒∼ 𝑖𝑛𝑑𝑒𝑝𝑒𝑛𝑒𝑛𝑡 𝑣𝑎𝑟𝑖𝑎𝑏𝑙𝑒𝑠
𝑑𝑣∼ 𝑣𝑎𝑟1+𝑣𝑎𝑟2
𝐼()
𝑑𝑣∼ 𝑣𝑎𝑟1+𝐼(𝑣𝑎𝑟1∗ ∗ 2)
𝑑𝑣∼ −1+𝑣𝑎𝑟1∗ 𝑣𝑎𝑟2⟷ 𝑑𝑣∼ −1+𝑣𝑎𝑟1+𝑣𝑎𝑟2+𝑣𝑎𝑟1:𝑣𝑎𝑟2
𝑑𝑣∼ 𝑣𝑎𝑟1+𝐶(𝑣𝑎𝑟2)",838,semantic
63946922-9a42-4d45-9f0c-3324a535f3c0,Introduction to Linear Models.pdf,CSCI_83,23,"Example - Fitting the Model
Fit the model using statsmodels.formula.api.ols to create a linear model object
## Intercept = 1.611  Slope = 0.882
Find the predicted values for each value of :
##           x         y  predicted## 0  0.000000  1.951736   1.611189## 1  0.204082  0.627047   1.791233## 2  0.408163  3.025441   1.971277## 3  0.612245  1.112869   2.151321## 4  0.816327  5.225976   2.331365
## Define the regression model and fit it to the data
ols_model = smf.ols(formula = 'y ~ x', data=sim_data).fit()
## Print the model coefficient
print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0 ], ols_model._results.params[1 ]))
𝑥
# Add predicted to pandas dataframe
sim_data['predicted'] = ols_model.predict(sim_data.x)# View head of data framesim_data.head(5 )",786,semantic
1ecfef3f-32a0-40f5-8349-fd9e7ae5dbff,Introduction to Linear Models.pdf,CSCI_83,24,"Example - Model Checking
Plot the regression line against the original data
This looks like a good ﬁt, but how good is it really?",129,semantic
4b9df581-2bff-4375-bd13-cd459e366c6d,Introduction to Linear Models.pdf,CSCI_83,25,"Evaluating Regression Models
Evaluation of regression models focuses on the residuals or errors
Residuals should be Normally distributed with  mean and constant variance
The residuals must be homoskedastic with respect to the ﬁtted values
Homoskedastic residuals have constant variance with predicted values
Any trend or structure in the residuals indicates a poor model ﬁt
In these cases variance is not constant and we say these are heteroskedastic residuals
Heteroskedastic residuals indicate that model has not incorporated all available information
=−𝐴 =−𝜖⃗  𝑦⃗  𝑏⃗  𝑦⃗ 𝑦̂ 
0
∼  (0, )𝜖⃗  𝜎2",595,semantic
167b58a6-6e3c-43c9-b5c3-2a0d9e892898,Introduction to Linear Models.pdf,CSCI_83,26,"Evaluating Regression Models
Residual plots is a key diagnostic for any regression model
Plot residual against the predicted values
These residuals look homoskedastic - we are happy!",182,semantic
c071d288-9ac4-45a8-8920-7eee201d17e3,Introduction to Linear Models.pdf,CSCI_83,27,"Evaluating Regression Models
Graphically test that the residuals are iid Normal
These plots look promising",106,semantic
778a4a72-0bde-4fdb-aad1-2cdfa6ba5e39,Introduction to Linear Models.pdf,CSCI_83,28,"Evaluating Regression Models
We can quantitatively understand model performance by deﬁning these relationships
The relationship between these metrics:
Or, we can say that the sum of squares explained by the model is:
𝑆𝑆𝑇𝑆𝑆𝐸𝑆𝑆𝑅
=𝑠𝑢𝑚 𝑠𝑞𝑢𝑎𝑟𝑒 𝑡𝑜𝑡𝑎𝑙 = ( −Σ𝑖𝑦𝑖 𝑌¯)2
=𝑠𝑢𝑚 𝑠𝑞𝑢𝑎𝑟𝑒 𝑒𝑥𝑝𝑙𝑎𝑖𝑛𝑒𝑑 = ( −Σ𝑖𝑦𝑖^ 𝑌¯)2
=𝑠𝑢𝑚 𝑠𝑞𝑢𝑎𝑟𝑒 𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙 = ( −Σ𝑖𝑦𝑖 𝑦𝑖^)2
𝑆𝑆𝑇=𝑆𝑆𝑅+𝑆𝑆𝐸
𝑆𝑆𝐸=𝑆𝑆𝑇−𝑆𝑆𝑅",358,semantic
2470021b-c1c2-4d03-8abd-fee0d9004882,Introduction to Linear Models.pdf,CSCI_83,29,"Evaluating Regression Models
Compute the sums of squares for the running example
## SST = 494.61
## SSE = 337.53
## SSR = 157.08
## SSE + SSR = 494.61
The model has explained most of the TSS
y_bar = np.mean(sim_data.y)
SST = np.sum(np.square(np.subtract(sim_data.y, y_bar)))SSR = np.sum(np.square(sim_data.resids))SSE = np.sum(np.square(np.subtract(sim_data.predicted, y_bar)))
print('SST = {0:6.2f}'.format(SST))
print('SSE = {0:6.2f}'.format(SSE))
print('SSR = {0:6.2f}'.format(SSR))
print('SSE + SSR = {0:6.2f}'.format(SSE + SSR))",533,semantic
eb2296b4-f081-4d4d-b696-266c423f89fe,Introduction to Linear Models.pdf,CSCI_83,30,"Evaluating Regression Models
We can compare the sum of square residual to the sum of square total to evaluate how well our model explains the data
We call the ratio   or the coefﬁcient of determination
The  for a perfect model would behave as follows:
A model which does not explain the data at all has:
𝑆𝑆𝑇−𝑆𝑆𝑅=𝑆𝑆𝐸
𝑆𝑆𝐸𝑆𝑆𝑇𝑅2
=1−𝑅2 𝑆𝑆𝑅𝑆𝑆𝑇
𝑅2
𝑆𝑆𝑅𝑅2→0→1
𝑆𝑆𝑅𝑅2→𝑆𝑆𝑇→0",362,semantic
ff964408-4777-492a-bffc-42f09f3c2303,Introduction to Linear Models.pdf,CSCI_83,31,"Evaluating Regression Models
As the number of model parameters increases the model will ﬁt the data better
But, the model will become over-ﬁt as the number of parameters increases
Must adjust model performance for degrees of freedom - adjusted 
This gives  as:
Or, we can rewrite  as:
𝑅2
𝑅2𝑎𝑑𝑗
𝑤ℎ 𝑒𝑟𝑒𝑑𝑓𝑆𝑆𝑅
𝑑𝑓𝑆𝑆𝑇
=1− =1−
𝑆𝑆𝑅𝑑𝑓𝑆𝑆𝑅
𝑆𝑆𝑇𝑑𝑓𝑆𝑆𝑇
𝑣𝑎𝑟𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙
𝑣𝑎𝑟𝑡𝑜𝑡𝑎𝑙
=𝑆𝑆𝑅 𝑑𝑒𝑔𝑟𝑒𝑒𝑠 𝑜𝑓 𝑓𝑟𝑒𝑒𝑑𝑜𝑚=𝑆𝑆𝑇 𝑑𝑒𝑔𝑟𝑒𝑒𝑠 𝑜𝑓 𝑓𝑟𝑒𝑒𝑑𝑜𝑚
𝑅2𝑎𝑑𝑗
𝑅2𝑎𝑑𝑗
𝑤ℎ 𝑒𝑟𝑒𝑛𝑘
=1−(1− )𝑅2𝑛−1𝑛−𝑘
=𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑑𝑎𝑡𝑎 𝑠𝑎𝑚𝑝𝑙𝑒𝑠=𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑚𝑜𝑑𝑒𝑙 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡𝑠
𝑅2𝑎𝑑𝑗
=1.0−𝑅2𝑎𝑑𝑗 𝑆𝑆𝑅𝑆𝑆𝑇 𝑛−1𝑛−1−𝑘",529,semantic
82417869-4d96-4687-88db-0f1cddcbeb0f,Introduction to Linear Models.pdf,CSCI_83,32,"Evaluating Regression Models
The summary table for the OLS model provides a number of summary statistics
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.682## Model:                            OLS   Adj. R-squared:                  0.676## Method:                 Least Squares   F-statistic:                     103.1## Date:                Thu, 27 Oct 2022   Prob (F-statistic):           1.52e-13## Time:                        09:27:07   Log-Likelihood:                -99.565## No.",730,semantic
653dc1c2-e242-4fc0-aa8e-d89ecba3a230,Introduction to Linear Models.pdf,CSCI_83,32,"Observations:                  50   AIC:                             203.1## Df Residuals:                      48   BIC:                             207.0## Df Model:                           1                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      1.6112      0.504      3.196      0.002       0.598       2.625## x              0.8822      0.087     10.156      0.000       0.708       1.057## ==============================================================================## Omnibus:                        0.850   Durbin-Watson:                   2.312## Prob(Omnibus):                  0.654   Jarque-Bera (JB):                0.450## Skew:                          -0.228   Prob(JB):                        0.799## Kurtosis:                       3.091   Cond. No. 11.7## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
ols_model.summary()",1316,semantic
4055c3ae-04c7-4944-a494-aeee81748e4c,Introduction to Linear Models.pdf,CSCI_83,33,"Evaluating Regression Models
We also can evaluate models by error metrics
The root mean square error (RMSE) is a measure of the mean of the squared residuals
## RMSE =   0.25
The median absolute error (MAE) is a robust measure of mean residuals
## MAE=    1.3
And many more possibilities…
𝑅𝑀𝑆𝐸= =( −Σ𝑛𝑖−1𝑦𝑖 𝑦𝑖^)2
𝑛√ 𝑆𝑆𝑅𝑛√
print('RMSE = {0:6.2}'.format(sqrt(np.sum(np.square(sim_data.resids)))/ float(sim_data.shape[0 ])))
𝑀𝐴𝐸=𝑚𝑒𝑑[| −|]𝑦𝑖 𝑦𝑖^
print('MAE= {0:6.2}'.format(np.median(np.absolute(sim_data.resids))))",511,semantic
40379007-ed85-4927-a3a7-b0e2969dedc8,Introduction to Linear Models.pdf,CSCI_83,34,"Evaluating Regression Models
When evaluating any machine learning model consider all evaluation methods available
No one method is most important all of the time
Different methods highlight different problems with your model
Don’t forget to check that the model must make sense for your application!",299,semantic
3adb5822-c119-4b3e-bc90-3a7dd68e1b88,Introduction to Linear Models.pdf,CSCI_83,35,"Example - Fitting center Model
Fit the model using statsmodels.formula.api.ols with centered independent variable to create a linear model object
## Intercept = 6.022  Slope = 0.882
We can now interpret this model
Intercept is the mean of the dependent variable
Slope is the rate of change of the dependent variable for unit change in independent variable
Interceopt is value of independent variable where independent varaibles all 
May not even be in deﬁned range of independent varaible
e.g. How can we interpret a negaive life expectancy
## Center the independent variable   
sim_data.loc[:,'x_centered'] = np.subtract(sim_data.x, np.mean(sim_data.x))## Define the regression model and fit it to the dataols_model_centered = smf.ols(formula = 'y ~ x_centered', data=sim_data).fit()
## Print the model coefficientprint('Intercept = %4.3f  Slope = %4.3f' % (ols_model_centered._results.params[0 ], ols_model_centered._results.params[1 ]))
=0",942,semantic
095ef3c9-4e68-434d-93a6-d7201c175504,Introduction to Linear Models.pdf,CSCI_83,36,"Example - Fitting center Model
Plot the regression line against the centered independent variable",97,semantic
6fec5f6e-8927-4149-9d12-a6c3b6868b2a,Introduction to Linear Models.pdf,CSCI_83,37,"Extending the Linear Model
We extend the linear model by adding new features or predictor variables
Higher order terms - e.g. polynomial regression
Other exogenous variables
We prefer the simplest model that does a reasonable job
The principle of Occam’s razor
Must consider the bias-variance trade-off
High complexity model ﬁts the training data well
Low bias
But might not generalize well to new cases - high variance
Lower complexity model can generalize to new cases
low variance
But does not ﬁt training data as well - high bias",533,semantic
ddce1012-0ecc-4dcb-a564-4cc4b1bec2d6,Introduction to Linear Models.pdf,CSCI_83,38,"Extending the Linear Model
Building a model matrix for a more complex linear model is easy
We now have  model coefﬁcients, including intercept term
With  features the model matrix is:
We still seek the least squares solution
The covariance matrix is now  x , including intercept term
𝑝+1
=[, , ,…, ]𝑏⃗  𝛽0𝛽1𝛽2 𝛽𝑝
𝑝
𝐴=
⎡
⎣
⎢⎢⎢⎢⎢⎢
1, , ,…,𝑥1,1𝑥1,2 𝑥1,𝑝
1, , ,…,𝑥2,1𝑥2,2 𝑥2,𝑝
1, , ,…,𝑥3,1𝑥3,2 𝑥3,𝑝
⋮ , ⋮ , ⋮ , ⋮ , ⋮ 1, , ,…,𝑥𝑛,1𝑥𝑛,2 𝑥𝑛,𝑝
⎤
⎦
⎥⎥⎥⎥⎥⎥
𝑝+1𝑝+1",452,semantic
527f11bb-faaf-41a8-b264-ea66b2cfba2f,Introduction to Linear Models.pdf,CSCI_83,39,"Example of Multi-Feature Linear Model
A new simulated data set",62,semantic
a06654f4-3822-4467-9a2f-6ba4ae5a6da9,Introduction to Linear Models.pdf,CSCI_83,40,"Example of Multi-Feature Linear Model
First, try a simple straight line model with intercept and slope terms
## Intercept = 10.256  Slope = 1.296",145,semantic
c478b42e-9827-4b87-b5c2-f3181375f632,Introduction to Linear Models.pdf,CSCI_83,41,"Example of Multi-Feature Linear Model
What do the residuals look like? These residuals look heteroskedastic!",108,semantic
6441f8ef-6446-4991-a50e-8a9d6fcdd8ac,Introduction to Linear Models.pdf,CSCI_83,42,"Example of Multi-Feature Linear Model
Test that the residuals are iid Normal
Do these residuals have Normal distribution?",121,semantic
0482736e-261b-4e7e-887b-e9ac922d11d1,Introduction to Linear Models.pdf,CSCI_83,43,"Example of Multi-Feature Linear Model
The model summary is:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.106## Model:                            OLS   Adj. R-squared:                  0.087## Method:                 Least Squares   F-statistic:                     5.684## Date:                Thu, 27 Oct 2022   Prob (F-statistic):             0.0211## Time:                        09:27:09   Log-Likelihood:                -191.24## No.",685,semantic
6ca62c54-957b-494b-a627-68a2a101b090,Introduction to Linear Models.pdf,CSCI_83,43,"Observations:                  50   AIC:                             386.5## Df Residuals:                      48   BIC:                             390.3## Df Model:                           1                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept     10.2555      1.600      6.408      0.000       7.038      13.473## x              1.2955      0.543      2.384      0.021       0.203       2.388## ==============================================================================## Omnibus:                        0.893   Durbin-Watson:                   1.502## Prob(Omnibus):                  0.640   Jarque-Bera (JB):                0.269## Skew:                          -0.037   Prob(JB):                        0.874## Kurtosis:                       3.352   Cond. No. 2.95## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""",1296,semantic
a2899926-0a1d-48c8-9678-2693594e6720,Introduction to Linear Models.pdf,CSCI_83,44,"Example of Multi-Feature Linear Model
Let’s add a second order polynomial term, so the model is now:
## Intercept = 4.051  Partial Slope = 1.296  Second Order Partial slope = 0.715
𝑦= +𝑥+𝛽0 𝛽1 𝛽2𝑥2",197,semantic
b665f273-ce1e-428f-bd78-278fa4d149cc,Introduction to Linear Models.pdf,CSCI_83,45,"Example of Multi-Feature Linear Model
What do the residuals look like with the second order term? These residuals are close to homoskedastic!",141,semantic
3e02c281-a345-4732-93ca-1acd2c0653fd,Introduction to Linear Models.pdf,CSCI_83,46,"Example of Multi-Feature Linear Model
Test that the residuals are iid Normal for the polynomial model
Do these residuals have close to a Normal distribution?",157,semantic
d6095d8b-6694-4012-9dbd-5d6527241683,Introduction to Linear Models.pdf,CSCI_83,47,"Example of Multi-Feature Linear Model
The second order model summary is:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.330## Model:                            OLS   Adj. R-squared:                  0.301## Method:                 Least Squares   F-statistic:                     11.55## Date:                Thu, 27 Oct 2022   Prob (F-statistic):           8.29e-05## Time:                        09:27:10   Log-Likelihood:                -184.04## No.",698,semantic
714c91fe-6753-4f6b-ba6d-c58934185683,Introduction to Linear Models.pdf,CSCI_83,47,"Observations:                  50   AIC:                             374.1## Df Residuals:                      47   BIC:                             379.8## Df Model:                           2                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      4.0507      2.101      1.928      0.060      -0.177       8.278## x              1.2955      0.476      2.725      0.009       0.339       2.252## I(x ** 2)      0.7154      0.181      3.960      0.000       0.352       1.079## ==============================================================================## Omnibus:                        0.305   Durbin-Watson:                   1.995## Prob(Omnibus):                  0.859   Jarque-Bera (JB):                0.014## Skew:                           0.009   Prob(JB):                        0.993## Kurtosis:                       3.081   Cond. No. 17.5## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""",1377,semantic
6db68e36-47c6-407c-ba73-4a54672faa09,Introduction to Linear Models.pdf,CSCI_83,48,"Dealing With Outliers
Outliers are a persistent problem with statistical and machine learning models
What are outliers? Errors or noisy measurements
Result of improper stratiﬁcation
But, may be of interest
Depending on the application can be the most interesting values!! May need to explicitly model
Example: Fraud detection
Example: Scientiﬁc discovery
Outliers can be hard to detect
Difﬁcult in high-dimensions
Often ﬁnd by inﬂuence on model",444,semantic
0e6e6384-603c-4a75-8e92-db8a8bb03177,Introduction to Linear Models.pdf,CSCI_83,49,"Dealing With Outliers
Example: Add a single outlier regression data set
## Intercept = 5.098  Partial Slope = 0.365
This outlier has high leverage and changes the slope signiﬁcantly",181,semantic
5daa9356-3df9-4c33-8c5f-510ddf915cd9,Introduction to Linear Models.pdf,CSCI_83,50,"Dealing With Outliers
Cook’s distance, , measures the inﬂuence of an outlier on a model
Cook’s distance for the ith data point is the degree of freedom adjusted average squared error against a model without this value
where, the jth prediction computed with all observations the jth prediction computed without the ith observation number of parameters number of data points
Cook’s distance is computed using a leave-one-out resampling algorithm! 𝐷𝑖
=𝐷𝑖 ( −Σ𝑛𝑗=1𝑌̂ 𝑗 𝑌̂ 𝑗(𝑖))2
𝑛(𝑝+1)𝜎2^
=𝑌̂ 𝑗 =𝑌̂ 𝑗(𝑖)𝑝=𝑛=",504,semantic
7a5df6dd-2341-4128-8db3-751bc9f0d38a,Introduction to Linear Models.pdf,CSCI_83,51,"Dealing With Outliers
Plot Cook’s distance as leverage vs. the residual size
Outlying points shown at edges of plot
Not all outliers have high leverage
Can detect outliers in moderate dimensions
_=influence_plot(ols_model_ol)",225,semantic
f616e5be-b464-4a50-94c7-4f70fb8b6d2d,Introduction to Linear Models.pdf,CSCI_83,52,"Dealing With Outliers
Why are linear models sensitive to outliers
Ordinary linear regression use squared error loss function
Optimal if the errors are iid Normal
Is an unbiased estimator
In 1-dimension median is robust to outliers
But far from an optimal estimator
High bias
Hard to implement beyond 1-dimension
We can study the response of estimators to outliers using an inﬂuence function",390,semantic
a876119f-85f0-418e-8c97-39edb9befa4d,Introduction to Linear Models.pdf,CSCI_83,53,"Dealing With Outliers
Compare the inﬂuence functions of the mean and median estimators
Mean estimator has linear inﬂuence function
Inﬂuence of outliers is unbounded
Derivative of the inﬂuence function is constant
Inﬂuence function of median estimator is discontinuous
Inﬂuence of any observation is constant
Derivative of inﬂuence function is not deﬁned
Inﬂuence functions for mean and median
Figure from Hampel, el.al., Robust Statistics, 1986",444,semantic
b928f11b-0cef-46e7-8e74-5cc520a08c5a,Introduction to Linear Models.pdf,CSCI_83,54,"Dealing With Outliers
Could we simply edit out the outliers? But what fraction of the data are outliers? Know as the alpha trimmed mean algorithm
Order the values and remove  highest and lowest
But, alpha trimming is a bit arbitrary
Is a biased estimator, with bias increasing with 
 is the median
Alpha trimming hard to implement in higher dimensions
Inﬂuence functions for alpha trimmed mean
Figure from Hampel, el.al., Robust Statistics, 1986
𝛼/2
𝛼
𝛼=0.5",457,semantic
77d5596f-1c98-4c2e-be11-842b3767064e,Introduction to Linear Models.pdf,CSCI_83,55,"Dealing With Outliers
Are there better estimators when outliers are present
Yes, but must accept some bias
Idea; estimator can be unbiased near the expected value, but limit inﬂuence of outliers
Trade-off between high robustness and low bias
Many ideas have been tried
A major research focus in the 1970s and 1980s
Huber estimator
Family of M-estimators",353,semantic
198ac0c4-389f-41aa-995c-c079fca7474e,Introduction to Linear Models.pdf,CSCI_83,56,"Dealing With Outliers
What are the properties of the Huber estimator? Inﬂuence function is linear near the mean but constant away from the mean
hinge point is at , where  median absolute deviation
Robustness and bias increases as  decreases
Huber estimator is low bias
Unbiased for samples near the point estimate
Constant inﬂuence away from the point estimate
Inﬂuence function of the Huber estimator
Figure from Hampel, el.al., Robust Statistics, 1986
±𝑡∗ 𝑀𝐴𝐷 𝑀𝐴𝐷=
𝑡",468,semantic
7b070987-5388-43f6-87b4-9c9b1619213f,Introduction to Linear Models.pdf,CSCI_83,57,"Dealing With Outliers
M-estimators tapper inﬂuence to zero
Approximately linear inﬂuence near point estimate
So nearly unbiased near the point estimate
Then inﬂuence tappers to 0 for extreme outliers
An example is Tukey’s biweight
Only a single parameter for biweight function, 
Robustness and bias increase with decreasing 
Inﬂuence function of the Tukey’s Biweight M-estimator
Figure from Hampel, el.al., Robust Statistics, 1986
𝑟
𝑟",434,semantic
416c9c1b-0db8-46eb-aeef-2e7a4f737763,Introduction to Linear Models.pdf,CSCI_83,58,"Dealing With Outliers
Example: regression with Huber loss function
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                     Robust linear Model Regression Results                    ## ==============================================================================## Dep. Variable:                      y   No. Observations:                   51## Model:                            RLM   Df Residuals:                       49## Method:                          IRLS   Df Model:                            1## Norm:                          HuberT                                         ## Scale Est.:                       mad                                         ## Cov Type:                          H1                                         ## Date:                Thu, 27 Oct 2022                                         ## Time:                        09:27:11                                         ## No.",935,semantic
50d7c8d5-59ac-40f3-8c41-51d474a5fb7c,Introduction to Linear Models.pdf,CSCI_83,58,"Iterations:                    15                                         ## ==============================================================================##                  coef    std err          z      P>|z|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      1.8991      0.503      3.779      0.000       0.914       2.884## x              0.8448      0.087      9.659      0.000       0.673       1.016## ==============================================================================## ## If the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .## """"""
## Define the robust regression model and fit it to the data
ols_model_huber = smf.rlm(formula ='y~x', data = sim_data_ol).fit()# Add predicted to pandas data framesim_data_ol['predicted_huber'] = ols_model_huber.predict(sim_data_ol.x)
## Display sumamryols_model_huber.summary()",995,semantic
c36eead2-9529-4e92-a784-5bbe5e2f4331,Introduction to Linear Models.pdf,CSCI_83,59,"Dealing With Outliers
Example: regression with Huber loss function
Notice the different slope for the regression with Huber loss",128,semantic
39aff864-de7d-475f-bdd1-388ef3c4ac33,Introduction to Linear Models.pdf,CSCI_83,60,"Linear Model Assumptions
There are a number of assumptions in linear models that you overlook at your peril! The feature or predictor variables should be independent of one another
This is rarely true in practice
Multi-colinearity between features makes the model under-determined
We assume that numeric features or predictors have zero mean and about the same scale
We do not want to bias the estimation of regression coefﬁcients with predictors that do not have a 0 mean
We do not want to have predictors with a large numeric range dominate training
Example: income is in the range of 10s or 100s of thousands and age is in the range of 10s, but apriori income is no more important than age as a predictor
Values of each predictor or feature should be iid
If variance changes with sample, the optimal value of the coefﬁcient could not be constant
If there serial correlation in the predictor values, the iid assumption is violated - but can account for this such as in time series models",989,semantic
d52fec62-d3d5-474c-9055-48178e6e2c44,Introduction to Linear Models.pdf,CSCI_83,61,"Summary
Linear models are a ﬂexible and widely used class of models
Fit model coefﬁcients by least squares estimation
Can use many types of predictor variables
SGD and L-FBGS algorithms allow massive scaling of linear models
We prefer the simplest model that does a reasonable job
The principle of Occam’s razor
Must consider the bias-variance trade-off",353,semantic
5f9b16ab-fb5d-4f43-aa34-4e39ffd8d01a,Introduction to Linear Models.pdf,CSCI_83,62,"Summary
When evaluating any machine learning model consider all evaluation methods available
No one method best all of the time
Homoskedastic Normally distributed residuals
Reasonable values , RMSE, etc
Are the model coefﬁcients all signiﬁcant? Different methods highlight different problems with your model
Don’t forget to check that the model must make sense for your application! 𝑅2",385,semantic
fd99c8aa-f855-4263-abae-bf686bf41634,IntroductionToClustering.pdf,CSCI_83,0,"CSCI E-83Introduction to Clustering ModelsSteve Elston
Copyright 2020,2021, Stephen F Elston. All rights reserved.",114,semantic
e778d7b5-ae83-4f19-bec8-873d7cd16281,IntroductionToClustering.pdf,CSCI_83,1,"Introduction to Unsupervised Learning•Until now we have been working with supervised machine learning•Training and evaluation cases are labeled•Data contain features, X, and labels, y•The model learns a function approximation using the labeled cases:𝑓𝑋=𝑦•We say the model is trained by a supervisor•Can we always expect to have labeled cases? •No!•Most data is not labeled•What can be learned from unlabeled data?",413,semantic
37dd0f45-e352-48e6-9bd1-d3d4198655c9,IntroductionToClustering.pdf,CSCI_83,2,"Introduction to Unsupervised Learning•Without labeled data, what can a model learn?•Can learn structure of data•Structure is learned by determining association between cases•Association based on measures of proximity, distance or dissimilarity•Clustering algorithms are data mining methods•Data mining seeks to find interesting relationships in data•We have already encountered feature importance as a data mining method",420,semantic
6dbe6522-dbf9-4462-953b-37849cdfa1e3,IntroductionToClustering.pdf,CSCI_83,3,"Introduction to Unsupervised Learning•Algorithms can use different distance or dissimilarity metrics•Structure based on distance metrics•Different algorithms use different metrics•What are the ideal properties of clusters? •Good clusters have two properties•Compactness: We what the clusters to be small with members close to each other•Separation: We want the clusters are well separated, a closeness property",410,semantic
fa5fe34f-bd11-40d0-9aed-1ac1c3806ce6,IntroductionToClustering.pdf,CSCI_83,4,Introduction to Unsupervised Learning•Why should we care about unsupervised learning?•Most data is not labeled•Learning data structure is useful in many applications•Find groups of similar purchases•Discover unusual or outlier events•Similar genes in microsassay data•Find groups of people similar behaviors – e.g. voting•Find patients with related symptoms•Data compression algorithms•And many more...,402,semantic
dbd14445-c65f-407c-92d9-50c93563f1da,IntroductionToClustering.pdf,CSCI_83,5,"Introduction to Unsupervised Learning•Different dissimilarity metrics will give different results•Dissimilarity metrics usually matter more than model choice•But which one should we use? •Evaluation is a significant problem with for unsupervised learning•There are no labels for objective evaluation•Evaluation is often subjective•But, do we have to pick one best model?•No•Different models can show different useful relationships",430,semantic
2f7d24d8-4aa8-47ba-ab68-a07decfa9c55,IntroductionToClustering.pdf,CSCI_83,6,"Measuring distance or dissimilarity•Structure in data is based on dissimilarity measured by some distance metric•The variables or cases generally have multiple observations or measurements•Distance measures are therefore multivariate    •Distance metrics can be computed for three types of variables•Numeric distance for numeric variables•Rank difference for ordinal variables•Coded difference, usually binary, for unordered categorical variables",446,semantic
6201ea6f-8948-4c19-85ec-9d7621c425c2,IntroductionToClustering.pdf,CSCI_83,7,"Measuring distance or dissimilarityAxioms of distance metrics are required properties of any measure between two points in a space 𝑑(𝑥,𝑦)•Distances are nonnegative 𝑑𝑥,𝑦≥0•Distances are positive except for the distance between a point and itself 𝑑𝑥,𝑦=0	𝑖𝑓𝑓	𝑥=𝑦•Distances are symmetric 𝑑𝑥,𝑦=𝑑(𝑦,𝑥)•Distances must follow the triangle inequality (no shortcuts!)  𝑑𝑥,𝑦≤𝑑𝑥,𝑧+𝑑𝑧,𝑦	𝑓𝑜𝑟	∀	𝑝𝑜𝑖𝑛𝑡𝑠	𝑥,𝑦,𝑧",392,semantic
28dc5648-2c76-4ae6-ae4b-84224da8a659,IntroductionToClustering.pdf,CSCI_83,8,"Measuring distance or dissimilarity•Structure in data is based on dissimilarity measured by some distance metric•How do we organize the dissimilarity metrics? •Create a dissimilarity matrix of the differences between each case xi and every other case x’i, d(xi, x’i)
DX=𝑑𝑥!,𝑥!,𝑑(𝑥!,𝑥"")⋯𝑑(𝑥!,𝑥#)⋮ ⋱ ⋮𝑑𝑛,𝑥!,𝑑(𝑥#,𝑥"")⋯𝑑(𝑥#,𝑥#)",322,semantic
25650d89-71bb-4bb9-b689-156fffb88272,IntroductionToClustering.pdf,CSCI_83,9,"Measuring distance or dissimilarity•A dissimilarity matrix contains the differences between each case xi and every other case x’i, d(xi, x’i)
DX=𝑑𝑥!,𝑥!,𝑑(𝑥!,𝑥"")⋯𝑑(𝑥!,𝑥#)⋮ ⋱ ⋮𝑑𝑛,𝑥!,𝑑(𝑥#,𝑥"")⋯𝑑(𝑥#,𝑥#)•In general, the dissimilarity matrix is symmetric•The diagonal elements of the dissimilarity matrix are all 0, there is no dissimilarity",334,semantic
a7476816-8fbe-458c-9b97-2a589f5e1519,IntroductionToClustering.pdf,CSCI_83,10,"Measuring distance or dissimilarity•Different metrics for numeric dissimilarity can be computed for numeric variables in p dimensions•Weighted sum of squared distance (square of Euclidian distance, L2)𝑑$""𝑥,𝑥′=	>%&! '𝑤%(𝑥%	−𝑥′%)""
•Weighted absolute distance or Manhattan distance (L1)𝑑$!𝑥,𝑥′=>%&! '𝑤%𝑥%−𝑥′%•Weighted Max; or 𝐿) norm𝑑$)𝑥,𝑥′=max%	𝑤%𝑥%−𝑥′%",351,semantic
b1f2eaf4-e39f-41ff-a06f-d2d7219a0cd9,IntroductionToClustering.pdf,CSCI_83,11,"Measuring distance or dissimilarity•Different metrics for numeric dissimilarity can be computed for numeric variables in p dimensions•Mahalanobis Distance (standardized Euclidean distance) from a mean µ    	𝑑(𝑥,𝜇)=𝑥−𝜇*𝑆+!𝑥−𝜇Where, 𝑆+! is the empirical estimate of covariance •Mahalanobis Distance between two points     	𝑑(𝑥,𝑥′)=𝑥−𝑥′*𝑆+!𝑥−𝑥′Where, 𝑆+!=𝑥G𝑥′* is the empirical estimate of covariance between the two vectors •Mahalanobis distance is scale invariant and unitless",475,semantic
846c6c52-fb17-4e03-b562-c2e08e0fbffc,IntroductionToClustering.pdf,CSCI_83,12,"Measuring distance or dissimilarityUnweighted sum of squares and Euclidian distance𝑑𝑥,𝑥′=	>%&! '	(𝑥%	−𝑥′%)""
X1
X2
(2,1)
(4,3)•Points at (2,1) and (4,3)•What is the distance between them? •Sum of squares𝑑!!𝑥,𝑥′=2−4""+1−3""=8•Euclidian Norm  𝑑!!𝑥,𝑥′=𝑑#(𝑥,𝑦)$/""=8=2.83•Euclidian Norm is ‘crow-flies’ distance",303,semantic
93cc6587-e551-4ce3-b99b-a5f72363995c,IntroductionToClustering.pdf,CSCI_83,13,"Measuring distance or dissimilarityManhattan Norm𝑑𝑥,𝑥′=>%&! '𝑥%−𝑥′%
X1
X2
(2,1)
(4,3)•Points at (2,1) and (4,3)•What is the distance between them? •Manhattan Norm𝑑&𝑥,𝑥′=2−4+1−3=4•Manhattan distance is distance traveled on a grid",228,semantic
37d6b73a-a207-47ea-bc85-1429b6eeed0e,IntroductionToClustering.pdf,CSCI_83,14,"Measuring distance or dissimilarity•Different metrics for numeric distance can be computed for numeric variables in p dimensions•Do all p dimensions matter equally in determining dissimilarity? •No!•The weights, wi, can be set for the dimensions of the attributes•Weights must add to 1.0:  ∑'($)𝑤'=1.0 •Why not use equal weights?•Attributes may not have equal importance•Example; are the distance I have to walk and the calories of my take-out order of the same importance in selecting where I get my food? •Perhaps, the calories matter more to me in determining dissimilarity between restaurants?",597,semantic
1d2b8c9d-6396-4f09-bd8c-9dfa9a2e612c,IntroductionToClustering.pdf,CSCI_83,15,"Measuring distance or dissimilarity•Different metrics for numeric distance can be computed for numeric variables in p dimensions•Numeric variable must be standardized before performing unsupervised learning•Otherwise, like many ML methods, the importance of a variable or attribute must not be determined by the numeric range•A useful default is to set the weights, wi, to adjust for the variance and normalization, r, for p variables 𝑤%= 1𝑟∗𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒𝑖, 𝑟=>%&! '𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒%•Or better, set weights to have meaning from the problem•Typically requires domain knowledge",562,semantic
4127900e-e55c-4591-8955-73319ee90be5,IntroductionToClustering.pdf,CSCI_83,16,"Measuring distance or dissimilarity•How can we express dissimilarity metrics for ordinal variables? •Star ratings, e.g. 1 to 5 stars•Dollar range, $, $$, $$$, $$$$•Based on rank differences•Let 1 star = 1, 2 star = 2, 3 star = 3, 4 star = 4, and 5 star = 5•The distance between a 2 star and 4 star restaurant is 2•Or in normalized form with M possible levels, and ranks 𝑥%,𝑥′% 𝑑𝑥%,𝑥′%=𝑥%−𝑥′%𝑀    On a 5-star scale, the dissimilarity between a 3 star and 5 star restaurant:3−55=0.4",480,semantic
d31741bf-b72c-4602-b24f-1e3792c62d91,IntroductionToClustering.pdf,CSCI_83,17,"Measuring distance or dissimilarity•How can we express dissimilarity metrics for unordered categorical variables? •Use a coding scheme•e.g. a binary scheme, 1 if different categories, 0 if the same•Then normalize•e.g., normalize by dividing by number of categories•Example; restaurant type from 4 choices:•Vegetarian, Thai, Mexican, Pizza, Burgers,….•Distance Vegetarian to Burgers = ¼ = 0.25•Distance Burgers to Burgers = 0•Can have more complex scheme if required",465,semantic
b2aad031-f98a-48ea-8cdb-62b67d50d225,IntroductionToClustering.pdf,CSCI_83,18,"Measuring distance or dissimilarity•Can we combine dissimilarities of different types of variables•Yes, but with can!•Scaling is important so some variable types do not dominate as a result of coding•The numeric range of values must be similar•e.g. We do not want ordinal variables to dominate numeric and categorical•But, weighting is problem dependent!•Unfortunately, no simple procedure for correct weighting•Do I care more about the star rating or type of food at a restaurant?",481,semantic
5df08d5f-5086-4bc5-89b6-b0f3101c7e81,IntroductionToClustering.pdf,CSCI_83,19,"Measuring distance or dissimilarity•An example of computing dissimilarity between restaurants
•Absolute dissimilarity between R1 and R4, a Mexican and vegetarian restaurant[¼ + |0.8-0.3| + |0.7-0.1| + |3-1|/4 + |5-4|/5]/6 = [0.25 + 0.5 + 0.6 + 0.5 + 0.2]/6 = 0.342•Absolute dissimilarity between R2 and R3, two pizza places[0 + |0.9-0.8| + |0.6-0.4| + |3-2|/4 + |4-3|/5]/6 = [0 + 0.1 + 0.2 + 0.25 + 0.2]/6 = 0.125
RestaurantTypeCaloriesDistancePrice RangeStarsR1 Mexican0.8 0.1 $ 4R2 Pizza 0.9 0.4 $$ 3R3 Pizza 0.8 0.6 $$$ 4R4 Vegetarian0.3 0.7 $$$ 5R5 Thai 0.6 0.3 $$ 4",570,semantic
3935694b-8e0f-4110-a2b7-c0937ef8fc5f,IntroductionToClustering.pdf,CSCI_83,20,"Measuring distance or dissimilarity•Another example of computing dissimilarity between restaurants
•Squared dissimilarity between R1 and R4, a Mexican and vegetarian restaurant[0.252 + 0.52 + 0.62 + 0.52 + 0.22]/6 = [0.125 + 0.25 + 0.36 + 0.25 + 0.04]/6 = 0.171•Squared dissimilarity between R2 and R3, two pizza places[0 + 0.12 + 0.22 + 0.252 + 0.22]/6 = [0 + 0.01 + 0.04 + 0.125 + 0.04]/6 = 0.029
RestaurantTypeCaloriesDistancePrice RangeStarsR1 Mexican0.8 0.1 $ 4R2 Pizza 0.9 0.4 $$ 3R3 Pizza 0.8 0.6 $$$ 4R4 Vegetarian0.3 0.7 $$$ 5R5 Thai 0.6 0.3 $$ 4",555,semantic
b6c6428f-730b-4199-9a1b-bf7fda58d1c0,IntroductionToClustering.pdf,CSCI_83,21,"Measuring distance or dissimilarity•How much do different metrics matter?•A lot!•Compare dissimilarity metrics between the restaurants
•The squared distance emphasizes larger differences
Mexican – Vegetarian2 Pizza PlacesAbsolute distance0.342 0.125Squared distance0.171 0.029",276,semantic
8face2a1-f140-4582-b7f3-a4fbfef4cd77,IntroductionToClustering.pdf,CSCI_83,22,"Measuring distance and similarity•Do we always work with dissimilarity?•No! •Some methods use measures of similarity•The closer points in a space are (smaller distance) the more similar they are•In many cases, similarity measures can be transform to dissimilarity •For data with positive and negative coding similarity must be in the range [-1,1]•Similarity = 1, maximum similarity, points are at the same location in the space•Similarity = 0, points are orthogonal in the space, no similarity•Similarity = -1, minimum similarity, points have completely opposite coding •For non-negative data similarity in range [0,1] •Example, binary data",640,semantic
e908251a-a52d-4d89-8699-7c5259fbacb2,IntroductionToClustering.pdf,CSCI_83,23,"Measuring Similarity•Different metrics for similarity can be computed for numeric variables in p dimensions•Pearson distance correlation, for two vectors of values (𝑥,𝑥,)𝑠𝑥,𝑥′𝐶𝑜𝑣(𝑥,𝑥′)𝜎""(𝑥)𝜎""(𝑥′)=(𝑥−̅𝑥)(𝑥′−̅𝑥′)*𝜎""(𝑥)𝜎""(𝑥′)	•Notice the different formulation as a similarity metric•Other correlation to measure similarity, e.g. Kendal, Spearman, more robust than Pearson",368,semantic
c42a957a-1f9f-4448-b517-bdd31e54d745,IntroductionToClustering.pdf,CSCI_83,24,"Measuring Similarity•Different metrics for similarity can be computed for discretely coded or numeric variables in p dimensions•Jaccard Similarity: For discretely coded data𝑠𝑥,𝑥′=𝑆𝑖𝑧𝑒	𝑜𝑓	𝐼𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛(𝑥,𝑥,)𝑆𝑖𝑧𝑒	𝑜𝑓	𝑈𝑛𝑖𝑜𝑛(𝑥,𝑥,)=𝑥∩𝑥′𝑥∪𝑥′•Jaccard similarity often used for data with many categories like natural languages•Jaccard similarity is non-Euclidean!•Cosine similarity: for numeric or discretely coded datasx,x,=cos𝜃%=𝑥G𝑥′𝑥𝑥′",428,semantic
1473c24e-fc95-4ae6-94e2-f3262d1a3357,IntroductionToClustering.pdf,CSCI_83,25,"Measuring SimilarityCosine Similaritydx,x,=cos𝜃%=𝑥G𝑥′𝑥𝑥′
X1
X2
(2,1)
(4,3)•Points at (2,1) and (4,3)•What is the distance between them? •Cosine distance𝑑𝑐𝑜𝑠𝑥,𝑥*=""∗,	.$∗/""!.$!""/!,!./!""/!=115$/""25$/""=0.984•If vectors point in same direction 𝜃=0,cos𝜃=1	•If vectors orthogonal𝜃=𝜋2,cos𝜃=0
q",285,semantic
05a733d2-8baa-4f8d-8cf2-60106b610125,IntroductionToClustering.pdf,CSCI_83,26,"Relationship Between Similarity and DistanceSome similarity measures can be transformed to distance metrics•Transform is an inverse function•Resulting distance usually has limited range (limited support)•Examples: Similarity MeasureTransform to Distance MetricJaccard Similarity𝑑-𝑥,𝑥,=1	−	𝑠-(𝑥,𝑥,)Cosine Similarity𝑑.#/01.2𝑥,𝑥,=𝑐𝑜𝑠+!(𝑠345(𝑥,𝑥′)𝜋",344,semantic
9ea770f0-271e-47c5-878a-231942cab2a9,IntroductionToClustering.pdf,CSCI_83,27,"Relationship Between Distance and SimilarityIn some cases distance measures can be transformed to similarity•Typically not a unique transformation   •Similarity must be in proper range:  •[-1,1]•[0,1] •Examples of transformations (e.g. for Euclidean metrics) :  𝑠𝑥,𝑥,=11+𝑑6(𝑥,𝑥,)𝑠𝑥,𝑥,=1𝑒7!(9,9"")",295,semantic
5ab2d563-91ec-448b-a1b5-c3bb64dff69f,IntroductionToClustering.pdf,CSCI_83,28,"K-Means Clustering•The K-means clustering algorithm is arguably the most widely used method•Long history•First proposed as a coding method by Stuart Lloyd in 1957 – not published until 1982•Term ‘k-means’ coined by MacQueen in 1967•The goal of the k-means algorithm is to find the best k clusters•How do we define best for the k-means algorithm?•Dissimilarity metric is sum of squared distances •Minimize dissimilarity within clusters – minimize the inertia within clusters
argmin𝑪>%&! =>9∈?#
	(𝑥−̅𝑥%)"",for	k	clusters	𝑪∈𝐶!,𝐶"",…,𝐶=",530,semantic
851d908c-95b5-4119-9d0a-f06c4cc25a68,IntroductionToClustering.pdf,CSCI_83,29,"K-Means Clustering•The goal of K-means clustering algorithm is to Minimize dissimilarity or inertia within clustersargmin𝑪>%&! =>9∈?#
	(𝑥−̅𝑥%)"",for	k	clusters	𝑪∈𝐶!,𝐶"",…,𝐶=
•Is there a practical algorithm to directly solve this optimization problem? •Unfortunately, no!",268,semantic
635eb4e4-06f9-454e-ba4f-813e9f9491de,IntroductionToClustering.pdf,CSCI_83,29,"•The unknown number of clusters, k, must be assigned•For each possible k, there are a combinatorial number of possible cluster assignments to search•No simple search algorithm is feasible!",188,semantic
032a26d0-6789-4881-ab30-ea4e8b253e48,IntroductionToClustering.pdf,CSCI_83,30,"K-Means Clustering•The goal of K-means clustering algorithm is to Minimize dissimilarity within clustersargmin𝑪>%&! =>9∈?#
	(𝑥−̅𝑥%)"",for	k	clusters	𝑪∈𝐶!,𝐶"",…,𝐶=
•Search will not work.•Need a good heuristic! •Lloyd’s algorithm for k-means clustering1.Select k and starting (initial) centroids 2.Find the nearest cluster centroid for each x, given the means of the clusters , ̅𝑥' 3.Update the k centroids, ̅𝑥',𝑖∈1,2,…,𝑘, given new cluster members4.Repeat steps 2 and 3 until change in all ̅𝑥' is minimal",501,semantic
2fe090f4-b6ae-4455-99db-d5d99766c1fc,IntroductionToClustering.pdf,CSCI_83,31,"K-Means Clustering•Lloyd’s algorithm for k-means clustering, initializes with random start and iterates over several stepsXXXX X XXXXX XXXX
2.Assign Random cluster centers
1.Start with data points3.Initial cluster assignments 4.Update cluster centers and assignments5.Iterate cluster centers and assignments until convergence",325,semantic
d499d868-8ed0-4774-9179-84d25a73781d,IntroductionToClustering.pdf,CSCI_83,32,"K-Means ClusteringLloyd’s algorithm for k-means clustering1.Select k and starting (initial) centroids 2.Find the nearest cluster centroid for each x, given the means of the clusters, ̅𝑥% min9∈?#,%∈{!,"",…=}𝑥−̅𝑥%2,∀	𝑖	3.Update the k centroid, ̅𝑥%,𝑖∈1,2,…,𝑘̅𝑥%=1𝑛>9∈?#𝑥𝑖
4.Repeat steps 2 and 3 until change in all ̅𝑥% is minimal - convergence",339,semantic
d51f0561-08de-4354-bd6a-6cc43b25355a,IntroductionToClustering.pdf,CSCI_83,33,"K-Means Clustering•There are several difficulties with using the k-means algorithm•Using sum of square distance leads to only convex clusters•What is the value of k, the number of clusters?•The number of clusters should reflect fundamental organization of the data•Is there anyway to know k in advance for high dimensional problems? •No, k is usually found empirically, and the selection is often a bit subjective•How to find good starting values of the centroids?•At convergence the resulting clusters depend on the starting values•Is there any algorithm for finding good starting values? •No!•So, multiple random starts are often used
Copyright 2020, Stephen F Elston.",670,semantic
0af2b710-0b34-4b15-a5e5-94084138f542,IntroductionToClustering.pdf,CSCI_83,33,All rights reserved.,20,semantic
ffd2a2f2-5fc2-42cf-8545-da1abeb7830d,IntroductionToClustering.pdf,CSCI_83,34,"K-Means Clustering•K-means clustering produces convex clusters•Clusters are linearly separated•Clusters are Voronoi regions•Analogous to k-NN methods
Credit: Scikit-Learn development team; Pedregosaet al., JMLR 12, pp. 2825-2830, 2011",234,semantic
5ce1eeaf-63ce-4111-9ab6-f8975b23d2fa,IntroductionToClustering.pdf,CSCI_83,35,"Evaluating Clustering Models•How do we evaluate clustering models? •No direct measure as with supervised machine learning•Can we use cross validation? •No, not directly, we have no labels•Use metrics that measure the properties of the clusters•Compactness of clusters•Separation between clusters•For k-means clustering the distance metric is quadratic•Use sum of square metrics",377,semantic
bba88058-d5ed-497c-9597-f918df380ce8,IntroductionToClustering.pdf,CSCI_83,36,Evaluating Clustering Models•How do we evaluate clustering models? •Within and between cluster sum of squares•Can compare cluster models•Determine k•Other methods•Silhouette coefficients – Useful for small datasets  •Calinski-Harabasz index•Many others…….•Notice that all of these methods assume clusters are Normally distributed!,330,semantic
37e19b65-ff06-4bf7-a103-c0ece975cb03,IntroductionToClustering.pdf,CSCI_83,37,"Evaluating Clustering Models•Between and within cluster metrics•Within cluster sum of squares is the sum of squares within each cluster𝑤𝑐𝑠𝑠=>?#
	 >9$∈?#(𝑥C−̅𝑥?#)""
•Total sum of squares𝑡𝑠𝑠=>%(𝑥%−̅𝑥)""
•Between cluster sum of squares𝑡𝑠𝑠=𝑤𝑐𝑠𝑠+𝑏𝑐𝑠𝑠𝑏𝑐𝑠𝑠=𝑡𝑠𝑠−𝑤𝑐𝑠𝑠",256,semantic
f2b21198-d906-4aee-8571-25b3dd051491,IntroductionToClustering.pdf,CSCI_83,38,Evaluating Clustering Models•Use WCSS and BCSS to compare clustering models•Limit number of clusters where WCSS and BCSS  have marginal changes,143,semantic
4f4a5e8c-655a-4d03-be8b-3e0a774d1006,IntroductionToClustering.pdf,CSCI_83,39,"Evaluating Clustering Models•Use WCSS to find best number of clusters•Can use knee in WCSS curve to determine best k•In the example shown, pick k=4, or k=5•Prefer simpler models – Occam’s razor!•Too many clusters is an over-fit model",233,semantic
49d73b24-2252-4096-a9ea-90887a19b047,IntroductionToClustering.pdf,CSCI_83,40,"Evaluating Clustering Models•The Calinski-Harabasz index or variance ratio criteria measure the ratio between the cluster compactness and separation•Starting with the WCSS matrix and the BCSS matrix, the Calinski-Harabasz index is a degree of freedom adjusted ration of the matrix trace (sum of diagonal)𝐶𝐻𝑘=𝑡𝑟(𝐵𝐶𝑆𝑆)𝑡𝑟(𝑊𝐶𝑆𝑆)𝑛D−𝑘𝑘−1•The larger the Calinski-Harabasz index the higher the ratio of cluster separation to cluster compactness.•Since it is based on variance measures, Calinski-Harabasz index tends to favor convex clusters",532,semantic
e4ace662-4390-4e1e-a6a7-87e5e3814a7d,IntroductionToClustering.pdf,CSCI_83,41,Hierarchical Clustering Algorithms•K-means algorithms form clusters with maximum compactness•Are there other ways to create clusters with compact clusters? •Hierarchical clustering algorithms create compact clusters •Algorithm sequentially considers groups of points•Maximum compact clusters created at each step of sequential algorithm•Create hierarchy of possible clusters•One cluster with all samples at the top•Single sample clusters at the bottom,451,semantic
c4084fa3-7e33-4bb0-96d6-1d1005065bdf,IntroductionToClustering.pdf,CSCI_83,42,"Hierarchical Clustering Algorithms•Two possible approaches to hierarchical clustering•Divisive clustering – top down•Start will all samples in one large clusters•Recursively split into compact clusters•Stop when only single samples at leaves – singletons  •Not discussed further here•Agglomerative clustering – bottom up•Start with each sample in an individual cluster - singletons•Build maximumly compact clusters, until all samples in one cluster•Forms a tree with singletons at the leaves and all samples at the root",519,semantic
476ebeff-bfd1-452a-b595-de2111fb09ed,IntroductionToClustering.pdf,CSCI_83,43,"Hierarchical Clustering Algorithms•Agglomerative clustering – bottom up•Use dissimilarity between samples to determine cluster compactness•Use most any distance metric•A linkage function determines the clusters to link at the next step•Single linkage uses the minimum distance between members of the clusters to link clusters𝑑!0𝐺,𝐻=min'∈2,4∈5𝑑(𝑥',𝑥4)•Complete linkage uses the minimum distance between members of the clusters to link clusters𝑑60𝐺,𝐻=max'∈2,4∈5𝑑(𝑥',𝑥4)•Average linkage uses the minimum average distance between members of the clusters to link clusters𝑑!0𝐺,𝐻=1𝑁21𝑁5L'∈2
	L4∈5
	𝑑(𝑥',𝑥4)",599,semantic
3c072c3a-493d-4cb5-ab27-f3da9e88e50e,IntroductionToClustering.pdf,CSCI_83,44,"Hierarchical Clustering Algorithms•Single linkage:•Uses single closest pair of samples to link clusters•Can combine clusters with low threshold, chaining behavior•Often produces clusters with poor compactness•Complete linkage:•Uses single furthest pair of sample to link clusters•Creates compact clusters•May have poor separation•Average linkage:•Uses average distances to links clusters•Tries to balance compactness and separation",431,semantic
44aa734c-fefe-4ff8-b2a9-d7b8abfe5e9d,IntroductionToClustering.pdf,CSCI_83,45,"Agglomerative Clustering Example•Start with data points in a 2-dimensional Euclidian space•Use Euclidean distance and average linkage to find the first points to link –the leaves of the hierarchy   •Continue to link points into clusters•At termination of algorithm all points are linked at root of hierarchy1
2
3
4
5
6",318,semantic
42f5fc16-c789-40ae-9c0c-d1ca6cb28747,IntroductionToClustering.pdf,CSCI_83,46,Hierarchical Clustering•Hierarchical cluster creates dendrogram•Number of clusters determined by depth of cut point•For example the cut-point shown results in 6 clusters•Or maybe 3 clusters?•Need some domain knowledge to determine which is useful,246,semantic
9dba6ed1-988d-44b1-8e12-0d7073a175e2,IntroductionToClustering.pdf,CSCI_83,47,"Hierarchical Clustering•Example of microassay of human tumors•The dendrograms are quite different•Choice of linkage function creates different hierarchy
Credit: Hastie, Tibsheirani and Friedeman, 2009",200,semantic
552d1562-40bb-40e8-9b3f-1a2b8c42b506,IntroductionToClustering.pdf,CSCI_83,48,"Evaluating Hierarchical ClusteringHow can we evaluate hierarchical clustering models? •No one best method•Often requires some subjective judgement•Different models may highlight different aspects of data structure•For Euclidean distance metric use same methods as k-means clustering•But, sum of squares is meaningless in non-Euclidian spaces!•And there is no cluster center (mean) except for Euclidean (𝐿"") space•Instead use a data point – the clusteriod•Radius of a cluster in non-Euclidean space is not well-defined",517,semantic
901c9115-4be5-417d-8d0e-b7e4e00c2a4f,IntroductionToClustering.pdf,CSCI_83,49,"Evaluating Hierarchical ClusteringHow can we evaluate hierarchical clustering models? •Can use the linkage metric •Consistent with the agglomeration criteria•But is not independent evaluation    •Cluster diameter: is the 𝐿) metric of the cluster•Does not require knowing the cluster center 𝑑6$=max',4	∀	'84𝑥'−𝑥′4•The global maximum over all clusters is then:  𝑑9:;𝐶'=	max6$𝑑6$",376,semantic
f7ee7bb5-2dfd-48a5-87f7-3e9d42d2a222,IntroductionToClustering.pdf,CSCI_83,50,"What Could Possibly Go Wrong? Curse of Dimensionality!•Cluster models scale poorly with dimensionality      •Consider sampling required to maintain the same uniformly distributed density in a hypercube: 
•An example of the Curse of Dimensionality! DimensionsSamples1 102 1003 1,0005 10510 1010",293,semantic
b5dc0ce8-aeed-4a96-a6b0-07f24def17ed,IntroductionToClustering.pdf,CSCI_83,51,"What Could Possibly Go Wrong? Curse of Dimensionality•Cluster models scale poorly with dimensionality •Distances all become the same as dimensionality ⇒∞      •To understand this problem, consider Euclidean distance measure:   •The volume of a hypersphere inside a d-dimensional hypercube with edge length 2d is:     𝑉𝑜𝑙𝐻𝑦𝑝𝑒𝑟𝑠𝑝ℎ𝑒𝑟𝑒𝑠 = ""2%E%/'7F7/""•The proportion of volumes of the hypercube and the spheres:    H41%&'()*'+()(*H41%&'(),-.(=E%/'7""%()F7/""⇒0,𝑎𝑠	𝑑	⇒∞ •Another example of the Curse of Dimensionality!Γ𝑥 is the Gamma Function",535,semantic
1a88da30-1148-46e7-9220-33f2f4bf0c31,IntroductionToClustering.pdf,CSCI_83,52,What Could Possibly Go Wrong? Curse of Dimensionality•The curse of dimensionality means that all clusters are the same in high dimensions•Sampling density decreases exponentially   •Distances are converge to the same size in a finite space  •The choice of metric does not help   •Implication is that high-dimensional cluster models are easy to overfit!!•Reducing dimensionality can help,386,semantic
63acb840-8314-49df-9f9f-693963654b9c,IntroductionToClustering.pdf,CSCI_83,53,Affinity Clustering •Can we build a clustering algorithm using a relationship to a few prototypes or exemplar points? •Yes•Affinity clustering uses a message passing algorithm to find these responsible or exemplar points and create a connected graph of cluster members•Affinity clustering provides useful results in several areas•Document similarity•Computer vision•Computational genetics,388,semantic
849116b0-fcbc-4572-96dc-ea6489c7e98e,IntroductionToClustering.pdf,CSCI_83,54,"Affinity Clustering •Can we build a clustering algorithm using a relationship to a few prototypes or exemplar points? •The number of representative points, and clusters, are determined by the algorithms•No need to specify number of clusters apriori •The message passing algorithm iteratively determines•Responsibility, r(i,k), of sample k to be the exemplar of sample i•Availability, a(i,k), of sample k to be the exemplar of sample i",434,semantic
23a9a598-8886-4b32-b7c2-268cee9830fa,IntroductionToClustering.pdf,CSCI_83,55,"Affinity Clustering •For the affinity clustering algorithm, points pass messages •Given the similarity between points, s(i,k), the algorithm follows these steps1.Set initial values, r(i,k) = 0, a(i,k) = 0 2.Update the responsibility𝑟𝑖,𝑘←𝑠𝑖,𝑘+max∀<*8<𝑎𝑖,𝑘+𝑠𝑖,𝑘*	3.Update the availability𝑎𝑖,𝑘←m𝑖𝑛0,𝑟𝑘,𝑘+1'/∀'/8{',<}
	 𝑟(𝑖*,𝑘)4.Repeat steps 2 and 3 until convergence",363,semantic
11e8bafe-6a33-4cfc-bce9-78e347f708f0,IntroductionToClustering.pdf,CSCI_83,56,"Affinity Clustering •Affinity propagation determines a number of clusters•Clusters linearly separated and convex•Can define clusters as Voronoi regions•Different similarity metrics create different clusters•Algorithm can be slow, O(n2) complexity 
Credit: Scikit-Learn development team; Pedregosaet al., JMLR 12, pp. 2825-2830, 2011",332,semantic
12bb4a78-a35f-4060-87df-e5bb73471af1,IntroductionToClustering.pdf,CSCI_83,57,"Density Clustering •Using sample density is another way to form clusters•High density points form clusters•How can we find high density points? •Find points with large number of near neighbors•Form clusters around these high density points•Naive algorithms have high complexity•Scalable algorithms are largely heuristic•Many algorithms, including•DBSCAN•OPTICS",360,semantic
7ffe96bd-1582-4027-b45e-742442f9b5b0,IntroductionToClustering.pdf,CSCI_83,58,Density Clustering •DBSCAN is the first (1996) large-scale density-based clustering algorithm•Still in use today•Many variations created•Scales to out of memory dataset size•DBSCAN minimizes the number of passes through the data base•DBSCAN finds a graph of nearest neighbors•Core points define the high density areas•Reachable non-core points are in a cluster but not core•Unreachable points are not in a cluster•Graph edge is bidirectional if both points are core•Graph edges from core to reachable non-core point is unidirectional,533,semantic
cf4f9679-71e9-44e6-9e77-132993705f18,IntroductionToClustering.pdf,CSCI_83,59,"Density Clustering •DBSCAN has two hyperparameters•minPts is the minimum number of near neighbors a core point must have•e is the maximum distance between neighbors•How do we define reachability?•Must have a path on the graph between points p1 and pn•The path can pass through other points, {p1,p2,…,pn}•Distance to neighbors must be less than e•Points not reachable from any other point are not reachable•DBSCAN makes passes thorough a database to create the graph•Highly scalable, O(n log(n))  •Uses memory, O(n2)",515,semantic
badf7d0e-1bd5-4105-aa6c-a53c1204dbec,IntroductionToClustering.pdf,CSCI_83,60,"Density Clustering •What are the steps of DBSCAN?•Start with some samples or observations•For each point, find near neighbors within distance e•A point is core if it has > minPts near neighbors (2 in this example)•If two point are core, connect with bidirectional edge•If point is non-core, connect with unidirectional edge•If point further that e from neighbors, point is non-reachable",386,semantic
d7c99f89-58ab-430e-ae18-16b719ebbce3,IntroductionToClustering.pdf,CSCI_83,61,"Density Clustering •What are some properties of DBSCAN?•Can find non-convex clusters•Is fast and efficient •Optimized for uniform sample density of clusters with fixed e and minPts•Selecting maximum distance to neighbors, e, is difficult in high dimensions•Clusters depend on distance metric•Clusters robust to noise, non-reachable samples
Credit: Wikipedia commons",365,semantic
117b6b20-3dd9-40ab-b199-0f546c01ef96,IntroductionToClustering.pdf,CSCI_83,62,Density Clustering •How can we overcome the limitations of DBSCAN?•Optimized for uniform density clusters•Hard to find good value of e in high dimensions •The OPTICS algorithm is an attempt to improve on DBSCAN•OPTICS uses a heuristic to find two distances•Core distance determines if a sample is core•Reachability distance determines if one sample is reachable from the other,376,semantic
5ba7285d-20a1-4b1d-96fe-ae1df679232e,IntroductionToClustering.pdf,CSCI_83,63,"Density Clustering •The OPTICS algorithm is an attempt to improve on DBSCAN•Core distance for a point, p, determines if a sample is corecore_dist+,-./012𝑝=, 𝑈𝑛𝑑𝑒𝑓𝑖𝑛𝑒𝑑	𝑖𝑓𝑁+<𝑚𝑖𝑛𝑃𝑡𝑠	𝑚𝑖𝑛𝑃𝑡𝑠	−𝑠𝑚𝑎𝑙𝑙𝑒𝑠𝑡	𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒	𝑖𝑛	𝑁+,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒•Reachability distance of observation, o, determines if one sample is reachable from the otherreachable_dist+,-./012𝑜,𝑝=G𝑈𝑛𝑑𝑒𝑓𝑖𝑛𝑒𝑑	𝑖𝑓𝑁+<𝑚𝑖𝑛𝑃𝑡𝑠	max(core3456+,-./012𝑝,𝑑𝑖𝑠𝑡(𝑝,𝑜),𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒•Both distances are undefined if density in the, 𝑁+, neighborhood is too low•If p and o and nearest neighbors, then e’ < e and p and o are in the same cluster",562,semantic
86665d85-8f6c-466a-95f1-ed8542bf1cef,IntroductionToClustering.pdf,CSCI_83,64,"Density Clustering •How can we understand the OPTICS algorithm?•Start with some samples or observations•For a point, p, find the points within e distance•If 𝑁I>𝑚𝑖𝑛𝑃𝑡𝑠, determine core_dist, CD•Find reachable_dist (RD)•Points beyond reachable_dist are not in the cluster
eCDRD",274,semantic
5c871015-fa59-47bd-a5e9-752eb1fbf237,IntroductionToClustering.pdf,CSCI_83,65,"Density Clustering •What are some properties of OPTICS?•OPTICS builds a graph with the distances determining clusters•The graph defines a dendogram•The reachability plot (bottom) shows the path distances in the dendogram•A cutoff on the reachability plot defines the clusters 
Credit: Wikipedia commons",302,semantic
76ccbca5-4cba-4220-ba7b-898f6eb36114,IntroductionToClustering.pdf,CSCI_83,66,"Density Clustering •What are some properties of OPTICS?•Dynamically determining core_dist helps with with variable sample density•Computation complexity higher on average than DBSCAN
Credit: Wikipedia commons",208,semantic
acc34f58-9d59-407b-ba1a-7db10d40c7ce,IntroductionToClustering.pdf,CSCI_83,67,"Summary•Unsupervised learning methods learn structure of data•Structure is learned by determining association between cases•Association based on measures of proximity, distance or dissimilarity•Clustering algorithms are data mining methods•Data mining seeks to find interesting relationships in data•We have already encountered feature importance as a data mining method",370,semantic
ac7439bf-d5a2-4084-b63f-67bee4aff105,IntroductionToClustering.pdf,CSCI_83,68,"Summary•Algorithms can use different distance or dissimilarity metrics•Structure based on distance metrics•Different algorithms use different metrics•What are the ideal properties of clusters? •Good clusters have two properties•Compactness: We what the clusters to be small with members close to each other•Separation: We want the clusters are well separated, a closeness property",380,semantic
8423f0cc-4d39-48c7-a8f6-7a75d238372e,IntroductionToClustering.pdf,CSCI_83,69,"Summary•Different models and dissimilarity metrics will give different results•Dissimilarity metrics usually matter more than model choice•But which one should we use? •Evaluation is a significant problem with for unsupervised learning•There are no labels for objective evaluation•Evaluation is often subjective•But, do we have to pick one best model?•No•Different models can show different useful relationships",411,semantic
9398555e-2176-4c20-9452-06b3855fe19a,IntroductionToClustering.pdf,CSCI_83,70,"Summary•Different models have different results and advantagesAlgorithmMethodCluster characteristicCharacteristicsScalabilityK-means Maximizes cluster compactness ConvexNeed to find kAssumes equal varianceVery high, O(n)
Agglomerative hierarchical Clusters into dendrogramConvexVery dependent on linkage functionVery high, O(n)Affinity clusteringResponsible point in clusterConvexLess scalableLimited, O(n2)DBSCANDensity graphNonconvexUniform density Very High, O(n log(n))OPTICSDensity graphNonconvexVariable densityHigh, O(n2)",528,semantic
71234d01-cc15-4239-ad0c-24106353bd1e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,0,"Introduction to Nonparametric Bootstrap Methods
Steve Elston
10/06/2022",71,semantic
0a8ab95f-59ec-4436-b587-76d366950d1e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,1,"Introduction
“There were others who had forced their way to the top from the lowest rung by the aid of their bootstraps.”James Joyce, ‘Ulysses’ 1922
Bootstrap and re-sampling methods are widely applicable statistical methods
Resampling methods are products of the computer age
Use computational resources unimaginable in the early 20th Century
Repeatedly re-sampling the data with nonparametric model relaxes some assumptions of classical statistical methods
Re-sampling methods draw heavily on the law of large number and the central limit theorem",548,semantic
485d4e6a-8c12-4694-b5ad-7176c791dd65,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,2,"Types of Resampling Methods
Commonly used re-sampling methods include:
Randomization or Permutation methods: aka exact tests
Have a long history; e.g. Fisher’s exact test (1922)
Practical approximate algorithms for larger data sets in computer era
Cross validation: resample into multiple folds without replacement
Leave n out method
Has origins in the 1950s
Widely used to evaluate machine learning (ML) models
Jackknife: leave one out re-sampling
Leave one out method
Early general purpose resampling method
Has origins in the 1950s
Nonparametric Bootstrap: resample with equivalent size and replacement - our focus here
Published by Prof Brad Efron in 1979",659,semantic
6b22b803-669a-423f-8bd0-8ed10f3a0608,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,3,"Nonparametric vs. Parametric Statistical Model
Many familiar statistic models are parametric, being based on a assumed likelihood
Likelihood models based on a parametric distributions
Parametric models have low variance estimates for statistics
But susceptible to poor choice of likelihood model
Example, least-squares error model uses a Normal likelihood
Parameters which must be estimated
Location and scale in one-dimension
Betas in higher dimensions",453,semantic
8656e4f3-2a8a-4732-a841-db21badd89b9,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,4,"Nonparametric vs. Parametric Statistical Model
Nonparametric model not based on a parametric likelihood
Use an empirical distribution estimated from observations
No likelihood model assumptions
Statistical properties estimated from this empirical distribution
Potentially high variance estimates
Need sufﬁcient sample size
Example, mean and variance estimates
Examples of nonparametric statistical estimators:
Permutation tests
Jackknife estimates
Nonparametric bootstrap",471,semantic
956eef79-5534-4aed-9ce9-14aceb355707,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,5,"General Characteristics of Nonparametric Resampling Methods
General characteristics of nonparametric resampling methods include
Allow computation of statistics from data samples for statistics with continuous derivatives
Repeatedly compute statistics from multiple resamples of dataset
The result converges to the sample distribution of the statistic being computed
Make minimal distributional assumptions, when compared to classical frequentist statistics",456,semantic
54fc2e11-3f54-41ca-a4f6-a8327b9e19c2,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,6,"Pitfalls of Resampling Methods
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the resampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Often higher variance than parametric models
Be suspicious of overly optimistic conﬁdence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
𝑥¯ 𝜇",664,semantic
3b4b2c9b-0a89-40b1-8965-d8a20481023e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,7,"Point Estimates vs. Distribution Estimates
The goal of frequentist statistics is to compute a point estimate and conﬁdence interval
Point estimate is the single most likely value for a statistic
Conﬁdence interval expresses the uncertainty of the point estimate
Parametric conﬁdence interval based on the properties of some assumed probability distribution
Are there alternatives to this classical frequentist approach? Here we focus on bootstrap methods which do not require explicit probability model",502,semantic
d42737ab-3a81-4497-89e2-dbc4b64c38ea,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,8,"Bootstrap Distribution
Rather than computing a point estimate directly, bootstrap methods compute a bootstrap distribution of a statistic
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conﬁdence interval is computed from the bootstrap distribution",680,semantic
9520d4fc-0303-473c-8172-b2ebe03c2ed4,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,9,"Bootstrap Distribution
Rather than repeatedly resample the population, bootstrapping repeatedly resamples an original sample
Resampling to estiamte the bootstrap distribution of a statistic",189,semantic
edad7771-1ade-4735-a9cd-17a6957b68b8,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,10,"Overview of the Bootstrap Algorithm
The bootstrap method follows a simple algorithm. Estimates of the point estimate of a statistic are accumulated by these steps:
1. Randomly Bernoulli sample sample n data with replacement from an original data sample of n values;
The resample is the same size as the original data sample
2.",326,semantic
a70fa7ba-4661-4059-81dd-632d9d399558,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,10,"Re-compute the statistic with each resample
3. Repeat steps 1 and 2 to accumulate the required number of bootstrap samples
4. Accumulated bootstrap values form the bootstrap distribution; An estimate of the sample distribution of the statistic
5. The mean of the computed statistic values is the bootstrap point estimate of the statistic
By law of large numbers, bootstrap point estimate converges
Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 200 bootstrap samples for point estimates
Other authors recommend a larger number (e.g. 1,000-2,000) of resamples given low computer cost",615,semantic
2eec29b9-be2c-4347-8275-e1550351387a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,11,"Overview of the Bootstrap Algorithm
Outline of bootstrap resampling algorithm to compute mean",93,semantic
7d2edcb5-29ff-49b8-afa6-1d093dc543dd,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,12,"Example; One Sample Bootstrap
Use sample of standardized scores of highschool students from UCLA Statistical Consulting
##      female  race  ses  schtyp  prog  read  write  math  science  socst## id                                                                     ## 70        0     4    1       1     1    57     52    41       47     57## 121       1     4    2       1     3    68     59    53       63     61## 86        0     4    3       1     1    44     33    54       58     31## 141       0     4    3       1     3    63     44    47       53     56## 172       0     4    2       1     2    47     52    57       53     61## 113       0     4    2       1     2    44     52    51       63     61## 50        0     3    2       1     1    50     59    42       53     61## 11        0     1    2       1     2    34     46    45       39     36## 84        0     4    2       1     1    63     57    54       58     51## 48        0     3    2       1     2    57     55    52       50     51",1008,semantic
4e1b8f22-4e40-4e2a-80d1-d05018614397,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,13,"Example; One Sample Bootstrap
Histogram of the math scores",58,semantic
212af512-ce86-4779-8130-2317c2e15555,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,14,"Example; One Sample Bootstrap
Function to compute single sample bootstrap estimate of statistic
## Bootstrap point estimate =  52.61
def bootstrap_statistic(x, b, statistic):
    '''    Function Computes b one-sample bootstrap estimates of data x    using statistic function. '''    n_samps = len(x)    boot_vals = []    for _ in range(b):
        ## The heavy work is done here. The statistic is computed         ## using the bootstrap sample of the observations         boot_vals.append(statistic(nr.choice(x, size=n_samps, replace=True)))
    boot_estimate = np.mean(boot_vals)    print('Bootstrap point estimate = {:6.2f}'.format(boot_estimate))    return(boot_estimate, boot_vals)          
bootstrap_mean_estimate, boot_means = bootstrap_statistic(math, 2 0 0 , np.mean)",776,semantic
c479c5e8-2819-4ef1-8e42-044e1c138f90,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,15,"Example; One Sample Bootstrap
Distribution of 200 bootstrap samples of mean estimates
## bootstrap point estimate =  52.66",122,semantic
e6f5cd4c-3d0c-43f2-88e7-069b94533b5e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,16,"Bootstrap Conﬁdence Intervals
Distribution of 2000 bootstrap bootstrap conﬁdence intervals? Use percentile method:
1. Deﬁne conﬁdence level, eg. 95% or α =0.05
2.",162,semantic
11d178a0-94e0-4784-a3c1-2942ad6668bb,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,16,"Order b bootstrap samples, $, by value
3. Lower CI index; 
4. Upper CI index; 
Percentile method is know to be biased
Bias correction methods available
Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 2,000 bootstrap samples to estimate conﬁdence intervals
Other authors recommend a larger number (e.g. 5,000-20,000) of resamples given low computer cost
𝑠𝑖
𝑖=𝑏∗ α/2
𝑖=𝑏∗ (1−α/2)",409,semantic
6b15d141-0309-48e8-b31e-256e952cf173,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,17,"Bootstrap Conﬁdence Intervals
Bootstrap conﬁdence intervals are known to be biased! Often bootstrap CIs are overly optimistic
Bias can be signiﬁcant for asymmetric distributions",177,semantic
14526383-d126-4139-874f-ed6df4b96845,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,18,"Example; One Sample Bootstrap
Bootstrap distribution of 2000 of mean estimates with conﬁdence intervals
## bootstrap point estimate =  52.63
## At alpha = 0.05, lower and upper bootstrap confidence intervals =  51.37    53.95",225,semantic
0c8fd61a-a075-4a18-8286-501a7619c339,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,19,"Two Sample Bootstrap
How can we apply the bootstrap algorithm for two-sample statistics? Example, difference of means of two independently sampled populations
How to generate bootstrap samples? Can we just sample the concatenation of the two samples? No!",254,semantic
ade752ef-5a2a-46ab-b3df-386284c8fef1,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,19,"There is no guarantee of a correct number of resamples for each group
Imbalanced sampling leads to bias
Must independently sample the two groups or populations
Use two independent bootstrap samples to compute statistic
Step one; compute statistic from independent resamples
Step two; compute (another) statistic from the two bootstrap estimates",344,semantic
e59a1b79-a364-48be-8788-f99993bd5c79,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,20,"Two Sample Bootstrap
Example: algorithm to compute difference of means:
1. Independently randomly Bernoulli sample n data with replacement from each original data sample; The number of resamples for eachpopulations is the number of samples for that population
2. Compute the statistic (e.g. mean) for the two resamples
3.",321,semantic
868a21a4-f728-4c24-814d-67ea31cba495,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,20,"Compute the two-sample statistic; e.g. difference of means
4. Repeat steps 1, 2, and 3 to accumulate the required number of bootstrap samples Accumulated bootstrap values form the bootstrap distribution; anestimate of the sample distribution of the statistic
5. The mean of the computed statistic values is the bootstrap point estimate of the statistic; e.g. difference of means
6. Compute CIs from bootstrap distribution",421,semantic
5c8a22fe-ce39-4018-977d-7b84ab803ee3,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,21,"Two Sample Bootstrap
Example; ﬁnd the bootstrap distribution of the difference in math scores between low and middle SES students.",130,semantic
d42160c5-75e9-4ad3-95e3-1198cc94466a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,22,"Example, Two Sample Bootstrap
def two_boot_two_stat(sample_1, sample_2, b, statistic_1, two_samp_statistic):    '''    Function computes two sample and two statistic bootstrap estimate. - sample_1 and sample_2, independent obervation vectors   
    - b, number of bootstrap samples to compute    - statistic 1, statistic applied to the two independent bootstrap samples    - two_sample_statistic, statistic applied to the independent bootstrap statistics
    '''    two_boot_values = []    n_samps_1 = len(sample_1)    n_samps_2 = len(sample_2)
    for _ in range(b):          ## Heavy lisfting is done here. First, the two independent bootstrap estimates        ## are computed from independent bootstrap resamples. Second, the statistic 
        ## of the two bootstrap estimates is computed.",794,semantic
47a8a67a-d2bf-449a-80a8-354fa521f34a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,22,"boot_estimate_1 = statistic_1(nr.choice(sample_1, size=n_samps_1, replace=True))        boot_estimate_2 = statistic_1(nr.choice(sample_2, size=n_samps_2, replace=True))        two_boot_values.append(two_samp_statistic(boot_estimate_1, boot_estimate_2))
    boot_estimate = np.mean(two_boot_values)    print('bootstrap point estimate = {:6.2f}'.format(boot_estimate))    return(boot_estimate, two_boot_values)    
math_low_ses = test_scores.loc[test_scores.loc[:,'ses']==1 ,'math'] math_mid_ses = test_scores.loc[test_scores.loc[:,'ses']==2 ,'math']bootstrap_diff_of_mean, boot_diffs = two_boot_two_stat(math_low_ses, math_mid_ses, 2 0 0 0 , np.mean, lambda x,y: x-y)",666,semantic
d2323728-97fd-4578-87d1-d3e0b34ea14e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,23,"Example, Two Sample Bootstrap
Compute and display the bootstrap distribution of the difference of student scores
## bootstrap point estimate =  -3.06
## At alpha = 0.05, lower and upper bootstrap confidence intervals =  -6.13     0.14",234,semantic
3396f467-9464-46a3-a563-37891dce1b68,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,24,"Summary
Bootstrap estimation is widely useful and requires minimal assumption
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conﬁdence interval is computed from the bootstrap distribution",620,semantic
f7f32ad2-d537-4028-91ce-4f777b0aebec,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,25,"Summary
There are several variations of the basi nonparametric bootstrap algorithm
One sample bootstrap
Inference on single populations
Two sample bootstrap
Inference on different populations
Special cases
Correlation coefﬁcients - part of your assignment",255,semantic
c320e891-2243-420f-a2b0-15882d819a9a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,26,"Summary
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the resampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Be suspicious of overly optimistic conﬁdence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
𝑥¯ 𝜇",596,semantic
9331e4d7-acff-4a25-b946-47a37b453c42,Perception for Scientific Visualization.pdf,CSCI_83,0,"Perception for Scientiﬁc Visualization
Steve Elston
08/28/2022",62,semantic
35cc1956-0eaf-4ba5-ab64-7a09e47e637f,Perception for Scientific Visualization.pdf,CSCI_83,1,"Why This Course? 21st Century datasets are large and complex
Complexity is often harder to address than size
Complexity makes understanding of relationships in data difﬁcult
Complexity addressed with computer-intensive methods
Our focus is on the big ideas of computer-intensive statistics and data analysis arising in the late 20th and early 21st Centuries",357,semantic
b84c638a-3ee9-4a46-b0e3-7dc60dd2ccc7,Perception for Scientific Visualization.pdf,CSCI_83,2,"Why This Course? Data science is the science of understanding data
Complexity makes understanding difﬁcult
Statistics is the science of making principled inferences from data
Inference leads to understanding
Inference is becoming harder with large complex data sets
Doing rigorous data science requires understanding statistics
Statistical practice has advanced signiﬁcantly to address large complex data sets
Statistical practice now dominated by computer-intensive methods",474,semantic
3364c971-61c1-4899-b1b4-30297314dd3e,Perception for Scientific Visualization.pdf,CSCI_83,3,"What We’ll Cover
Our focus is on the big ideas of computer-intensive statistics and data analysis arising in the late 20th and early 21st Centuries
Exploratory data analysis (EDA) to understand relationships in big complex data sets
Foundations of algorithms used throughout statistics and machine learning such as maximum likelihood
Computer intensive resampling methods for building models and inference, Bootstrapping and MCMC Bayes
Large scale hypothesis testing and sparse models for complex and high-dimensional data sets
Bayesian hierarchical models for complex relationships
Modern time series and forecasting algorithms for data with serial correlation
Robust statistics to deal with data violating model assumptions",725,semantic
36c93ab6-a4ee-4846-8e22-e41b780930c9,Perception for Scientific Visualization.pdf,CSCI_83,4,"Course Objectives
This fast-moving survey course helps build your toolbox for modeling complex data
Broad introduction to the theoretical and methodological basis of data science
Conditional probability theory
Sampling theory
Statistical estimation theory - classical and resampling based
Understand models for complex datasets
Understanding data relationships and inference
How these methods work and when to used them
How conﬁdent should we be in our inferences? Moving beyond a cookbook or blog post approach to data science",527,semantic
dbb3d5d1-3412-42d9-a4eb-8d7f98745ff7,Perception for Scientific Visualization.pdf,CSCI_83,5,"Instructor: Steve Elston
Data science consultant with several decades of experience
Instructor for Harvard since 2016
Lead team that commercialized Bell Labs S, now open source R
Company co-founder and held executive positions in several industries
Creator of multiple edX courses, author of O’Reily books and articles
Holder of 5 issued patents
BS, physics and math (minor), University of New Mexico
MS and PhD, geophysics, Princeton University – NSF, John von Neuman Supercomputing Fellow",490,semantic
ee79d1fb-f5b7-4b45-8dab-91bb73e3dc57,Perception for Scientific Visualization.pdf,CSCI_83,6,"Teaching Assistant: Moustafa Saleh
A principal data scientist at Oracle Cloud
Received PhD in computer science from University of Texas at San Antonio
Worked previously at Microsoft’s Advanced Threats Protection team developing ML solutions for malware detection
Research mainly focused on applying machine learning solutions to cyber-security challenges",354,semantic
867fbd53-6eb7-4e40-a9ba-50ca066692f1,Perception for Scientific Visualization.pdf,CSCI_83,7,Teaching Assistant: Tatyana Boland,34,semantic
098c4ba1-d9fb-4624-a512-d54838cd7ec7,Perception for Scientific Visualization.pdf,CSCI_83,8,"Grading: Undergraduate
Activity Grade weight
Participation (graded discussions) 10%
Assignments 90%
Hands-on assignments tie theory to practice applied to data examples
Most of us only recall methods we actaully use
Lectures provide introduction only
Expect to work out some details
Do not hesitate to ask for help on concepts or coding!!",338,semantic
8cb2b4c2-2e11-472f-b77e-846b24e36074,Perception for Scientific Visualization.pdf,CSCI_83,9,"Grading: Graduate
Activity Grade weight
Participation (graded discussions) 10%
Assignments 60%
Independent project proposal 5%
Independent project report 25%
Hands-on assignments tie theory to practice applied to data examples
Most of us only recall methods we actaully use
Lectures provide introduction only
Expect to work out some details
Do not hesitate to ask for help on concepts or coding!! You will execute an end-to-end project on an appropriate problem of you choice",475,semantic
ed8cc1a0-a03a-4f22-99a3-17a2d5c334e3,Perception for Scientific Visualization.pdf,CSCI_83,10,"Grading: Graduate
Independent Graduate Project:
An end-to-end project you will execute independently
Pay careful attention to grading rubric in Canvas
Can be great addition to your data science project portfolio
Pick a problem of particular interest to you! Plan to spend about 80 hours on your analysis and report
Sufﬁcient data must be available
Must use analytical methods within the scope of this course - e.g. no advanced ML or deep learning
Start thinking about your project soon - don’t put it off
Resources to help you get started are under the Resources tab in Ed
List of possible data sources
Example project proposals and reports",640,semantic
c52aaca0-eb00-4136-a2d5-2ff1b8f43231,Perception for Scientific Visualization.pdf,CSCI_83,11,"Late Assignment Policy
Timely feedback is an important part of the learning process
To allow the timely release of solutions for assignments this course applied a late assignment policy:
Up to one day late - no penalty
Up to 6 days late - less 20%
More than 6 days late - no credit
Advice: start assignments and your project as soon as you can so you have time to address problems and ask questions! Note: No extension is possible for Graduate Independent Projects!",465,semantic
847758f5-341b-4ec3-9507-3a177e19cdb0,Perception for Scientific Visualization.pdf,CSCI_83,12,"Class Schedule
Class meeting Tuesdays, 6:00 pm US Eastern Time:
Focus on theory to understand concepts
Limited time for code discussions
Section meetings on Wednesday or Thursday, 6:00 pm US Eastern Time:
Focus on answer student questions - your questions! Discus code and coding problems
Background and supplementary material as needed
Poll to ﬁnd best day for class
All class meetings are recorded for on-demand viewing",421,semantic
a4112b43-f1c2-4453-8f69-63c407c276a9,Perception for Scientific Visualization.pdf,CSCI_83,13,"Communications
Communicating with your instructors and other students is a signiﬁcant aspect of participation in this course! Ask questions about the course material, homework, etc. Ask questions in the public forum so others can answer and gain from the discussion: if you have a question others do as well!",308,semantic
bb64a473-0599-48fb-923c-e7c50190a63a,Perception for Scientific Visualization.pdf,CSCI_83,13,"Answer other students’ questions
Comment on weekly graded discussion topics
Ed is the primary communications method- Generally use public posts - okay to include code snippets- Option to ask instructors private questions
Ask for the help you need!",247,semantic
caeff8c1-8a32-4a00-94b4-27386b456719,Perception for Scientific Visualization.pdf,CSCI_83,14,"Communications
For private matters, you can directly communicate with the instructional team:- Grades- Absences- Etc
Steve Elston, Instructor, stephen.elston gmail.comMoustafa Saleh, TA, msaleh83 gmail.comTatyana Boland, TA, tatyanaboland gmail.com
Ofﬁce hours: If you need individual assistance, please ask to schedule ofﬁce hours. Don’t be shy! Communications by Canvas may be signiﬁcantly delayed!",400,semantic
adc6d4ee-0d75-4208-89bf-b9b9ce098d50,Perception for Scientific Visualization.pdf,CSCI_83,15,"Poll
Then, back to the lecture",30,semantic
c1105274-7fcc-4c98-8e3d-7c285f1dbccc,Perception for Scientific Visualization.pdf,CSCI_83,16,"Why Exploration and Visualization? Exploratory data analysis (EDA) tools are essential to good data science
Isn’t the goal of data science to build machine learning models? Not always!",184,semantic
8afb6ee4-2a57-4978-989a-cbbae1727417,Perception for Scientific Visualization.pdf,CSCI_83,16,"Often we need to understand relationships found in data
Explain scientiﬁc or behavioral relationships
Determine if a relationship is important
Our goal is to gain deep understanding for complex problem
EDA methods
Statistical inference",235,semantic
72cbd461-70a4-425e-bc45-12561b198391,Perception for Scientific Visualization.pdf,CSCI_83,17,"Why Exploration and Visualization? Exploratory data analysis (EDA) tools are essential to good data science
Why not just start building machine learning models? Understanding relationships in data saves missteps and unexplained poor model performance
Which variables are actually important? How do these variables behave?",321,semantic
f9d6a101-fe56-4ffe-b3e5-c1ca854a77c8,Perception for Scientific Visualization.pdf,CSCI_83,17,"Are there errors and outliers in the data? How good is a model ﬁt? Communications is an important component of data science
Analytic results are only useful if they are understood and trusted
Graphical presentation greatly assists understanding by less technical colleagues",273,semantic
6d900b96-b0da-4b93-9786-2cdc15cd57fb,Perception for Scientific Visualization.pdf,CSCI_83,18,"Why is Perception Important? Goal: Communicate information visually
Visualization technique maximize the information a viewer perceives
Gain insights when exploring relationships in data
Communicate insights to others
limits of human perception are a signiﬁcant factor in understanding complex relationships
Can apply results of the considerable research on human perceptions for data visualization",398,semantic
7ea6c9ed-4cef-4a6e-850a-ed6b128a005e,Perception for Scientific Visualization.pdf,CSCI_83,19,"Use Aesthetics to Improve Perception
Use aesthetics to improve perception
We take a very broad view of the term ‘aesthetic’ here
A plot aesthetics is any property of a visualization which highlight aspects of the data relationships
Aesthetics are used to project additional dimensions of complex data
Plots generally restricted to 2-dimensional surface
Must projet multiple dimensions of complex data on 2-d surface",415,semantic
17ba13ba-7b23-4371-9455-e853982eb9c3,Perception for Scientific Visualization.pdf,CSCI_83,20,"Organization of Plot Aesthetics
We can organize aesthetics by their effectiveness:
1. Easy to perceive plot aesthetics: help most people gain understanding of data relationships
2. Aesthetics with moderate perceptive power: useful properties to project data relationships when used sparingly
3.",294,semantic
42d07b1d-1d27-4e50-8965-bc0b4aec5255,Perception for Scientific Visualization.pdf,CSCI_83,20,Aesthetics with limited perceptive power: useful within strict limits,69,semantic
c920a371-af49-442f-b941-c549c2786e2e,Perception for Scientific Visualization.pdf,CSCI_83,21,"Properties of Common Aesthetics
Property or AestheticPerceptionData Types
Aspect ratio Good Numeric
Regression lines Good Numeric plus categorical
Marker position Good Numeric
Bar length Good Counts, numeric
Sequential color palette Moderate Numeric, ordered categorical
Marker size Moderate Numeric, ordered categorical
Line types Limited Categorical
Qualitative color paletteLimited Categorical
Marker shape Limited Categorical
Area Limited Numeric or categorical
Angle Limited Numeric",487,semantic
a095bfc5-ff29-4b59-9c29-2b36c02f6f54,Perception for Scientific Visualization.pdf,CSCI_83,22,"Aspect Ratio
Aspect ratio has a signiﬁcant inﬂuence on how a viewer perceives a chart
Correct aspect ratio can help highlight important relationships in complex data sets
But, wrong aspect ratio can hide or mislead! We express aspect ratio as follows:
Banking angle is key to understanding how the aspect ratio affects perception
𝑎𝑠𝑝𝑒𝑐𝑡 𝑟𝑎𝑡𝑖𝑜=  :1𝑤𝑖𝑑𝑡ℎ ℎ 𝑒𝑖𝑔ℎ 𝑡",361,semantic
796d7630-d579-492a-b360-4d3c91c1bb86,Perception for Scientific Visualization.pdf,CSCI_83,23,"Example of Changing Aspect Ratio
Longest scientiﬁc time series is the sunspot count:
##      YEAR  SUNACTIVITY## 0  1700.0          5.0## 1  1701.0         11.0## 2  1702.0         16.0## 3  1703.0         23.0## 4  1704.0         36.0",235,semantic
8af89d51-7f6c-4931-b6b4-fac080683187,Perception for Scientific Visualization.pdf,CSCI_83,24,"Example of Changing Aspect Ratio
Example uses data from 1700 to 1980
Can you perceive the asymmetry in these sunspot cycles?",124,semantic
09553455-3e4d-4cf2-8a34-94f1c0030337,Perception for Scientific Visualization.pdf,CSCI_83,25,"Example of Changing Aspect Ratio
Notice how changing aspect ratio change perception of the asymmetry?",101,semantic
4a9069d0-7d04-441f-989f-a5808a24f129,Perception for Scientific Visualization.pdf,CSCI_83,26,"Sequential and Divergent Color Palettes
Use of color as an aesthetic in visualization is a complicated subject. color is often used, also often abused
A qualitative palette is a palette of individual colors for categorical values
Sequential palettes and divergent palettes are a sequence of colors
Numeric variables
Ordered categorical variable",344,semantic
8496a93c-813a-4bdc-9f25-f7df37f7bdcd,Perception for Scientific Visualization.pdf,CSCI_83,27,"Auto Weight by Sequential Color Palette
fig, ax = plt.subplots(figsize=(8 ,1 0 ))fig.subplots_adjust(top=0 . 8 , bottom=0 . 2 )ax = sns.scatterplot(x='city_mpg', y='curb_weight', data=auto_price,                      hue = 'price', palette = 'magma', ax=ax)
_=ax.set_title('City MPG vs.",286,semantic
ce39b880-4c7f-4ce5-9282-84ee618ad669,Perception for Scientific Visualization.pdf,CSCI_83,27,"Weight \nPrice by color') #, fontsize=12)_=ax.set_xlabel('City MPG') # , fontsize=12)_=ax.set_ylabel('Vehicle Weight') #, fontsize=12) 
plt.show()",146,semantic
41ae8161-9d85-46ce-bf50-2a80f2b37d02,Perception for Scientific Visualization.pdf,CSCI_83,28,Auto Weight by Sequential Color Palette,39,semantic
9261b35f-4731-48fe-a248-6d88236057a5,Perception for Scientific Visualization.pdf,CSCI_83,29,"Limits of color
Regardless of the approach there are some signiﬁcant limitations
A signiﬁcant number of people are color blind. Red-green color blindness is most common
Even the best sequential or divergent palettes show only relative value of numeric variables
Perception of exact numeric values is difﬁcult, except in special cases
Cannot perceive large number of colors for categories",387,semantic
63cf220d-3acf-4bc6-99c4-ae414b500fb9,Perception for Scientific Visualization.pdf,CSCI_83,30,"Marker Size
Marker size is moderately effective aesthetic
Used properly, marker size can highlight important realationships in complex data sets
Numeric values
Ordinal variables
Viewers can generally perceive relative differences, but not actual values
Small size differences are not preceptable
Only relative relationship in numeric variables
Limited steps of categorical variables",382,semantic
9af51469-df7e-4f55-9a50-ecd417a43e72,Perception for Scientific Visualization.pdf,CSCI_83,31,"Engine Size by Marker Size and Price by Sequential Color Palette
fig, ax = plt.subplots(figsize=(8 ,7 ))ax = sns.scatterplot(x='city_mpg', y='curb_weight', data=auto_price,                     hue = 'price', palette = 'magma',                     size = 'engine_size', sizes = (5 0 . 0 , 4 0 0 . 0 ),
                     ax=ax)_=ax.set_title('City MPG vs.",356,semantic
c83b7ca7-481d-48a4-a44d-1277b36bbc2f,Perception for Scientific Visualization.pdf,CSCI_83,31,"Weight \nPrice by color, engine size by size') #, fontsize=18)_=ax.set_xlabel('City MPG') #, fontsize=16)
_=ax.set_ylabel('Vehicle Weight')# , fontsize=16) plt.show()",166,semantic
dfd6457b-1216-44c7-a981-affc403edd1e,Perception for Scientific Visualization.pdf,CSCI_83,32,Engine Size by Marker Size and Price by Sequential Color Palette,64,semantic
9c9cd3b2-58a1-4820-a6fe-0bdf537f6b00,Perception for Scientific Visualization.pdf,CSCI_83,33,"Line Plots and Line Type
Line plots connect discrete, ordered, data points by a line
Can use different colors and line pattern types to differentiate categories
Only useful for a limited number of lines on one graph
Too many similar colors and line patterns on one plot leads to viewer confusion and poor perception",315,semantic
26316129-12cb-4181-93c3-4531f9c48ea6,Perception for Scientific Visualization.pdf,CSCI_83,34,Limits of Line Type,19,semantic
ddb1ba87-e669-49fb-830a-c2f2c41a3663,Perception for Scientific Visualization.pdf,CSCI_83,35,"Marker Shape
Marker shape is useful for displaying categorical relationships
This aesthetic is only useful when two conditions are met:
1. The number of categories is small
2. Distinctive shape are chosen for the markers
Human perception limits the number of shapes humans can perceive well",290,semantic
8c20f833-92bd-4af1-a98d-e0df056e9cc6,Perception for Scientific Visualization.pdf,CSCI_83,36,Aspiration by Marker Shape,26,semantic
876d5741-4c53-485c-a42d-2921be189ca2,Perception for Scientific Visualization.pdf,CSCI_83,37,"Regression Lines
Regression lines draw viewers attention
Typically use a nonlinear regression line
Polynomial
Splines - piece wise model
Lowess - local nonlinear regression
Bootstrap conﬁdence intervals show range of probable trends
More about bootstrap resampling later
Ideally want strait line relationship
Nonlinear relationships often arise from non-Normal distributions
Linear relationship is more intuitive",412,semantic
ea104da0-f50b-48ce-9f40-2fb435ccb29b,Perception for Scientific Visualization.pdf,CSCI_83,38,"Regression Line and Tramsformation Example
Regression lines highlight the trends for gas and diesel engines
Second order polynomial ﬁt",134,semantic
9db1ab0d-717b-459f-adf9-f32c35a3169e,Perception for Scientific Visualization.pdf,CSCI_83,39,"Regression Line and Tramsformation Example
What are the distributions of these variables? Notice the right skew of these distributions",134,semantic
7b28939d-039c-4e11-87f9-1cfaabc426d0,Perception for Scientific Visualization.pdf,CSCI_83,40,"Regression Line and Tramsformation Example
Can transform distribution of variables
Want distribution closer to Normal
Many possible transformations
Logarithmic: Often good choice for variables with values 
Square and square root: Good choice for many physical systems
Power transformation: Find best ﬁt transformation
Fit to paramteric distribution: Test if variable follows a known distribution
>0",398,semantic
d07b6236-41fb-4de2-b346-7160a41a87ec,Perception for Scientific Visualization.pdf,CSCI_83,41,"Regression Line and Tramsformation Example
Can transform distribution of variables
Multiple algorithms have been developed
Box-Cox, the ﬁrst and still widely used for values 
Yeo-Johnson, works for values 
The Box-Cox transform ﬁts a value for  that minimizes error with respect to a Normal distribution
See the Scikit-Learn Users’ Guide for more details
>0
≤0
𝜆
=𝑥( 𝜆) 𝑖
⎧
⎩
⎨⎪
⎪
−1𝑥𝜆𝑖𝜆
ln()𝑥𝑖
if 𝜆≠0,
if 𝜆=0,",410,semantic
424d9602-d285-4eea-9721-e54a3e5539c8,Perception for Scientific Visualization.pdf,CSCI_83,42,"Regression Line and Tramsformation Example
Transform the distributions
pt = PowerTransformer(method='box-cox')
pt.fit(np.array(auto_price['price']).reshape(-1 , 1 ))auto_price['power_price'] = pt.transform(np.array(auto_price['price']).reshape(-1 , 1 ))
fig, ax = plt.subplots(1 ,2 ,figsize=(1 2 , 4 ))ax[0 ]=sns.kdeplot(x = 'power_price',                       hue = 'fuel_type',                      data=auto_price,
                      ax=ax[1 ]);ax[0 ].set_title('Density of power transformed price');ax[0 ].set_xlabel('Transformed price');
ax[1 ]=sns.kdeplot(x = 'curb_weight',                       hue = 'fuel_type',                      data=auto_price,                      ax=ax[1 ]);
ax[1 ].set(xscale=""log"");ax[1 ].set_title('Density of log vehicle weight');ax[1 ].set_xlabel('Log vehicle weight');plt.show()",822,semantic
a78ba6f8-4666-4d3f-8361-e6859e37692c,Perception for Scientific Visualization.pdf,CSCI_83,43,"Regression Line and Tramsformation Example
Transform the distributions
Distribution of logarithm of the curb weight",115,semantic
7c508751-c280-49d1-9d60-e44edcaf71dc,Perception for Scientific Visualization.pdf,CSCI_83,44,"Regression Line and Tramsformation Example
Transform the distributions
Distribution of power transformed price, 
Transformed distribution has minimal skew! 𝜆=−0.64",163,semantic
5b87cd58-b40c-4cc6-b152-66b2950429a4,Perception for Scientific Visualization.pdf,CSCI_83,45,Regression Line and Tramsformation Example,42,semantic
32c8f210-0415-4eb9-8a38-1ee01c5c1746,Perception for Scientific Visualization.pdf,CSCI_83,46,Second order polynomial ﬁt,26,semantic
6d2165d0-5b6b-4d93-8036-ba076894e808,Perception for Scientific Visualization.pdf,CSCI_83,47,"Summary
We have explored these key points- Visualization is a powerful EDA method- Understand relationships in data- Communicate data scince insights
Proper use of plot aesthetics enable projection of multiple dimensions of complex data onto the 2-dimensional plot surface. All plot aesthetics have limitations which must be understood to use them effectively
The effectiveness of a plot aesthetic varies with the type and the application
Regression lines help to focus viewer on trends
Transformations to linear relationships can be informative",545,semantic
31ffaf6f-ad10-4f7e-b491-cdfe37dcd0c9,Visualization of Large Complex Data.pdf,CSCI_83,0,"Visualization of Large Complex Data
Steve Elston
09/11/2023",59,semantic
328fc2ec-4842-425f-9507-f9dbfc235a21,Visualization of Large Complex Data.pdf,CSCI_83,1,"Review: Why is Perception Important? Goal: Communicate information visually
Visualization technique maximize the information a viewer perceives
Limits of human perception are a signiﬁcant factor in understanding complex relationships
Apply results of research on human perceptions for data visualization",303,semantic
3c72e656-7dd1-4a36-a018-f956aec7620a,Visualization of Large Complex Data.pdf,CSCI_83,2,"Use Aesthetics to Improve Perception
Use aesthetics to improve perception
Take a very broad view of the term ‘aesthetic’
A plot aesthetics is any property of a visualization which highlight aspects of the data relationships
Aesthetics are used to project additional dimensions of complex data onto the 2-dimensional plot surface",328,semantic
5aa4e915-8cbc-4539-8daf-999c18281811,Visualization of Large Complex Data.pdf,CSCI_83,3,"Regression Lines
Regression lines draw viewers attention
Typically use a nonlinear regression line
Polynomial
Splines - piece wise model
Lowess - local nonlinear regression
Bootstrap conﬁdence intervals show range of probable trends
More or bootstrap resampling later
Ideally want strait line relationship
Nonlinear relationships often arise from non-Normal distributions
Linear relationship is more intuitive",409,semantic
3c998444-7357-4967-ba5e-db4692189892,Visualization of Large Complex Data.pdf,CSCI_83,4,"Properties of Common Aesthetics
Property or AestheticPerceptionData Types
Aspect ratio Good Numeric
Regression lines Good Numeric plus categorical
Marker position Good Numeric
Bar length Good Counts, numeric
Sequential color palette Moderate Numeric, ordered categorical
Marker size Moderate Numeric, ordered categorical
Line types Limited Categorical
Qualitative color paletteLimited Categorical
Marker shape Limited Categorical
Area Limited Numeric or categorical
Angle Limited Numeric",487,semantic
b78dc44e-0118-4712-8093-ad8cfc6377e3,Visualization of Large Complex Data.pdf,CSCI_83,5,"Visualizing Large Complex Data is Difﬁcult
Problem: Modern data sets are growing in size and complexity
Goal: Understand key relationships in large complex data sets
Difﬁculty: Large data volume
Modern computational systems have massive capacity
Example: Use map-reduce algorithms on cloud clusters
Difﬁculty: Large numbers of variables
Huge number of variables with many potential relationships
This is the hard part! Note: we will address use of dimensionality reduction techniques in another lesson, time permitting",518,semantic
efc75cf6-62d0-430b-873a-79eb9ccd371e,Visualization of Large Complex Data.pdf,CSCI_83,6,"Limitation of Scientiﬁc Graphics
All scientiﬁc graphics are limited to a 2-dimensional projection
But, complex data sets have a great many dimensions
We need methods to project large complex data onto 2-dimensions
Generally, multiple views are required to understand complex data sets
Don’t expect one view to show all important relationships
Develop understanding over many views
Try many views, don’t expect most to be very useful",432,semantic
4295a2c6-183d-4453-b3fe-b1891bd19400,Visualization of Large Complex Data.pdf,CSCI_83,7,"Approaches to display of complex data relationships
Generally combine multiple methods to effectively display complex data
Use plots that inherently scale
Avoid over-plotting to ensure plot is understandable
Choose plot types that do not exhibit overplotting
Often a creative case speciﬁc plot type is best
Use multi-axis plots
Scatter plot matricies
Facet plots
Filter cases using cognositics",393,semantic
1f01f49c-d423-4323-b01c-898579939cf5,Visualization of Large Complex Data.pdf,CSCI_83,8,"Scalable Chart Types
Some chart types are inherently scalable. Bar plots: Counts can be computed; e.g. use map-reduce
Histograms: Data is binned in parallel
Box plots: Finding the quartiles is a scalable counting process
KDE and violin plots: Similarly to the box plot, using kernel density estimation",301,semantic
2cd44d17-db71-4e43-b3b5-60fbd3747542,Visualization of Large Complex Data.pdf,CSCI_83,9,"Over-plotting
Over-plotting occurs in plots when the markers lie one on another. Common, even in relatively small data sets
Scatter plots can look like a blob and be completely uninterpretable
Over-plotting is a signiﬁcant problem in EDA and presentation graphics",263,semantic
35930ea8-8886-4416-82ef-7d8a01304e06,Visualization of Large Complex Data.pdf,CSCI_83,10,"Dealing with Over-plotting
What can we do about over-plotting? Marker transparency: so one can see markers underneath; useful in cases with minimal overlap of markers
Marker size: smaller marker size reduces over-plotting within limits
Adding jitter: adding a bit of random jitter to variables with limited number of values
Random down-sampling: for very large data sets, you may only need a representative sample to understand key data relationships",450,semantic
a88c8fac-0ed9-4ab0-9d44-ea7e7da0e3b4,Visualization of Large Complex Data.pdf,CSCI_83,11,Example of Overplotting,23,semantic
4b022ef0-9125-4cd0-87e1-a579a099071a,Visualization of Large Complex Data.pdf,CSCI_83,13,"Use Transparency, Marker Size, Downsampling
Down sample to 20%, alpha = 0.1, size = 2",85,semantic
33845c81-0259-4bf9-9e9b-9b9180a3ebfc,Visualization of Large Complex Data.pdf,CSCI_83,15,"Other Methods to Display Large Data Sets
Alternatives to avoid over-plotting for truly large data sets
Hex bin plots: the 2-dimensional equivalent of the histogram
Frequency of values is tabulated into 2-dimensional hexagonal bins
Displayed using a sequential color palette
2-d kernel density estimation plots: natural extension of the 1-dimensional KDE plot
Good for moderately large data
Heat map: values of one variable against another
Categorical (count) or continuous variables
Carefully choose color pallet, sequential or divergent
Mosaic plots: display multidimensional count (categorical) data
Uses tile size and color to project multiple dimensions
2-d equivalent of a multivariate bar chart
Dimensionality reduction: we will discuss this later in the course",767,semantic
5980d2fe-1977-4952-afd2-bb732cc27392,Visualization of Large Complex Data.pdf,CSCI_83,16,"Hexbin Plot
Example: Density of sale price by time",50,semantic
7de2e9b0-7d67-4858-b6ec-c5276587102e,Visualization of Large Complex Data.pdf,CSCI_83,18,"Countour Plot
Example: Contour plot of 2-D KDE of sale price vs. time",69,semantic
aaf6b3c0-80cb-4cfd-930d-70317db6be9d,Visualization of Large Complex Data.pdf,CSCI_83,19,"Heat Map
Example: Airline passenger counts by month and year displayed by squential heat map
Heat map of",104,semantic
1b2bf1f9-11fa-4e87-b55a-c0c8868d70f7,Visualization of Large Complex Data.pdf,CSCI_83,20,"Mosaic Plots
How can we display multidimensional count (categorical) data at scale? Mosaic plots displays the relative proportion of counts of a contingency table
Plot area is divided and fully ﬁlled by a set of tiles
The larger the count, the larger tile area",260,semantic
5a1435df-4c52-40a3-b078-9babd7474e79,Visualization of Large Complex Data.pdf,CSCI_83,21,"Mosaic Plots
Example: How do counts change with working vs. non-working day, weather and year? Mosaic plot displays tiles with size proportional to count
Counts conditioned on variables, working day, weather, year
bike_share_df = pd.read_csv('../data/BikeSharing.csv')
## Add month column with namesbike_share_df.loc[:,'month'] = [calendar.month_abbr[i] for i in bike_share_df.loc[:,'mnth']]
## Add column with human readable weather conditionsweather = {1 :'Clear', 2 :'Mist', 3 :'Light precipitation', 4 :'Heavy precipitation'}
bike_share_df.loc[:,'Weather'] = [weather[i] for i in bike_share_df.loc[:,'weathersit']]
season = {1 :'winter', 2 :'spring', 3 :'summer', 4 :'fall'}bike_share_df.loc[:,'Season'] = [season[i] for i in bike_share_df.loc[:,'season']]
year = {0 :'2011', 1 :'2012'}bike_share_df.loc[:,'year'] = [year[i] for i in bike_share_df.loc[:,'yr']]working = {0 :'No_Work', 1 :'Working'}
bike_share_df.loc[:,'WorkingDay'] = [working[i] for i in bike_share_df.loc[:,'workingday']]
categorical_cols = ['year','Season', 'WorkingDay', 'Weather']
fig, ax = plt.subplots(figsize=(2 5 , 2 0 ))plt.rcParams.update({'font.size': 1 8 })
_=mosaicplot.mosaic(bike_share_df.loc[:,categorical_cols],                     index=list(categorical_cols),                     title = 'Counts of weather conditions',                    ax=ax)
plt.show()",1347,semantic
655310da-c921-4d3b-8ec7-0b57a40442ca,Visualization of Large Complex Data.pdf,CSCI_83,22,Mosaic Plots,12,semantic
bdce6979-26ee-4de8-9bf2-241896b03500,Visualization of Large Complex Data.pdf,CSCI_83,23,Facet plot of wind by month,27,semantic
98490140-55d7-4b9b-a1fa-0558755074cc,Visualization of Large Complex Data.pdf,CSCI_83,24,"Other Methods to Display Large Data Sets
Sometimes a creative alternative is best
Often situation speciﬁc; many possibilities
Finding a good one can require signiﬁcant creativity! Example, choropleth for multi-dimensional geographic data
Example, time series of box plots",271,semantic
a25648e6-4f5d-4ce6-8ed4-e0451ace1547,Visualization of Large Complex Data.pdf,CSCI_83,25,"Time Series of Box Plots
Example: Time ordered box plots of quarterly sales price",81,semantic
f248fc1a-e392-4f26-b642-d29cab71d455,Visualization of Large Complex Data.pdf,CSCI_83,26,"Displays for Complex Data
How can we understand the relationships in complex high-dimensional data with many variables? Must confront limitations for 2-dimensional projection inherent in computer graphics
Need methods to scale to higher dimensions
Apply combination of arrays of plots and ﬁltering
Aesthetics: used to increase the number of projected dimensions",361,semantic
687e3add-c7e0-4002-ac86-dbfe36547edd,Visualization of Large Complex Data.pdf,CSCI_83,27,"Displays for Complex Data
How can we understand the relationships in complex high-dimensional data with many variables? Arrays of plots: subsets show relationships in a complex data set
Pairwise scatter plots: matrix of all pairwise combinations of variables
pairwise scatter plots can be created for subsets of large and complex data sets. Faceting: uses values of categorical or numeric variables to plot subsets
Subsets are displayed on an array of plots
Typically use axes on same scale to ensure correct perception of relationships
Faceting goes by several other monikers, conditional plotting, method of small multiples, lattice plotting
Cognostics: sort large number of variables to ﬁnd important relationships",717,semantic
ae3f7eb7-15dc-4fb3-894b-84dd203eda90,Visualization of Large Complex Data.pdf,CSCI_83,28,"Arrays of Plots
Display multiple plot views in an array or grid
Create an array of plots which project multiple related views of data relationships
Organize axes to give multidimensional view
Example, scatterplot with kde plots on the margins
Supported by Seaborn jointplot",273,semantic
4074e8e7-93fe-45e4-a38a-de1cf17fd810,Visualization of Large Complex Data.pdf,CSCI_83,29,"Scatter Plot Matrix
Scatter plot matrix used to investigate relationships between a number of variables
Key idea: Display a scatter plots of each variable versus all other variables
Primarily EDA tool
Conveys lots of information - requires study! Each pairwise relationship is displayed twice
Two possible orientations
Or two different plot types
Can place histograms and KDE plots on diagonal",393,semantic
9aed1aff-74cc-4bcb-8cb1-f5ba5e7976bd,Visualization of Large Complex Data.pdf,CSCI_83,30,"Scatter Plot Matrix
Scatter plot matrix create two dimensional array of plots of variable pairs- Upper triangular plots: Scatter plots with regression lines- Lower triangular plots: Hexbin plots of density- Diagonal plots: Histograms of variables
g = sns.PairGrid(diabetes.drop('Sex', axis=1 ), hue='sex_categorical', palette=""Set2"", height=1 . 5 )_=g.map_upper(sns.regplot, order=1 , truncate=True, scatter_kws={'s':0 . 5 })_=g.map_lower(plt.hexbin, alpha=0 .",460,semantic
bafec02d-72d3-4dfb-af53-3c87830e60f9,Visualization of Large Complex Data.pdf,CSCI_83,30,"5 , cmap='Blues', gridsize=1 5 , linewi dths=0 )
_=g.map_diag(plt.hist, histtype=""step"",linewidth=1 )plt.show()",111,semantic
12383e44-4381-4060-8a63-3b91e430e86f,Visualization of Large Complex Data.pdf,CSCI_83,31,"Scatter Plot Matrix
Scatterplot matrix with different plot types",64,semantic
bea251e9-2d5a-47f2-af6d-c5ceaa856ee4,Visualization of Large Complex Data.pdf,CSCI_83,32,"Facet Plots
Facet plots revolutionized statistical graphics starting about 30 years ago
Facet plots extend the number of dimensions projected onto 2-d plot surface
Key idea: Create array of plots of subsets of the data
Create subsets using a group-by operation on other variables
Lay out grid of plots on axes with same scale
Organize by row and column grouping variables
Display same plot type for each group
Can add speciﬁc aesthetics, etc.",442,semantic
19a43b39-511b-42f8-842b-12509a13b03e,Visualization of Large Complex Data.pdf,CSCI_83,33,"Facet Plots
Like many good ideas facet plotting was invented several times
Multiple contemporaneous inventors and names
Tufte, 1990, introduced method of small multiples
Cleveland, 1992, introduced trellis plotting
Also known as conditioned plots
Nowdays, most packages use term facet plot",289,semantic
5a9b7332-978d-446f-9487-d91417848777,Visualization of Large Complex Data.pdf,CSCI_83,34,"Facet Plot with wind speed by Month
Facet plot projected on a grid
Grid deﬁned by variable values for row and column
Can tile or shingle numeric variables for grid
Tile categorical variables
Overlapping shingles of numeric variables
Can create variables by combinations of categories from other variables
Example: Histogram of wind speed conditioned on month
Plot grid has month in columns
Histogram of windspeed is created for each monthly subset
g = sns.FacetGrid(bike_share_df, col='month', col_order=calendar.month_abbr[1 :] ,col_wrap = 4 , height=5 )g = g.map(plt.hist, ""windspeed"", bins=2 0 , color=""b"", alpha=0 . 5 )",623,semantic
c14a13b3-c819-47d0-a42f-8465c4281128,Visualization of Large Complex Data.pdf,CSCI_83,35,"Facet Plot with wind speed by Month
Facet plot of wind by month",63,semantic
85ac8619-1632-4dc2-85bf-46b9a91da634,Visualization of Large Complex Data.pdf,CSCI_83,36,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Grid deﬁned by season on the rows and weather in the columns
Scatter plot of count of riders by hour of the data for each group
g = sns.FacetGrid(bike_share_df, col=""Weather"", col_order=weather.values(), row=""Season"", height=2 , aspect=2 )g = g.map(sns.scatterplot, ""hr"", ""cnt"", s=3 , alpha=0 . 2 )
for ax in g.axes.flat:    ax.set_title(ax.get_title(), fontsize=1 0 )",491,semantic
f3f66f9d-a5b7-4d72-9725-fcf0c2c78e0e,Visualization of Large Complex Data.pdf,CSCI_83,37,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Facet plot of count by season and weather",164,semantic
a184840c-e900-465f-9c23-0220f4d80c5a,Visualization of Large Complex Data.pdf,CSCI_83,38,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Grid deﬁned by season on the rows and weather in the columns
Boxplots of count of riders by hour of the data for each group
Time-ordered box plots can improve perception over scatter plots
Eye tends to follow the lines
g = sns.FacetGrid(bike_share_df, col=""Weather"", col_order=weather.values(), row=""Season"", height=2 , aspect=2 )
g = g.map(sns.boxplot, ""hr"", 'cnt', color='lightgray', order=None)for ax in g.axes.flat:    ax.set_title(ax.get_title(), fontsize=1 0 )for i in range(4 ):
    g.axes[i,0 ].set_ylabel('Count')",645,semantic
657c5a5f-0d1e-488b-9667-0b8222ecd6c0,Visualization of Large Complex Data.pdf,CSCI_83,39,"Facet Plot of Hourly Counts by Weather and Season
Example: Box plot counts of riders by hour, conditioned on weather and season
Facet plot of count by season and weather",169,semantic
e3bd6c36-b816-42df-9ae1-c8f691ecda7f,Visualization of Large Complex Data.pdf,CSCI_83,40,"Congnostics
How can we visualize very high dimensional data? Modern data sets have thousands to millions of variables
Cannot possibly look at all of these
Idea: need to ﬁnd the most important relationships
Use a cognostic to sort relationship
Cognostic is a statistic to sort data
Sort the variables or relationships by the cognostic
Plot relationships with most interesting cognostic
Idea originally proposed by Tukey, 1982, 1985",430,semantic
2dcf3d22-b1ae-48b5-9009-67f2e6649a74,Visualization of Large Complex Data.pdf,CSCI_83,41,"Cognistic: States With Fastest Rate of Housing Price Increase
## Add an intercept column to the data framehousing.loc[:,'intercept'] = [1 . 0 ] * housing.shape[0 ]## Demean the decimal time columnmean_time = housing.loc[:,'time_decimal'].mean()
housing.loc[:,'time_demean'] = housing.loc[:,'time_decimal'].subtract(mean_time)
## Find the slope coefficients for each state
def prepare_temp(df, group_value, group_variable = 'state'):    temp = df.loc[df.loc[:,group_variable]==group_value,:].copy()    mean_price = np.mean(temp.loc[:,'log_medSoldPriceSqft'])    temp.loc[:,'log_medSoldPriceSqft'] = np.subtract(temp.loc[:,'log_medSoldPriceSqft'], mean_price)
    std_price = np.std(temp.loc[:,'log_medSoldPriceSqft'])    temp.loc[:,'log_medSoldPriceSqft'] = np.divide(temp.loc[:,'log_medSoldPriceSqft'], std_price)    return(temp, mean_price, std_price)
 
def compute_slopes(df, column, group_variable='state'):    slopes = []
    entities = []    intercepts = []    for e in df.loc[:,column].unique():
        temp, mean_price, std_price = prepare_temp(df, e, group_variable=column)        temp_OLS = sm.OLS(temp.loc[:,'log_medSoldPriceSqft'],temp.loc[:,['intercept','time_demean']]).fit()        slopes.append(temp_OLS.params.time_demean)        intercepts.append(temp_OLS.params.intercept)
        entities.append(e)     slopes_df = pd.DataFrame({'slopes':slopes, 'intercept_coef':intercepts, 'entity_name':entities})        slopes_df.sort_values(by='slopes', ascending=False, inplace=True)
    slopes_df.reset_index(inplace=True, drop=True)     return slopes_df
#compute_slopes(housing, 'state')state_slopes = compute_slopes(housing, 'state')
## PLot states with the fastest growing pricing
def find_changes(df, slopes, start, end, col='state'):    increase  = slopes.loc[start:end,'entity_name']    increase_df = df.loc[df.loc[:,col].isin(increase),:]
    increase_df = increase_df.merge(slopes, how='left', right_on='entity_name', left_on=col)    return(increase_df, increase)big_increase_states, increase_states = find_changes(housing, state_slopes, 0 , 7 )    
## Display scatterplot vs timedef plot_price_by_entity(df, order, entity='state', xlims=[2 0 0 7 . 5 , 2 0 1 6 . 5 ]):    g = sns.FacetGrid(df, col=entity, col_wrap=4 , height=2 , col_order=order)
    g = g.map(sns.regplot, 'time_decimal', 'log_medSoldPriceSqft',               line_kws={'color':'red'}, scatter_kws={'alpha': 0 .",2397,semantic
05c02f05-34b0-4a76-b58b-95df13f68603,Visualization of Large Complex Data.pdf,CSCI_83,41,"1 , 's':0 . 5 })    g.set(xlim=(xlims[0 ],xlims[1 ]))    plt.show()
_=plot_price_by_entity(big_increase_states, increase_states)",128,semantic
3f96de5a-3720-4992-b182-34e2ca6c964e,Visualization of Large Complex Data.pdf,CSCI_83,42,"Cognistic: States With Fastest Rate of Housing Price Increase
States with greatest increase in housing price",108,semantic
d49a8855-d1af-4af3-b039-3a4e8af67e00,Visualization of Large Complex Data.pdf,CSCI_83,43,"Summary
We have explored these key points
Proper use of plot aesthetics enable projection of multiple dimensions of complex data onto the 2-dimensional plot surface. All plot aesthetics have limitations which must be understood to use them effectively
The effectiveness of a plot aesthetic varies with the type and the application
Visualization of modern data sets, growing in size and complexity
Visualization limited by 2-dimensional projection
Goal: Understand key relationships in large complex data sets
Difﬁculty: Large data volume
Modern computational systems have massive capacity
Example: Use map-reduce algorithms on cloud clusters
Difﬁculty: Large numbers of variables
Huge number of variables with many potential relationships
This is the hard part!",761,semantic
86335df1-d211-4a1d-bc81-145801469ecc,Visualization of Large Complex Data.pdf,CSCI_83,44,"Summary
Generally combine multiple methods to effectively display complex data
Use plots that inherently scale
Avoid over-plotting to ensure plot is understandable
Choose plot types that do not exhibit overplotting
Often a creative case speciﬁc plot type is best
Use multi-axis plots
Scatter plot matricies
Facet plots
Filter cases using cognositics",349,semantic
